{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview",
  "title": "Gen AI evaluation service overview  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGen AI evaluation service overview\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Gen AI evaluation service provides enterprise-grade tools for objective, data-driven assessment of generative AI models. It supports and informs a number of development tasks like model migrations, prompt editing, and fine-tuning.\nGen AI evaluation service features\nThe defining feature of the Gen AI evaluation service is the ability to use\nadaptive rubrics\n, a set of tailored pass or fail tests for each individual prompt. Evaluation rubrics are similar to unit tests in software development and aim to improve model performance across a variety of tasks.\nThe Gen AI evaluation service supports the following common evaluation methods:\nAdaptive rubrics (Recommended)\n: Generates a unique set of pass or fail\nrubrics\nfor each individual prompt in your dataset.\nStatic rubrics\n: Apply a fixed set of scoring criteria across all prompts.\nComputation-based metrics\n: Use deterministic algorithms like\nROUGE\nor\nBLEU\nwhen a ground truth is available.\nCustom functions\n: Define your own evaluation logic in Python for specialized requirements.\nEvaluation dataset generation\nYou can create an evaluation dataset through the following methods:\nUpload a file\ncontaining complete prompt instances, or provide a prompt template alongside a corresponding file of variable values to populate the completed prompts.\nSample directly from production logs\nto evaluate the real-world usage of your model.\nUse synthetic data generation\nto generate a large number of consistent examples for any prompt template.\nSupported interfaces\nYou can define and run your evaluations using the following interfaces:\nGoogle Cloud console\n: A\nweb user interface\nthat provides a guided, end-to-end workflow. Manage your datasets, run evaluations, and dive deep into interactive reports and visualizations. See\nPerform evaluation using the console\n.\nPython SDK\n: Programmatically run evaluations and render side-by-side model comparisons directly in your Colab or Jupyter environment. See\nPerform evaluation using the GenAI Client in Vertex AI SDK\nUse cases\nThe Gen AI evaluation service lets you see how a model performs on your specific tasks and against your unique criteria providing valuable insights which cannot be derived from public leaderboards and general benchmarks. This supports critical development tasks, including:\nModel migrations\n: Compare model versions to understand behavioral differences and fine-tune your prompts and settings accordingly.\nFinding the best model\n: Run head-to-head comparisons of Google and third-party models on your data to establish a performance baseline and identify the best fit for your use case.\nPrompt improvement\n: Use evaluation results to guide your customization efforts. Re-running an evaluation creates a tight feedback loop, providing immediate, quantifiable feedback on your changes.\nModel fine-tuning\n: Evaluate the quality of a fine-tuned model by applying consistent evaluation criteria to every run.\nAgent evaluation\n: Evaluate the performance of an agent using\nagent-specific metrics, such as agent traces and response quality.\nEvaluations with adaptive rubrics\nAdaptive rubrics\nare the recommended method for most evaluation use cases and are typically the fastest way to get started with evaluations.\nInstead of using a general set of rating rubrics like most LLM-as-a-judge systems, the test-driven evaluation framework adaptively generates a unique set of pass or fail\nrubrics\nfor each individual prompt in your dataset. This approach ensures that every evaluation is relevant to the specific task being evaluated.\nThe evaluation process for each prompt uses a two-step system:\nRubric generation\n: The service first analyzes your prompt and generates a list of specific, verifiable tests—the rubrics—that a good response should meet.\nRubric validation\n: After your model generates a response, the service assesses the response against each rubric, delivering a clear\nPass\nor\nFail\nverdict and a rationale.\nThe final result is an aggregated pass rate and a detailed breakdown of which rubrics the model passed, giving you actionable insights to diagnose issues and measure improvements.\nBy moving from high-level, subjective scores to granular, objective test results, you can adopt an evaluation-driven development cycle and bring the software engineering best practices to the process of building generative AI applications.\nRubrics evaluation example\nTo understand how the Gen AI evaluation service generates and uses rubrics, consider this example:\nUser prompt\n:\nWrite a four-sentence summary of the provided article about renewable energy, maintaining an optimistic tone.\nFor this prompt, the\nrubric generation\nstep might produce the following rubrics:\nRubric 1\n: The response is a summary of the provided article.\nRubric 2\n: The response contains exactly four sentences.\nRubric 3\n: The response maintains an optimistic tone.\nYour model may produce the following response:\nThe article highlights significant growth in solar and wind power. These advancements are making clean energy more affordable. The future looks bright for renewables. However, the report also notes challenges with grid infrastructure.\nDuring\nrubric validation\n, the Gen AI evaluation service assesses the response against each rubric:\nRubric 1\n: The response is a summary of the provided article.\nVerdict\n:\nPass\nReason\n: The response accurately summarizes the main points.\nRubric 2\n: The response contains exactly four sentences.\nVerdict\n:\nPass\nReason\n: The response is composed of four distinct sentences\nRubric 3\n: The response maintains an optimistic tone.\nVerdict\n:\nFail\nReason\n: The final sentence introduces a negative point, which detracts from the optimistic tone.\nThe final pass rate for this response is 66.7%. To compare two models, you can evaluate their responses against this same set of generated tests and compare their overall pass rates.\nEvaluation workflow\nCompleting an evaluation typically requires going through the following steps:\nCreate an evaluation dataset\n: Assemble a dataset of prompt instances that reflect your specific use case. You can include reference answers (ground truth) if you plan to use computation-based metrics.\nDefine evaluation metrics\n: Choose the metrics you want to use to measure model performance. The SDK supports all metric types, while the console supports adaptive rubrics.\nGenerate model responses\n: Select one or more models to generate responses for your dataset. The SDK supports any model callable via\nLiteLLM\n, while the console supports Google Gemini models.\nRun the evaluation\n: Execute the evaluation job, which assesses each model's responses against your selected metrics.\nInterpret the results\n: Review the aggregated scores and individual responses to analyze model performance.\nGetting started with evaluations\nYou can get started with evaluations\nusing the console\n.\nAlternatively, the following code shows how to complete an evaluation with the GenAI Client in Vertex AI SDK:\nfrom\nvertexai\nimport\nClient\nfrom\nvertexai\nimport\ntypes\nimport\npandas\nas\npd\nclient\n=\nClient\n(\nproject\n=\nPROJECT_ID\n,\nlocation\n=\nLOCATION\n)\n# Create an evaluation dataset\nprompts_df\n=\npd\n.\nDataFrame\n({\n\"prompt\"\n:\n[\n\"Write a simple story about a dinosaur\"\n,\n\"Generate a poem about Vertex AI\"\n,\n],\n})\n# Get responses from one or multiple models\neval_dataset\n=\nclient\n.\nevals\n.\nrun_inference\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\nsrc\n=\nprompts_df\n)\n# Define the evaluation metrics and run the evaluation job\neval_result\n=\nclient\n.\nevals\n.\nevaluate\n(\ndataset\n=\neval_dataset\n,\nmetrics\n=\n[\ntypes\n.\nRubricMetric\n.\nGENERAL_QUALITY\n]\n)\n# View the evaluation results\neval_result\n.\nshow\n()\nThe Gen AI evaluation service offers two SDK interfaces:\nGenAI Client in Vertex AI SDK\n(Recommended) (Preview)\nfrom vertexai import client\nThe GenAI Client is the newer, recommended interface for evaluation, accessed through the unified Client class. It supports all evaluation methods and is designed for workflows that include model comparison, in-notebook visualization, and insights for model customization.\nEvaluation module in Vertex AI SDK\n(GA)\nfrom vertexai.evaluation import EvalTask\nThe evaluation module is the\nolder interface\n, maintained for backward compatibility with existing workflows but no longer under active development. It is accessed through the\nEvalTask\nclass. This method supports standard LLM-as-a-judge and computation-based metrics but does not support newer evaluation methods like adaptive rubrics.\nSupported regions\nThe following regions are supported for the Gen AI evaluation service:\nIowa (\nus-central1\n)\nNorthern Virginia (\nus-east4\n)\nOregon (\nus-west1\n)\nLas Vegas, Nevada (\nus-west4\n)\nBelgium (\neurope-west1\n)\nNetherlands (\neurope-west4\n)\nParis, France (\neurope-west9\n)\nAvailable notebooks\nNotebook links\nDescription\nGetting Started: Quick Gen AI Evaluation\nProvides an introduction to the Gen AI evaluation service.\nEvaluating third-party models with the Gen AI evaluation service\nDemonstrates how to use the Vertex Gen AI Evaluation SDK to evaluate various types of third-party models, including models accessed using API (like OpenAI, Anthropic), Model as a Service (MaaS) from Vertex Model Garden, and Bring Your Own Model (BYOM) endpoints.\nModel migration with the Gen AI evaluation service\nShows how to use the Vertex AI SDK for Gen AI evaluation service to compare two first-party models (such as Gemini 2.0 Flash to Gemini 2.5 Flash). It highlights using predefined adaptive rubric-based metrics and how evaluation results can guide prompt optimization. Key features like multi-candidate evaluation, in-notebook visualization, and asynchronous batch evaluation are also covered.\nWhat's next\nPerform evaluation using the console\nPerform evaluation using the SDK\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Gen AI evaluation service overview  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:27:47"
  }
}