{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api",
  "title": "Live API  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nLive API\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Live API enables low-latency, two-way voice and video interactions\nwith Gemini. Use the Live API to provide end users with\nnatural, human-like voice conversations, including the ability to interrupt the\nmodel's responses with voice commands.\nThis document covers the basics of using Live API, including its\ncapabilities, starter examples, and basic use case code examples. If you're\nlooking for information on how to start an interactive conversation using the\nLive API, see\nInteractive conversations with the Live API\n. If you're looking for information\non what tools the Live API can use, see\nBuilt-in\ntools\n.\nTry in Vertex AI\nSupported models\nThe Live API is supported for use in both the Google Gen AI SDK and\nusing Vertex AI Studio. Some features (like text input and output) are\nonly available using the Gen AI SDK.\nYou can use the Live API with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash\nPrivate GA\n*\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\ngemini-live-2.5-flash-preview-native-audio\nPublic preview; Discontinuation date: October 18, 2025\n*\nReach out to your Google account team representative to request\naccess.\nFor more information including technical specifications and limitations, see the\nLive API reference\nguide\n.\nLive API capabilities\nReal-time multimodal\nunderstanding:\nConverse\nwith Gemini about what it sees on a video feed or through screen\nsharing, using built-in support for streaming audio and video.\nBuilt-in tool usage:\nSeamlessly integrate\ntools like\nfunction calling\nand\nGrounding with Google Search\ninto your\nconversations for more practical and dynamic interactions.\nLow latency interactions:\nHave low latency, human-like interactions with\nGemini.\nMultilingual support:\nConverse in 24 supported languages.\n(GA versions only)\nSupport for\nProvisioned Throughput\n:\nUse fixed-cost, fixed-term subscription\navailable in several term-lengths that reserves throughput for supported\ngenerative AI models on Vertex AI, including Live API.\nHigh-quality transcription:\nLive API supports\ntext transcription\nfor both input and output audio.\nGemini 2.5 Flash with Live API also includes\nnative audio\nas a public preview offering. Native audio introduces:\nAffective Dialog:\nLive API\nunderstands and responds to the user's tone of voice. The same words spoken\nin different ways can lead to vastly different and more nuanced\nconversations.\nProactive Audio and context awareness:\nLive API intelligently disregards ambient conversations and other\nirrelevant audio, understanding when to listen and when to remain silent.\nFor more information on native audio, see\nBuilt-in\ntools\n.\nSupported audio formats\nThe Live API supports the following audio formats:\nInput audio:\nRaw 16-bit PCM audio at 16kHz, little-endian\nOutput audio:\nRaw 16-bit PCM audio at 24kHz, little-endian\nSupported video formats\nLive API supports video frames input at 1FPS. For best results, use\nnative 768x768 resolution at 1FPS.\nStarter examples\nYou can get started using the Live API with one of the following\nnotebook tutorials, demo applications, or guides.\nNotebook tutorials\nDownload these notebook tutorials from GitHub, or open the notebook tutorials in\nthe environment of your choice.\nUse WebSockets with the Live API\nStreaming audio and video\nDemo applications and guides\nWebSocket demo app\nAgent Development Kit: Custom Audio Streaming\nProject Livewire\nAdditional examples\nTo get even more utility from Live API, try these examples that use\nLive API's audio processing, transcription, and voice response\ncapabilities.\nGet text responses from audio input\nYou can send audio and receive text responses by converting the audio to a\n16-bit PCM, 16kHz, mono format. The following example reads a WAV file and sends\nit in the correct format:\nPython\n# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav\n# Install helpers for converting files: pip install librosa soundfile\nimport\nasyncio\nimport\nio\nfrom\npathlib\nimport\nPath\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nimport\nsoundfile\nas\nsf\nimport\nlibrosa\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"TEXT\"\n]}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nbuffer\n=\nio\n.\nBytesIO\n()\ny\n,\nsr\n=\nlibrosa\n.\nload\n(\n\"sample.wav\"\n,\nsr\n=\n16000\n)\nsf\n.\nwrite\n(\nbuffer\n,\ny\n,\nsr\n,\nformat\n=\n\"RAW\"\n,\nsubtype\n=\n\"PCM_16\"\n)\nbuffer\n.\nseek\n(\n0\n)\naudio_bytes\n=\nbuffer\n.\nread\n()\n# If already in correct format, you can use this:\n# audio_bytes = Path(\"sample.pcm\").read_bytes()\nawait\nsession\n.\nsend_realtime_input\n(\naudio\n=\ntypes\n.\nBlob\n(\ndata\n=\naudio_bytes\n,\nmime_type\n=\n\"audio/pcm;rate=16000\"\n)\n)\nasync\nfor\nresponse\nin\nsession\n.\nreceive\n():\nif\nresponse\n.\ntext\nis\nnot\nNone\n:\nprint\n(\nresponse\n.\ntext\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nGet voice responses from text input\nUse this example to send text input and receive synthesized speech responses:\nPython\nimport\nasyncio\nimport\nnumpy\nas\nnp\nfrom\nIPython.display\nimport\nAudio\n,\nMarkdown\n,\ndisplay\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nContent\n,\nLiveConnectConfig\n,\nHttpOptions\n,\nModality\n,\nPart\n,\nSpeechConfig\n,\nVoiceConfig\n,\nPrebuiltVoiceConfig\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nvoice_name\n=\n\"Aoede\"\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nspeech_config\n=\nSpeechConfig\n(\nvoice_config\n=\nVoiceConfig\n(\nprebuilt_voice_config\n=\nPrebuiltVoiceConfig\n(\nvoice_name\n=\nvoice_name\n,\n)\n),\n),\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\n\"gemini-live-2.5-flash\"\n,\nconfig\n=\nconfig\n,\n)\nas\nsession\n:\ntext_input\n=\n\"Hello? Gemini are you there?\"\ndisplay\n(\nMarkdown\n(\nf\n\"**Input:**\n{\ntext_input\n}\n\"\n))\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n(\ntext\n=\ntext_input\n)]))\naudio_data\n=\n[]\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\n(\nmessage\n.\nserver_content\n.\nmodel_turn\nand\nmessage\n.\nserver_content\n.\nmodel_turn\n.\nparts\n):\nfor\npart\nin\nmessage\n.\nserver_content\n.\nmodel_turn\n.\nparts\n:\nif\npart\n.\ninline_data\n:\naudio_data\n.\nappend\n(\nnp\n.\nfrombuffer\n(\npart\n.\ninline_data\n.\ndata\n,\ndtype\n=\nnp\n.\nint16\n)\n)\nif\naudio_data\n:\ndisplay\n(\nAudio\n(\nnp\n.\nconcatenate\n(\naudio_data\n),\nrate\n=\n24000\n,\nautoplay\n=\nTrue\n))\nFor more examples of sending text, see\nour Getting Started\nguide\n.\nTranscribe audio\nThe Live API can transcribe both input and output audio. Use the\nfollowing example to enable transcription:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"AUDIO\"\n],\n\"input_audio_transcription\"\n:\n{},\n\"output_audio_transcription\"\n:\n{}\n}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nmessage\n=\n\"Hello? Gemini are you there?\"\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[{\n\"text\"\n:\nmessage\n}]},\nturn_complete\n=\nTrue\n)\nasync\nfor\nresponse\nin\nsession\n.\nreceive\n():\nif\nresponse\n.\nserver_content\n.\nmodel_turn\n:\nprint\n(\n\"Model turn:\"\n,\nresponse\n.\nserver_content\n.\nmodel_turn\n)\nif\nresponse\n.\nserver_content\n.\ninput_transcription\n:\nprint\n(\n\"Input transcript:\"\n,\nresponse\n.\nserver_content\n.\ninput_transcription\n.\ntext\n)\nif\nresponse\n.\nserver_content\n.\noutput_transcription\n:\nprint\n(\n\"Output transcript:\"\n,\nresponse\n.\nserver_content\n.\noutput_transcription\n.\ntext\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nWebSockets\n# Set model generation_config\nCONFIG\n=\n{\n'response_modalities'\n:\n[\n'AUDIO'\n],\n}\nheaders\n=\n{\n\"Content-Type\"\n:\n\"application/json\"\n,\n\"Authorization\"\n:\nf\n\"Bearer\n{\nbearer_token\n[\n0\n]\n}\n\"\n,\n}\n# Connect to the server\nasync\nwith\nconnect\n(\nSERVICE_URL\n,\nadditional_headers\n=\nheaders\n)\nas\nws\n:\n# Setup the session\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\n{\n\"setup\"\n:\n{\n\"model\"\n:\n\"gemini-2.0-flash-live-preview-04-09\"\n,\n\"generation_config\"\n:\nCONFIG\n,\n'input_audio_transcription'\n:\n{},\n'output_audio_transcription'\n:\n{}\n}\n}\n)\n)\n# Receive setup response\nraw_response\n=\nawait\nws\n.\nrecv\n(\ndecode\n=\nFalse\n)\nsetup_response\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n(\n\"ascii\"\n))\n# Send text message\ntext_input\n=\n\"Hello? Gemini are you there?\"\ndisplay\n(\nMarkdown\n(\nf\n\"**Input:**\n{\ntext_input\n}\n\"\n))\nmsg\n=\n{\n\"client_content\"\n:\n{\n\"turns\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[{\n\"text\"\n:\ntext_input\n}]}],\n\"turn_complete\"\n:\nTrue\n,\n}\n}\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\nmsg\n))\nresponses\n=\n[]\ninput_transcriptions\n=\n[]\noutput_transcriptions\n=\n[]\n# Receive chucks of server response\nasync\nfor\nraw_response\nin\nws\n:\nresponse\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n())\nserver_content\n=\nresponse\n.\npop\n(\n\"serverContent\"\n,\nNone\n)\nif\nserver_content\nis\nNone\n:\nbreak\nif\n(\ninput_transcription\n:=\nserver_content\n.\nget\n(\n\"inputTranscription\"\n))\nis\nnot\nNone\n:\nif\n(\ntext\n:=\ninput_transcription\n.\nget\n(\n\"text\"\n))\nis\nnot\nNone\n:\ninput_transcriptions\n.\nappend\n(\ntext\n)\nif\n(\noutput_transcription\n:=\nserver_content\n.\nget\n(\n\"outputTranscription\"\n))\nis\nnot\nNone\n:\nif\n(\ntext\n:=\noutput_transcription\n.\nget\n(\n\"text\"\n))\nis\nnot\nNone\n:\noutput_transcriptions\n.\nappend\n(\ntext\n)\nmodel_turn\n=\nserver_content\n.\npop\n(\n\"modelTurn\"\n,\nNone\n)\nif\nmodel_turn\nis\nnot\nNone\n:\nparts\n=\nmodel_turn\n.\npop\n(\n\"parts\"\n,\nNone\n)\nif\nparts\nis\nnot\nNone\n:\nfor\npart\nin\nparts\n:\npcm_data\n=\nbase64\n.\nb64decode\n(\npart\n[\n\"inlineData\"\n][\n\"data\"\n])\nresponses\n.\nappend\n(\nnp\n.\nfrombuffer\n(\npcm_data\n,\ndtype\n=\nnp\n.\nint16\n))\n# End of turn\nturn_complete\n=\nserver_content\n.\npop\n(\n\"turnComplete\"\n,\nNone\n)\nif\nturn_complete\n:\nbreak\nif\ninput_transcriptions\n:\ndisplay\n(\nMarkdown\n(\nf\n\"**Input transcription >**\n{\n''\n.\njoin\n(\ninput_transcriptions\n)\n}\n\"\n))\nif\nresponses\n:\n# Play the returned audio message\ndisplay\n(\nAudio\n(\nnp\n.\nconcatenate\n(\nresponses\n),\nrate\n=\n24000\n,\nautoplay\n=\nTrue\n))\nif\noutput_transcriptions\n:\ndisplay\n(\nMarkdown\n(\nf\n\"**Output transcription >**\n{\n''\n.\njoin\n(\noutput_transcriptions\n)\n}\n\"\n))\nPricing for Live API transcription is determined by the number of text\noutput tokens. To learn more, please see the\nVertex AI pricing page\n.\nMore information\nFor more information on using the Live API, see:\nBest practices with the Live API\nLive API reference guide\nInteractive conversations\nBuilt-in tools\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Live API  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:25:53"
  }
}