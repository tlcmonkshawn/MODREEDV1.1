{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.Tool",
  "title": "Python client libraries  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nPython\nClient libraries\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nVersion latest\nkeyboard_arrow_down\n1.122.0 (latest)\n1.121.0\n1.120.0\n1.119.0\n1.118.0\n1.117.0\n1.95.1\n1.94.0\n1.93.1\n1.92.0\n1.91.0\n1.90.0\n1.89.0\n1.88.0\n1.87.0\n1.86.0\n1.85.0\n1.84.0\n1.83.0\n1.82.0\n1.81.0\n1.80.0\n1.79.0\n1.78.0\n1.77.0\n1.76.0\n1.75.0\n1.74.0\n1.73.0\n1.72.0\n1.71.1\n1.70.0\n1.69.0\n1.68.0\n1.67.1\n1.66.0\n1.65.0\n1.63.0\n1.62.0\n1.60.0\n1.59.0\nVertex Generative AI SDK for Python\nThe Gen AI Modules in the Vertex SDK help developers use Google’s generative AI\nGemini models\nto build AI-powered features and applications in Vertex.\nThe modules currently available are: Evaluation, Agent Engines, Prompt Management, and Prompt Optimization. See below for instructions on getting started with each module. For other Gemini features on Vertex, use the\nGen AI SDK\n.\nInstallation\nTo install the\ngoogle-cloud-aiplatform\nPython package, run the following command:\npip3 install --upgrade --user \"google-cloud-aiplatform>=1.114.0\"\nImports:\nimport\nvertexai\nfrom vertexai import types\nClient initialization\nclient = vertexai.Client(project='my-project', location='us-central1')\nGen AI Evaluation\nTo run evaluation, first generate model responses from a set of prompts.\nimport pandas as pd\nprompts_df = pd.DataFrame({\n\"prompt\": [\n\"What is the capital of France?\",\n\"Write a haiku about a cat.\",\n\"Write a Python function to calculate the factorial of a number.\",\n\"Translate 'How are you?' to French.\",\n],\n})\ninference_results = client.evals.run_inference(\nmodel=\"gemini-2.5-flash\",\n(\"def factorial(n):\\n\"\n\"    if n < 0:\\n\"\n\"        return 'Factorial does not exist for negative numbers'\\n\"\n\"    elif n == 0:\\n\"\n\"        return 1\\n\"\n\"    else:\\n\"\n\"        fact = 1\\n\"\n\"        i = 1\\n\"\n\"        while i <= n:\\n\"\n\"            fact *= i\\n\"\n\"            i += 1\\n\"\n\"        return fact\"),\n)\ninference_results.show()\nThen run evaluation by providing the inference results and specifying the metric types.\neval_result = client.evals.evaluate(\ndataset=inference_results,\nmetrics=[\ntypes.RubricMetric.GENERAL_QUALITY,\n]\n)\neval_result.show()\nAgent Engine with Agent Development Kit (ADK)\nFirst, define a function that looks up the exchange rate:\ndef get_exchange_rate(\ncurrency_from: str = \"USD\",\ncurrency_to: str = \"EUR\",\ncurrency_date: str = \"latest\",\n):\n\"\"\"Retrieves the exchange rate between two currencies on a specified date.\nUses the Frankfurter API (https://api.frankfurter.app/) to obtain\nexchange rate data.\nReturns:\ndict: A dictionary containing the exchange rate information.\nExample: {\"amount\": 1.0, \"base\": \"USD\", \"date\": \"2023-11-24\",\n\"rates\": {\"EUR\": 0.95534}}\n\"\"\"\nimport requests\nresponse = requests.get(\nf\"https://api.frankfurter.app/{currency_date}\",\nparams={\"from\": currency_from, \"to\": currency_to},\n)\nreturn response.json()\nNext, define an ADK Agent:\nfrom google.adk.agents import Agent\nfrom vertexai.agent_engines import AdkApp\napp = AdkApp(agent=Agent(\nmodel=\"gemini-2.0-flash\",        # Required.\nname='currency_exchange_agent',  # Required.\ntools=[get_exchange_rate],       # Optional.\n))\nTest the agent locally using US dollars and Swedish Krona:\nasync for event in app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nTo deploy the agent to Agent Engine:\nremote_app = client.agent_engines.create(\nagent=app,\nconfig={\n\"requirements\": [\"google-cloud-aiplatform[agent_engines,adk]\"],\n},\n)\nYou can also run queries against the deployed agent:\nasync for event in remote_app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nPrompt Optimization\nTo do a zero-shot prompt optimization, use the\noptimize_prompt\nmethod.\nprompt = \"Generate system instructions for a question-answering assistant\"\nresponse = client.prompt_optimizer.optimize_prompt(prompt=prompt)\nprint(response.raw_text_response)\nif response.parsed_response:\nprint(response.parsed_response.suggested_prompt)\nTo call the data-driven prompt optimization, call the\noptimize\nmethod.\nIn this case however, we need to provide\nvapo_config\n. This config needs to\nhave either service account or project\nnumber\nand the config path.\nPlease refer to this\ntutorial\nfor more details on config parameter.\nimport logging\nproject_number = PROJECT_NUMBER # replace with your project number\nservice_account = f\"{project_number}-compute@developer.gserviceaccount.com\"\nvapo_config = types.PromptOptimizerVAPOConfig(\nconfig_path=\"gs://your-bucket/config.json\",\nservice_account_project_number=project_number,\nwait_for_completion=False\n)\n# Set up logging to see the progress of the optimization job\nlogging.basicConfig(encoding='utf-8', level=logging.INFO, force=True)\nresult = client.prompt_optimizer.optimize(method=\"vapo\", config=vapo_config)\nWe can also call optimize method async.\nawait client.aio.prompt_optimizer.optimize(method=\"vapo\", config=vapo_config)\nPrompt Management\nFirst define your prompt as a dictionary or\ntypes.Prompt\nobject. Then call\ncreate()\n.\nprompt = {\n\"prompt_data\": {\n\"contents\": [{\"parts\": [{\"text\": \"Hello, {name}! How are you?\"}]}],\n\"system_instruction\": {\"parts\": [{\"text\": \"Please answer in a short sentence.\"}]},\n\"variables\": [\n{\"name\": {\"text\": \"Alice\"}},\n],\n\"model\": \"gemini-2.5-flash\",\n},\n}\nprompt_resource = client.prompts.create(\nprompt=prompt,\n)\nNote that you can also use the\ntypes.Prompt\nobject to define your prompt. Some\nof the types used to do this are from the\nGen AI SDK\n.\nimport types\nfrom google.genai import types as genai_types\nprompt = types.Prompt(\nprompt_data=types.PromptData(\ncontents=[genai_types.Content(parts=[genai_types.Part(text=\"Hello, {name}! How are you?\")])],\nsystem_instruction=genai_types.Content(parts=[genai_types.Part(text=\"Please answer in a short sentence.\")]),\nvariables=[\n{\"name\": genai_types.Part(text=\"Alice\")},\n],\nmodel=\"gemini-2.5-flash\",\n),\n)\nRetrieve a prompt by calling\nget()\nwith the\nprompt_id\n.\nretrieved_prompt = client.prompts.get(prompt_id=prompt_resource.prompt_id)\nAfter creating or retrieving a prompt, you can call\ngenerate_content()\nwith\nthat prompt using the Gen AI SDK.\nThe following uses a utility function available on Prompt objects to transform a\nPrompt object into a list of Content objects for use with\ngenerate_content\n. To\nrun this you need to have the Gen AI SDK installed, which you can do via\npip install google-genai\n.\nfrom google import genai\nfrom google.genai import types as genai_types\n# Create a Client in the Gen AI SDK\ngenai_client = genai.Client(vertexai=True, project=\"your-project\", location=\"your-location\")\n# Call generate_content() with the prompt\nresponse = genai_client.models.generate_content(\nmodel=retrieved_prompt.prompt_data.model,\ncontents=retrieved_prompt.assemble_contents(),\n)\nWarning\nThe following Generative AI modules in the Vertex AI SDK are deprecated as of\nJune 24, 2025 and will be removed on June 24, 2026:\nvertexai.generative_models\n,\nvertexai.language_models\n,\nvertexai.vision_models\n,\nvertexai.tuning\n,\nvertexai.caching\n. Please use the\nGoogle Gen AI SDK\nto access these\nfeatures. See\nthe migration guide\nfor details. You can continue using all other Vertex AI SDK modules, as they are\nthe recommended way to use the API.\nImports:\nimport\nvertexai\nInitialization:\nvertexai.init(project='my-project', location='us-central1')\nBasic generation:\nfrom vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\"Why is sky blue?\"))\nUsing images and videos\nfrom vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-pro-vision\")\n# Local image\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_model.generate_content([\"What is shown in this image?\", image]))\n# Image from Cloud Storage\nimage_part = generative_models.Part.from_uri(\"gs://download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\", mime_type=\"image/jpeg\")\nprint(vision_model.generate_content([image_part, \"Describe this image?\"]))\n# Text and video\nvideo_part = Part.from_uri(\"gs://cloud-samples-data/video/animals.mp4\", mime_type=\"video/mp4\")\nprint(vision_model.generate_content([\"What is in the video? \", video_part]))\nChat\nfrom vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-ultra-vision\")\nvision_chat = vision_model.start_chat()\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_chat.send_message([\"I like this image.\", image]))\nprint(vision_chat.send_message(\"What things do I like?.\"))\nSystem instructions\nfrom vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\n\"gemini-1.0-pro\",\nsystem_instruction=[\n\"Talk like a pirate.\",\n\"Don't use rude words.\",\n],\n)\nprint(model.generate_content(\"Why is sky blue?\"))\nFunction calling\n# First, create tools that the model is can use to answer your questions.\n# Describe a function by specifying it's schema (JsonSchema format)\nget_current_weather_func = generative_models.FunctionDeclaration(\nname=\"get_current_weather\",\ndescription=\"Get the current weather in a given location\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\"\n},\n\"unit\": {\n\"type\": \"string\",\n\"enum\": [\n\"celsius\",\n\"fahrenheit\",\n]\n}\n},\n\"required\": [\n\"location\"\n]\n},\n)\n# Tool is a collection of related functions\nweather_tool = generative_models.Tool(\nfunction_declarations=[get_current_weather_func],\n)\n# Use tools in chat:\nmodel = GenerativeModel(\n\"gemini-pro\",\n# You can specify tools when creating a model to avoid having to send them with every request.\ntools=[weather_tool],\n)\nchat = model.start_chat()\n# Send a message to the model. The model will respond with a function call.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\n# Then send a function response to the model. The model will use it to answer.\nprint(chat.send_message(\nPart.from_function_response(\nname=\"get_current_weather\",\nresponse={\n\"content\": {\"weather\": \"super nice\"},\n}\n),\n))\nAutomatic Function calling\nNote: The\nFunctionDeclaration.from_func\nconverter does not support nested types for parameters. Please provide full\nFunctionDeclaration\ninstead.\nfrom vertexai.preview.generative_models import GenerativeModel, Tool, FunctionDeclaration, AutomaticFunctionCallingResponder\n# First, create functions that the model can use to answer your questions.\ndef get_current_weather(location: str, unit: str = \"centigrade\"):\n\"\"\"Gets weather in the specified location.\nArgs:\nlocation: The location for which to get the weather.\nunit: Optional. Temperature unit. Can be Centigrade or Fahrenheit. Defaults to Centigrade.\n\"\"\"\nreturn dict(\nlocation=location,\nunit=unit,\nweather=\"Super nice, but maybe a bit hot.\",\n)\n# Infer function schema\nget_current_weather_func = FunctionDeclaration.from_func(get_current_weather)\n# Tool is a collection of related functions\nweather_tool = Tool(\nfunction_declarations=[get_current_weather_func],\n)\n# Use tools in chat:\nmodel = GenerativeModel(\n\"gemini-pro\",\n# You can specify tools when creating a model to avoid having to send them with every request.\ntools=[weather_tool],\n)\n# Activate automatic function calling:\nafc_responder = AutomaticFunctionCallingResponder(\n# Optional:\nmax_automatic_function_calls=5,\n)\nchat = model.start_chat(responder=afc_responder)\n# Send a message to the model. The model will respond with a function call.\n# The SDK will automatically call the requested function and respond to the model.\n# The model will use the function call response to answer the original question.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nEvaluation\nTo perform bring-your-own-response(BYOR) evaluation, provide the model responses in the\nresponse\ncolumn in the dataset. If a pairwise metric is used for BYOR evaluation, provide the baseline model responses in the\nbaseline_model_response\ncolumn.\nimport pandas as pd\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples\neval_dataset = pd.DataFrame({\n\"prompt\"  : [...],\n\"reference\": [...],\n\"response\" : [...],\n\"baseline_model_response\": [...],\n})\neval_task = EvalTask(\ndataset=eval_dataset,\nmetrics=[\n\"bleu\",\n\"rouge_l_sum\",\nMetricPromptTemplateExamples.Pointwise.FLUENCY,\nMetricPromptTemplateExamples.Pairwise.SAFETY\n],\nexperiment=\"my-experiment\",\n)\neval_result = eval_task.evaluate(experiment_run_name=\"eval-experiment-run\")\nTo perform evaluation with Gemini model inference, specify the\nmodel\nparameter with a\nGenerativeModel\ninstance.  The input column name to the model is\nprompt\nand must be present in the dataset.\nfrom vertexai.evaluation import EvalTask\nfrom vertexai.generative_models import GenerativeModel\neval_dataset = pd.DataFrame({\n\"reference\": [...],\n\"prompt\"  : [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[\"exact_match\", \"bleu\", \"rouge_1\", \"rouge_l_sum\"],\nexperiment=\"my-experiment\",\n).evaluate(\nmodel=GenerativeModel(\"gemini-1.5-pro\"),\nexperiment_run_name=\"gemini-eval-run\"\n)\nIf a\nprompt_template\nis specified, the\nprompt\ncolumn is not required. Prompts can be assembled from the evaluation dataset, and all prompt template variable names must be present in the dataset columns.\nimport pandas as pd\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel\neval_dataset = pd.DataFrame({\n\"context\"    : [...],\n\"instruction\": [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[MetricPromptTemplateExamples.Pointwise.SUMMARIZATION_QUALITY],\n).evaluate(\nmodel=GenerativeModel(\"gemini-1.5-pro\"),\nprompt_template=\"{instruction}. Article: {context}. Summary:\",\n)\nTo perform evaluation with custom model inference, specify the\nmodel\nparameter with a custom inference function. The input column name to the\ncustom inference function is\nprompt\nand must be present in the dataset.\nfrom openai import OpenAI\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples\nclient = OpenAI()\ndef custom_model_fn(input: str) -> str:\nresponse = client.chat.completions.create(\nmodel=\"gpt-3.5-turbo\",\nmessages=[\n{\"role\": \"user\", \"content\": input}\n]\n)\nreturn response.choices[0].message.content\neval_dataset = pd.DataFrame({\n\"prompt\"  : [...],\n\"reference\": [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[MetricPromptTemplateExamples.Pointwise.SAFETY],\nexperiment=\"my-experiment\",\n).evaluate(\nmodel=custom_model_fn,\nexperiment_run_name=\"gpt-eval-run\"\n)\nTo perform pairwise metric evaluation with model inference step, specify\nthe\nbaseline_model\ninput to a\nPairwiseMetric\ninstance and the candidate\nmodel\ninput to the\nEvalTask.evaluate()\nfunction. The input column name\nto both models is\nprompt\nand must be present in the dataset.\nimport pandas as pd\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples, PairwiseMetric\nfrom vertexai.generative_models import GenerativeModel\nbaseline_model = GenerativeModel(\"gemini-1.0-pro\")\ncandidate_model = GenerativeModel(\"gemini-1.5-pro\")\npairwise_groundedness = PairwiseMetric(\nmetric_prompt_template=MetricPromptTemplateExamples.get_prompt_template(\n\"pairwise_groundedness\"\n),\nbaseline_model=baseline_model,\n)\neval_dataset = pd.DataFrame({\n\"prompt\"  : [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[pairwise_groundedness],\nexperiment=\"my-pairwise-experiment\",\n).evaluate(\nmodel=candidate_model,\nexperiment_run_name=\"gemini-pairwise-eval-run\",\n)\nAgent Engine\nBefore you begin, install the packages with\npip3 install --upgrade --user \"google-cloud-aiplatform[agent_engines,adk]>=1.111\"\nFirst, define a function that looks up the exchange rate:\ndef get_exchange_rate(\ncurrency_from: str = \"USD\",\ncurrency_to: str = \"EUR\",\ncurrency_date: str = \"latest\",\n):\n\"\"\"Retrieves the exchange rate between two currencies on a specified date.\nUses the Frankfurter API (https://api.frankfurter.app/) to obtain\nexchange rate data.\nReturns:\ndict: A dictionary containing the exchange rate information.\nExample: {\"amount\": 1.0, \"base\": \"USD\", \"date\": \"2023-11-24\",\n\"rates\": {\"EUR\": 0.95534}}\n\"\"\"\nimport requests\nresponse = requests.get(\nf\"https://api.frankfurter.app/{currency_date}\",\nparams={\"from\": currency_from, \"to\": currency_to},\n)\nreturn response.json()\nNext, define an ADK Agent:\nfrom google.adk.agents import Agent\nfrom vertexai.agent_engines import AdkApp\napp = AdkApp(agent=Agent(\nmodel=\"gemini-2.0-flash\",        # Required.\nname='currency_exchange_agent',  # Required.\ntools=[get_exchange_rate],       # Optional.\n))\nTest the agent locally using US dollars and Swedish Krona:\nasync for event in app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nTo deploy the agent to Agent Engine:\nvertexai.init(\nproject='my-project',\nlocation='us-central1',\nstaging_bucket=\"gs://my-staging-bucket\",\n)\nremote_app = vertexai.agent_engines.create(\napp,\nrequirements=[\"google-cloud-aiplatform[agent_engines,adk]\"],\n)\nYou can also run queries against the deployed agent:\nasync for event in remote_app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nDocumentation\nYou can find complete documentation for the Vertex AI SDKs and the Gemini model in the Google Cloud\ndocumentation\nContributing\nSee\nContributing\nfor more information on contributing to the Vertex AI Python SDK.\nLicense\nThe contents of this repository are licensed under the\nApache License, version 2.0\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-10-30 UTC.",
  "metadata": {
    "title": "Python client libraries  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:26:05"
  }
}