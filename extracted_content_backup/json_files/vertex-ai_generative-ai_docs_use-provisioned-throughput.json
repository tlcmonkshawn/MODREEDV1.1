{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput",
  "title": "Use Provisioned Throughput  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nUse Provisioned Throughput\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains how Provisioned Throughput works, how to control\noverages or bypass Provisioned Throughput, and how to monitor usage.\nHow Provisioned Throughput works\nThis section explains how Provisioned Throughput works by using\nquota checking through the quota enforcement period.\nProvisioned Throughput quota checking\nYour Provisioned Throughput maximum quota is a multiple of the\nnumber of generative AI scale units (GSUs) purchased and the throughput per GSU.\nIt's checked each time you make a request within your\nquota enforcement\nperiod\n, which is how frequently the maximum Provisioned Throughput\nquota is enforced.\nAt the time a request is received, the true response size is unknown. Because we\nprioritize speed of response for real-time applications,\nProvisioned Throughput estimates the output token size. If the\ninitial estimate exceeds the available Provisioned Throughput\nmaximum quota, the request is processed as pay-as-you-go. Otherwise, it is\nprocessed as Provisioned Throughput. This is done by comparing the\ninitial estimate to your Provisioned Throughput maximum quota.\nWhen the response is generated and the true output token size is known, actual\nusage and quota are reconciled by adding the difference between the estimate and\nthe actual usage to your available Provisioned Throughput quota\namount.\nProvisioned Throughput quota enforcement period\nFor Gemini models, the\nquota enforcement period can take up to 30 seconds and is subject to change.\nThis means that you might temporarily experience prioritized traffic that\nexceeds your quota amount on a per-second basis in some cases, but you shouldn't\nexceed your quota on a 30-second basis. These periods are based on the\nVertex AI internal clock time and are independent of when requests are\nmade.\nFor example, if you purchase one GSU of\ngemini-2.0-flash-001\n, then you\nshould expect 3,360 tokens per second of always-on throughput. On average, you\ncan't exceed 100,800 tokens on a 30-second basis, which is calculated using\nthe following formula:\n3,360 tokens per second * 30 seconds = 100,800 tokens\nIf, in a day, you submitted only one request that consumed 8,000 tokens in a\nsecond, it might still be processed as a Provisioned Throughput\nrequest, even though you exceeded your 3,360 tokens per second limit at the time\nof the request. This is because the request didn't exceed the threshold of\n100,800 tokens per 30 seconds.\nControl overages or bypass Provisioned Throughput\nUse the API to control overages when you exceed your purchased throughput\nor to bypass Provisioned Throughput on a per-request basis.\nRead through each option to determine what you must do to meet your use case.\nDefault behavior\nIf you exceed your purchased amount of throughput, the overages go to on-demand\nand are billed at the\npay-as-you-go rate\n. After your\nProvisioned Throughput order is active, the default behavior takes place\nautomatically. You don't have to change your code to begin consuming your\norder as long as you are consuming it in the region provisioned.\nUse only Provisioned Throughput\nIf you are managing costs by avoiding on-demand charges, use only\nProvisioned Throughput. Requests which exceed the\nProvisioned Throughput order amount return an\nerror\n429\n.\nWhen sending requests to the API, set the\nX-Vertex-AI-LLM-Request-Type\nHTTP\nheader to\ndedicated\n.\nUse only pay-as-you-go\nThis is also referred to as using on-demand. Requests bypass the Provisioned Throughput\norder and are sent directly to pay-as-you-go. This might be useful\nfor experiments or applications that are in development.\nWhen sending requests to the API, set the\nX-Vertex-AI-LLM-Request-Type\nHTTP header to\nshared\n.\nExample\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nHttpOptions\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n,\nheaders\n=\n{\n# Options:\n# - \"dedicated\": Use Provisioned Throughput\n# - \"shared\": Use pay-as-you-go\n# https://cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput\n\"X-Vertex-AI-LLM-Request-Type\"\n:\n\"shared\"\n},\n)\n)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"How does AI work?\"\n,\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\n\"net/http\"\n\"google.golang.org/genai\"\n)\n//\ngenerateText\nshows\nhow\nto\ngenerate\ntext\nProvisioned\nThroughput\n.\nfunc\ngenerateText\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n,\nHeaders\n:\nhttp\n.\nHeader\n{\n//\nOptions\n:\n//\n-\n\"dedicated\"\n:\nUse\nProvisioned\nThroughput\n//\n-\n\"shared\"\n:\nUse\npay\n-\nas\n-\nyou\n-\ngo\n//\nhttps\n:\n//\ncloud\n.\ngoogle\n.\ncom\n/\nvertex\n-\nai\n/\ngenerative\n-\nai\n/\ndocs\n/\nuse\n-\nprovisioned\n-\nthroughput\n\"X-Vertex-AI-LLM-Request-Type\"\n:\n[]\nstring\n{\n\"shared\"\n},\n},\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\ngenai\n.\nText\n(\n\"How does AI work?\"\n)\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nnil\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nArtificial\nIntelligence\n(\nAI\n)\nisn\n't magic, nor is it a single \"thing.\" Instead, it'\ns\na\nbroad\nfield\nof\ncomputer\nscience\nfocused\non\ncreating\nmachines\nthat\ncan\nperform\ntasks\nthat\ntypically\nrequire\nhuman\nintelligence\n.\n//\n.....\n//\nIn\nSummary\n:\n//\n...\nreturn\nnil\n}\nREST\nAfter you\nset up your environment\n,\nyou can use REST to test a text prompt. The following sample sends a request to the publisher\nmodel endpoint.\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"X-Vertex-AI-LLM-Request-Type: dedicated\"\n\\\n# Options: dedicated, shared\n$URL\n\\\n-d\n'{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Hello.\"}]}]}'\nUse Provisioned Throughput with an API Key\nIf you've purchased Provisioned Throughput for a specific project,\nGoogle model, and region, and want to use it to send a request with an API key,\nthen you must include the project ID, model, location, and API key as parameters\nin your request.\nFor information about how to create a Google Cloud API key bound to a\nservice account, see\nGet a Google Cloud API key\n.\nTo learn how to send requests to the Gemini API using an API key, see\nthe\nGeminiAPI in Vertex AI quickstart\n.\nFor example, the following sample shows\nhow to submit a request with an API key while using Provisioned Throughput:\nREST\nAfter you\nset up your environment\n,\nyou can use REST to test a text prompt. The following sample sends a request to the publisher\nmodel endpoint.\ncurl\n\\\n-X\nPOST\n\\\n-H\n\"Content-Type: application/json\"\n\\\n\"https://aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent?key=\nYOUR_API_KEY\n\"\n\\\n-d\n$'{\n\"contents\": [\n{\n\"role\": \"user\",\n\"parts\": [\n{\n\"text\": \"Explain how AI works in a few words\"\n}\n]\n}\n]\n}'\nMonitor Provisioned Throughput\nYou can self-monitor your Provisioned Throughput usage using a set\nof metrics that are measured on the\naiplatform.googleapis.com/PublisherModel\nresource type.\nProvisioned Throughput traffic monitoring is a public Preview\nfeature.\nDimensions\nYou can filter on metrics using the following dimensions:\nDimension\nValues\ntype\ninput\noutput\nrequest_type\ndedicated\n: Traffic is processed using Provisioned Throughput.\nspillover\n: Traffic is processed as\npay-as-you-go quota after you exceed your Provisioned Throughput\nquota.\nshared\n: If Provisioned Throughput is active,\nthen traffic is processed as pay-as-you-go quota using the shared\nHTTP header\n.\nIf Provisioned Throughput isn't active, then traffic is\nprocessed as pay-as-you-go, by default.\nPath prefix\nThe path prefix for a metric is\naiplatform.googleapis.com/publisher/online_serving\n.\nFor example, the full path for the\n/consumed_throughput\nmetric is\naiplatform.googleapis.com/publisher/online_serving/consumed_throughput\n.\nMetrics\nThe following Cloud Monitoring metrics are available on the\naiplatform.googleapis.com/PublisherModel\nresource for the\nGemini models. Use the\ndedicated\nrequest types to filter for\nProvisioned Throughput usage.\nMetric\nDisplay name\nDescription\n/dedicated_gsu_limit\nLimit (GSU)\nDedicated limit in GSUs. Use this metric to understand your Provisioned Throughput maximum quota in GSUs.\n/tokens\nTokens\nInput and output token count distribution.\n/token_count\nToken count\nAccumulated input and output token count.\n/consumed_token_throughput\nToken throughput\nThroughput usage, which accounts for the burndown rate in tokens and incorporates quota reconciliation. See\nProvisioned Throughput quota checking\n.\nUse this metric to understand how your Provisioned Throughput quota was used.\n/dedicated_token_limit\nLimit (tokens per second)\nDedicated limit in tokens per second. Use this metric to understand your Provisioned Throughput maximum quota for token-based models.\n/characters\nCharacters\nInput and output character count distribution.\n/character_count\nCharacter count\nAccumulated input and output character count.\n/consumed_throughput\nCharacter throughput\nThroughput usage, which accounts for the burndown rate in characters and incorporates quota reconciliation\nProvisioned Throughput quota checking\n.\nUse this metric to understand how your Provisioned Throughput quota was used.\nFor token-based models, this metric is equivalent to the throughput consumed in tokens multiplied by 4.\n/dedicated_character_limit\nLimit (characters per second)\nDedicated limit in characters per second. Use this metric to understand your Provisioned Throughput maximum quota for character-based models.\n/model_invocation_count\nModel invocation count\nNumber of model invocations (prediction requests).\n/model_invocation_latencies\nModel invocation latencies\nModel invocation latencies (prediction latencies).\n/first_token_latencies\nFirst token latencies\nDuration from request received to first token returned.\nAnthropic models also have a filter for Provisioned Throughput but\nonly for\ntokens\nand\ntoken_count\n.\nDashboards\nDefault monitoring dashboards for Provisioned Throughput provide\nmetrics\nthat let you better understand your usage and\nProvisioned Throughput utilization. To access the dashboards, do the\nfollowing:\nIn the Google Cloud console, go to the\nProvisioned Throughput\npage.\nGo to Provisioned Throughput\nTo view the Provisioned Throughput utilization of each model\nacross your orders, select the\nUtilization summary\ntab.\nIn the\nProvisioned Throughput utilization by model\ntable, you can view\nthe following for the selected time range:\nTotal number of GSUs you had.\nPeak throughput usage in terms of GSUs.\nThe average GSU utilization.\nThe number of times you reached your Provisioned Throughput limit.\nSelect a model from the\nProvisioned Throughput utilization by\nmodel\ntable to see more metrics specific to the selected model.\nLimitations of the dashboard\nThe dashboard might display unexpected results, especially for fluctuating\ntraffic that's either spiky or infrequent (for example, less than 1 query per\nsecond). The following reasons might contribute to those results:\nTime ranges that are larger than 12 hours can lead to a less accurate\nrepresentation of the quota enforcement period. Throughput metrics\nand their derivatives, such as utilization, display averages across alignment\nperiods that are based on the selected time range. When the time range\nexpands, each alignment period also expands. The alignment period expands\nacross the calculation of the average usage. Because quota enforcement is\ncalculated at a sub-minute level, setting the time range to a period of 12\nhours or less results in minute-level data that is more comparable to the\nactual quota enforcement period. For more information on alignment periods,\nsee\nAlignment: within-series\nregularization\n. For more\ninformation about time ranges, see\nRegularizing time\nintervals\n.\nIf multiple requests were submitted at the same time, monitoring aggregations\nmight impact your ability to filter down to specific requests.\nProvisioned Throughput throttles traffic when a request was made\nbut reports usage metrics after the quota is reconciled.\nProvisioned Throughput quota enforcement periods are independent\nfrom and might not align with monitoring aggregation periods or\nrequest-or-response periods.\nIf no errors occurred, you might see an error message within the error rate\nchart. For example,\nAn error occurred requesting data. One or more resources\ncould not be found.\nMonitor Genmedia models\nProvisioned Throughput monitoring isn't available on\nVeo 3 and Imagen models.\nAlerting\nAfter alerting is enabled, set default alerts to help you manage your traffic\nusage.\nEnable alerts\nTo enable alerts in the dashboard, do the following:\nIn the Google Cloud console, go to the\nProvisioned Throughput\npage.\nGo to Provisioned Throughput\nTo view the Provisioned Throughput utilization of each model\nacross your orders, select the\nUtilization summary\ntab.\nSelect\nRecommended alerts\n, and the following alerts display:\nProvisioned Throughput Usage Reached Limit\nProvisioned Throughput Utilization Exceeded 80%\nProvisioned Throughput Utilization Exceeded 90%\nCheck the alerts that help you manage your traffic.\nView more alert details\nTo view more information about alerts, do the following:\nGo to the\nIntegrations\npage.\nGo to Integrations\nEnter\nvertex\ninto the\nFilter\nfield and press\nEnter\n.\nGoogle\nVertex AI\nappears.\nTo view more information, click\nView details\n. The\nGoogle\nVertex AI details\npane displays.\nSelect\nAlerts\ntab, and you can select an\nAlert Policy\ntemplate.\nWhat's next\nTroubleshoot\nError code\n429\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Use Provisioned Throughput  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:26:58"
  }
}