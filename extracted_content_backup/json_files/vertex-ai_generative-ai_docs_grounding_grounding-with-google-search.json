{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search",
  "title": "Grounding with Google Search  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGrounding with Google Search\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains how to ground a model's responses using\nGoogle Search, which uses publicly-available web data. Also,\nSearch suggestions are explained, which are included in\nyour responses.\nGrounding with Google Search\nIf you want to connect your model with world knowledge, a wide possible range of\ntopics, or up-to-date information on the Internet, then use Grounding with Google Search.\nTo learn more about model grounding in Vertex AI,\nsee the\nGrounding overview\n.\nSupported models\nThis section lists the models that support grounding with\nSearch.\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nSupported languages\nFor a list of supported languages, see\nLanguages\n.\nGround your model with Google Search\nUse the following instructions to ground a model with publicly available web\ndata.\nConsiderations\nTo use grounding with Google Search, you must enable\nGoogle Search Suggestions. For more information, see\nUse Google Search suggestions\n.\nFor ideal results, use a temperature of\n1.0\n. To learn more about setting\nthis configuration, see the\nGemini API request\nbody\nfrom the model reference.\nGrounding with Google Search has a limit of one million queries per day. If\nyou require more queries, contact\nGoogle Cloud support\nfor assistance.\nSearch results can be customized for a specific geographic location of the end\nuser by using the latitude and longitude coordinates. For more information, see\nthe\nGrounding\nAPI.\nConsole\nTo use Grounding with Google Search with the Vertex AI Studio, follow these steps:\nIn the Google Cloud console, go to the\nVertex AI Studio\npage.\nGo to\nVertex AI Studio\nClick the\nFreeform\ntab.\nIn the side panel, click the\nGround model responses\ntoggle.\nClick\nCustomize\nand set Google Search as the source.\nEnter your prompt in the text box and click\nSubmit\n.\nYour prompt responses now ground to Google Search.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nGenerateContentConfig\n,\nGoogleSearch\n,\nHttpOptions\n,\nTool\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"When is the next total solar eclipse in the United States?\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\n# Use Google Search Tool\nTool\n(\ngoogle_search\n=\nGoogleSearch\n())\n],\n),\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# 'The next total solar eclipse in the United States will occur on ...'\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithGoogleSearch\nshows\nhow\nto\ngenerate\ntext\nusing\nGoogle\nSearch\n.\nfunc\ngenerateWithGoogleSearch\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"When is the next total solar eclipse in the United States?\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nTools\n:\n[]\n*\ngenai\n.\nTool\n{\n{\nGoogleSearch\n:\n&\ngenai\n.\nGoogleSearch\n{}},\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThe\nnext\ntotal\nsolar\neclipse\nin\nthe\nUnited\nStates\nwill\noccur\non\nMarch\n30\n,\n2033\n,\nbut\nit\nwill\nonly\n...\nreturn\nnil\n}\nJava\nLearn how to install or update the\nJava\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.GenerateContentConfig\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.GoogleSearch\n;\nimport\ncom.google.genai.types.HttpOptions\n;\nimport\ncom.google.genai.types.Tool\n;\npublic\nclass\nToolsGoogleSearchWithText\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\nmodelId\n=\n\"gemini-2.5-flash\"\n;\ngenerateContent\n(\nmodelId\n);\n}\n//\nGenerates\ntext\nwith\nGoogle\nSearch\ntool\npublic\nstatic\nString\ngenerateContent\n(\nString\nmodelId\n)\n{\n//\nInitialize\nclient\nthat\nwill\nbe\nused\nto\nsend\nrequests\n.\nThis\nclient\nonly\nneeds\nto\nbe\ncreated\n//\nonce\n,\nand\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nhttpOptions\n(\nHttpOptions\n.\nbuilder\n()\n.\napiVersion\n(\n\"v1\"\n)\n.\nbuild\n())\n.\nbuild\n())\n{\n//\nCreate\na\nGenerateContentConfig\nand\nset\nGoogle\nSearch\ntool\nGenerateContentConfig\ncontentConfig\n=\nGenerateContentConfig\n.\nbuilder\n()\n.\ntools\n(\nTool\n.\nbuilder\n()\n.\ngoogleSearch\n(\nGoogleSearch\n.\nbuilder\n()\n.\nbuild\n())\n.\nbuild\n())\n.\nbuild\n();\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\n\"When is the next total solar eclipse in the United States?\"\n,\ncontentConfig\n);\nSystem\n.\nout\n.\nprint\n(\nresponse\n.\ntext\n());\n//\nExample\nresponse\n:\n//\nThe\nnext\ntotal\nsolar\neclipse\nin\nthe\nUnited\nStates\nwill\noccur\non\n...\nreturn\nresponse\n.\ntext\n();\n}\n}\n}\nREST\nBefore using any of the request data,\nmake the following replacements:\nLOCATION\n: The region to process the request. To use the\nglobal endpoint\n, exclude the location from the endpoint name and configure the location of the resource to global.\nPROJECT_ID\n: Your\nproject ID\n.\nMODEL_ID\n: The model ID of the multimodal model.\nTEXT\n:\nThe text instructions to include in the prompt.\nEXCLUDE_DOMAINS\n: Optional: List of domains that aren't to be used for grounding.\nLATITUDE\n: Optional: The latitude of the end user's location. For example, a latitude of\n37.7749\nrepresents San Francisco. You can obtain latitude and longitude coordinates using services like Google Maps or other geocoding tools.\nLONGITUDE\n: Optional: The longitude of the end user's location. For example, a longitude of\n-122.4194\nrepresents San Francisco.\nHTTP method and URL:\nPOST https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\nRequest JSON body:\n{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"\nTEXT\n\"\n}]\n}],\n\"tools\": [{\n\"googleSearch\": {\n\"exclude_domains\": [ \"domain.com\", \"domain2.com\" ]\n}\n}],\n\"toolConfig\": {\n\"retrievalConfig\": {\n\"latLng\": {\n\"latitude\":\nLATITUDE\n,\n\"longitude\":\nLONGITUDE\n}\n}\n},\n\"model\": \"projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n\"\n}\nTo send your request, expand one of these options:\ncurl (Linux, macOS, or Cloud Shell)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\"\nPowerShell (Windows)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\" | Select-Object -Expand Content\nYou should receive a JSON response similar to the following:\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"text\": \"The weather in Chicago this weekend, will be partly cloudy. The temperature will be between 49°F (9°C) and 55°F (13°C) on Saturday and between 51°F (11°C) and 56°F (13°C) on Sunday. There is a slight chance of rain on both days.\\n\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n\"groundingMetadata\": {\n\"webSearchQueries\": [\n\"weather in Chicago this weekend\"\n],\n\"searchEntryPoint\": {\n\"renderedContent\": \"...\"\n},\n\"groundingChunks\": [\n{\n\"web\": {\n\"uri\": \"https://www.google.com/search?q=weather+in+Chicago,+IL\",\n\"title\": \"Weather information for locality: Chicago, administrative_area: IL\",\n\"domain\": \"google.com\"\n}\n},\n{\n\"web\": {\n\"uri\": \"...\",\n\"title\": \"weatherbug.com\",\n\"domain\": \"weatherbug.com\"\n}\n}\n],\n\"groundingSupports\": [\n{\n\"segment\": {\n\"startIndex\": 85,\n\"endIndex\": 214,\n\"text\": \"The temperature will be between 49°F (9°C) and 55°F (13°C) on Saturday and between 51°F (11°C) and 56°F (13°C) on Sunday.\"\n},\n\"groundingChunkIndices\": [\n0\n],\n\"confidenceScores\": [\n0.8662828\n]\n},\n{\n\"segment\": {\n\"startIndex\": 215,\n\"endIndex\": 261,\n\"text\": \"There is a slight chance of rain on both days.\"\n},\n\"groundingChunkIndices\": [\n1,\n0\n],\n\"confidenceScores\": [\n0.62836814,\n0.6488607\n]\n}\n],\n\"retrievalMetadata\": {}\n}\n}\n],\n\"usageMetadata\": {\n\"promptTokenCount\": 10,\n\"candidatesTokenCount\": 98,\n\"totalTokenCount\": 108,\n\"trafficType\": \"ON_DEMAND\",\n\"promptTokensDetails\": [\n{\n\"modality\": \"TEXT\",\n\"tokenCount\": 10\n}\n],\n\"candidatesTokensDetails\": [\n{\n\"modality\": \"TEXT\",\n\"tokenCount\": 98\n}\n]\n},\n\"modelVersion\": \"gemini-2.0-flash\",\n\"createTime\": \"2025-05-19T14:42:55.000643Z\",\n\"responseId\": \"b0MraIMFoqnf-Q-D66G4BQ\"\n}\nUnderstand your response\nIf your model prompt successfully grounds to Google Search from the\nVertex AI Studio or from the API, then the responses include metadata with\nsource links (web URLs). However, there are several reasons this metadata might\nnot be provided, and the prompt response won't be grounded. These reasons\ninclude low source relevance or incomplete information within the model's\nresponse.\nGrounding support\nDisplaying grounding support is required, because it aids you in validating\nresponses from the publishers and adds avenues for further learning.\nGrounding support for responses from Google Search sources should be shown\nboth inline and in aggregate. For example, see the following image as a\nsuggestion on how to do this.\nUse of alternative search engine options\nWhen using Grounding with Google Search a Customer Application can:\nOffer alternative search engine options,\nMake other search engines the default option,\nDisplay their own or third-party search suggestions or search results as long\nas: any non-Google results must be displayed separately from Google's Grounded\nResults and Search Suggestions, and shown in a way that does not confuse\nusers or suggest they are from Google.\nBenefits\nThe following complex prompts and workflows that require planning, reasoning,\nand thinking can be done when you use Google Search as a tool:\nYou can ground to help ensure responses are based on the latest and most\naccurate information.\nYou can retrieve artifacts from the web to do analysis.\nYou can find relevant images, videos, or other media to assist in multimodal\nreasoning or task generation.\nYou can perform coding, technical troubleshooting, and other specialized\ntasks.\nYou can find region-specific information, or assist in translating content\naccurately.\nYou can find relevant websites for browsing.\nUse Google Search suggestions\nWhen you use grounding with Google Search, and you receive\nSearch suggestions in your response, you must display the\nSearch suggestions in production and in your applications.\nFor more information on grounding with Google Search, see\nGrounding with Google Search\n.\nSpecifically, you must display the search queries that are included in the\ngrounded response's metadata. The response includes:\n\"content\"\n: LLM-generated response.\n\"webSearchQueries\"\n: The queries to be used for\nSearch suggestions.\nFor example, in the following code snippet, Gemini responds to a\nSearch grounded prompt, which is asking about a type of\ntropical plant.\n\"predictions\"\n:\n[\n{\n\"content\"\n:\n\"Monstera is a type of vine that thrives in bright indirect light…\"\n,\n\"groundingMetadata\"\n:\n{\n\"webSearchQueries\"\n:\n[\n\"What's a monstera?\"\n],\n}\n}\n]\nYou can take this output, and display it by using Search\nsuggestions.\nRequirements for Search suggestions\nThe following are requirements for suggestions:\nRequirement\nDescription\nDo\nWhile complying with the\ndisplay\nrequirements\n, the Search suggestion is displayed exactly as provided without any changes.\nWhen you interact with the Search suggestion, you are\ntaken directly to the Search results page (SRP).\nDon't\nInclude any screens or additional steps between the user's tap and the display\nof the SRP.\nDisplay any other search results or suggestions next to the\nSearch suggestion or the associated grounded LLM\nresponse.\nDisplay requirements\nThe following are the display requirements:\nDisplay the Search suggestion exactly as provided, and\ndon't make any modifications to colors, fonts, or appearance. Ensure the\nSearch suggestion renders as specified in the following\nmocks such as light and dark mode:\nWhenever a grounded response is shown, its corresponding\nSearch suggestion should remain visible.\nFor branding, you must strictly follow Google's guidelines for third-party use\nof Google brand features at the\nWelcome to our Brand Resource\nCenter\n.\nWhen you use grounding with Search,\nSearch suggestion chips display. The field that contains\nthe suggestion chips must be the same width as the grounded response from the\nLLM.\nBehavior on tap\nWhen a user taps the chip, they are taken directly to a\nSearch results page (SRP) for the search term displayed in\nthe chip. The SRP can open either within your in-application browser or in a\nseparate browser application. It's important to not minimize, remove, or\nobstruct the SRP's display in any way. The following animated mockup illustrates\nthe tap-to-SRP interaction.\nCode to implement a Search suggestion\nWhen you use the API to ground a response to search, the model response provides\ncompliant HTML and CSS styling in the\nrenderedContent\nfield, which you\nimplement to display Search suggestions in your\napplication.\nWhat's next\nTo learn more about grounding, see\nGrounding overview\n.\nTo learn how to send chat prompt requests, see\nMultiturn chat\n.\nTo learn about responsible AI best practices and Vertex AI's safety filters,\nsee\nSafety best practices\n.\nLearn how to\nsend chat prompt requests\n.\nLearn about\nresponsible AI best practices and Vertex AI safety filters\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Grounding with Google Search  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:26:10"
  }
}