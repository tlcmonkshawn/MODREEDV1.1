{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/streamed-conversations",
  "title": "Interactive conversations with the Live API  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nInteractive conversations with the Live API\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Live API enables low-latency, two-way voice and video interactions\nwith Gemini.\nThis guide covers how to set up a two-way interactive\nconversation, adjust audio settings, manage sessions, and more.\nSupported models\nYou can use the Live API with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash\nPrivate GA\n*\ngemini-live-2.5-flash-preview-native-audio\nPublic preview\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\n*\nReach out to your Google account team representative to request\naccess.\nStart a conversation\nConsole\nOpen\nVertex AI Studio > Stream realtime\n.\nClick\nmic\nStart session\nto initiate the conversation.\nTo end the session, click\nstop_circle\nStop session\n.\nPython\nTo enable real-time, interactive conversations, run the following\nexample on a local computer with microphone and speaker access (not a\nColab notebook). This example sets up a conversation with the API,\nallowing you to send audio prompts and receive audio responses:\n\"\"\"\n# Installation\n# on linux\nsudo apt-get install portaudio19-dev\n# on mac\nbrew install portaudio\npython3 -m venv env\nsource env/bin/activate\npip install google-genai\n\"\"\"\nimport\nasyncio\nimport\npyaudio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nCHUNK\n=\n4200\nFORMAT\n=\npyaudio\n.\npaInt16\nCHANNELS\n=\n1\nRECORD_SECONDS\n=\n5\nMODEL\n=\n'gemini-live-2.5-flash'\nINPUT_RATE\n=\n16000\nOUTPUT_RATE\n=\n24000\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"AUDIO\"\n],\n\"input_audio_transcription\"\n:\n{},\n# Configure input transcription\n\"output_audio_transcription\"\n:\n{},\n# Configure output transcription\n}\nasync\ndef\nmain\n():\nprint\n(\nMODEL\n)\np\n=\npyaudio\n.\nPyAudio\n()\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nMODEL\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\n#exit()\nasync\ndef\nsend\n():\nstream\n=\np\n.\nopen\n(\nformat\n=\nFORMAT\n,\nchannels\n=\nCHANNELS\n,\nrate\n=\nINPUT_RATE\n,\ninput\n=\nTrue\n,\nframes_per_buffer\n=\nCHUNK\n)\nwhile\nTrue\n:\nframe\n=\nstream\n.\nread\n(\nCHUNK\n)\nawait\nsession\n.\nsend\n(\ninput\n=\n{\n\"data\"\n:\nframe\n,\n\"mime_type\"\n:\n\"audio/pcm\"\n})\nawait\nasyncio\n.\nsleep\n(\n10\n**-\n12\n)\nasync\ndef\nreceive\n():\noutput_stream\n=\np\n.\nopen\n(\nformat\n=\nFORMAT\n,\nchannels\n=\nCHANNELS\n,\nrate\n=\nOUTPUT_RATE\n,\noutput\n=\nTrue\n,\nframes_per_buffer\n=\nCHUNK\n)\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\nserver_content\n.\ninput_transcription\n:\nprint\n(\nmessage\n.\nserver_content\n.\nmodel_dump\n(\nmode\n=\n\"json\"\n,\nexclude_none\n=\nTrue\n))\nif\nmessage\n.\nserver_content\n.\noutput_transcription\n:\nprint\n(\nmessage\n.\nserver_content\n.\nmodel_dump\n(\nmode\n=\n\"json\"\n,\nexclude_none\n=\nTrue\n))\nif\nmessage\n.\nserver_content\n.\nmodel_turn\n:\nfor\npart\nin\nmessage\n.\nserver_content\n.\nmodel_turn\n.\nparts\n:\nif\npart\n.\ninline_data\n.\ndata\n:\naudio_data\n=\npart\n.\ninline_data\n.\ndata\noutput_stream\n.\nwrite\n(\naudio_data\n)\nawait\nasyncio\n.\nsleep\n(\n10\n**-\n12\n)\nsend_task\n=\nasyncio\n.\ncreate_task\n(\nsend\n())\nreceive_task\n=\nasyncio\n.\ncreate_task\n(\nreceive\n())\nawait\nasyncio\n.\ngather\n(\nsend_task\n,\nreceive_task\n)\nasyncio\n.\nrun\n(\nmain\n())\nPython\nSet up a conversation with the API that lets you send text prompts and receive audio responses:\n# Set model generation_config\nCONFIG\n=\n{\n\"response_modalities\"\n:\n[\n\"AUDIO\"\n]}\nheaders\n=\n{\n\"Content-Type\"\n:\n\"application/json\"\n,\n\"Authorization\"\n:\nf\n\"Bearer\n{\nbearer_token\n[\n0\n]\n}\n\"\n,\n}\nasync\ndef\nmain\n()\n->\nNone\n:\n# Connect to the server\nasync\nwith\nconnect\n(\nSERVICE_URL\n,\nadditional_headers\n=\nheaders\n)\nas\nws\n:\n# Setup the session\nasync\ndef\nsetup\n()\n->\nNone\n:\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\n{\n\"setup\"\n:\n{\n\"model\"\n:\n\"gemini-live-2.5-flash\"\n,\n\"generation_config\"\n:\nCONFIG\n,\n}\n}\n)\n)\n# Receive setup response\nraw_response\n=\nawait\nws\n.\nrecv\n(\ndecode\n=\nFalse\n)\nsetup_response\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n(\n\"ascii\"\n))\nprint\n(\nf\n\"Connected:\n{\nsetup_response\n}\n\"\n)\nreturn\n# Send text message\nasync\ndef\nsend\n()\n->\nbool\n:\ntext_input\n=\ninput\n(\n\"Input > \"\n)\nif\ntext_input\n.\nlower\n()\nin\n(\n\"q\"\n,\n\"quit\"\n,\n\"exit\"\n):\nreturn\nFalse\nmsg\n=\n{\n\"client_content\"\n:\n{\n\"turns\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[{\n\"text\"\n:\ntext_input\n}]}],\n\"turn_complete\"\n:\nTrue\n,\n}\n}\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\nmsg\n))\nreturn\nTrue\n# Receive server response\nasync\ndef\nreceive\n()\n->\nNone\n:\nresponses\n=\n[]\n# Receive chucks of server response\nasync\nfor\nraw_response\nin\nws\n:\nresponse\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n())\nserver_content\n=\nresponse\n.\npop\n(\n\"serverContent\"\n,\nNone\n)\nif\nserver_content\nis\nNone\n:\nbreak\nmodel_turn\n=\nserver_content\n.\npop\n(\n\"modelTurn\"\n,\nNone\n)\nif\nmodel_turn\nis\nnot\nNone\n:\nparts\n=\nmodel_turn\n.\npop\n(\n\"parts\"\n,\nNone\n)\nif\nparts\nis\nnot\nNone\n:\nfor\npart\nin\nparts\n:\npcm_data\n=\nbase64\n.\nb64decode\n(\npart\n[\n\"inlineData\"\n][\n\"data\"\n])\nresponses\n.\nappend\n(\nnp\n.\nfrombuffer\n(\npcm_data\n,\ndtype\n=\nnp\n.\nint16\n))\n# End of turn\nturn_complete\n=\nserver_content\n.\npop\n(\n\"turnComplete\"\n,\nNone\n)\nif\nturn_complete\n:\nbreak\n# Play the returned audio message\ndisplay\n(\nMarkdown\n(\n\"**Response >**\"\n))\ndisplay\n(\nAudio\n(\nnp\n.\nconcatenate\n(\nresponses\n),\nrate\n=\n24000\n,\nautoplay\n=\nTrue\n))\nreturn\nawait\nsetup\n()\nwhile\nTrue\n:\nif\nnot\nawait\nsend\n():\nbreak\nawait\nreceive\n()\nStart the conversation, input your prompts, or type\nq\n,\nquit\nor\nexit\nto exit.\nawait\nmain\n()\nChange language and voice settings\nThe Live API uses Chirp 3 to support synthesized speech responses in a\nvariety of HD voices and languages. For a full list and demos of each voice, see\nChirp 3: HD voices\n.\nTo set the response voice and language:\nConsole\nOpen\nVertex AI Studio > Stream realtime\n.\nIn the\nOutputs\nexpander, select a voice from the\nVoice\ndrop-down.\nIn the same expander, select a language from the\nLanguage\ndrop-down.\nClick\nmic\nStart session\nto start the session.\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nspeech_config\n=\nSpeechConfig\n(\nvoice_config\n=\nVoiceConfig\n(\nprebuilt_voice_config\n=\nPrebuiltVoiceConfig\n(\nvoice_name\n=\nvoice_name\n,\n)\n),\nlanguage_code\n=\n\"en-US\"\n,\n),\n)\nChange voice activity detection settings\nVoice activity detection\n(VAD) allows the model to recognize when a person is\nspeaking. This is essential for creating natural conversations, as it allows a\nuser to interrupt the model at any time.\nThe model automatically performs voice activity detection (VAD) on a continuous\naudio input stream. You can configure the VAD settings using the\nrealtimeInputConfig.automaticActivityDetection\nfield of the\nsetup\nmessage\n. When VAD detects\nan interruption, the ongoing generation is canceled and discarded. Only the\ninformation already sent to the client is retained in the session history. The\nserver then sends a message to report the interruption.\nIf the audio stream pauses for more than a second (for example, if the user\nturns off the microphone), send an\naudioStreamEnd\nevent to flush any cached audio. The client can resume sending audio data at any\ntime.\nAlternatively, disable automatic VAD by setting\nrealtimeInputConfig.automaticActivityDetection.disabled\nto\ntrue\nin the setup\nmessage. With this configuration, the client detects user speech and sends\nactivityStart\nand\nactivityEnd\nmessages at the appropriate times. An\naudioStreamEnd\nisn't sent. Interruptions\nare marked by\nactivityEnd\n.\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"TEXT\"\n],\nrealtime_input_config\n=\nRealtimeInputConfig\n(\nautomatic_activity_detection\n=\nAutomaticActivityDetection\n(\ndisabled\n=\nFalse\n,\n# default\nstart_of_speech_sensitivity\n=\nStartSensitivity\n.\nSTART_SENSITIVITY_LOW\n,\n# Either START_SENSITIVITY_LOW or START_SENSITIVITY_HIGH\nend_of_speech_sensitivity\n=\nEndSensitivity\n.\nEND_SENSITIVITY_LOW\n,\n# Either END_SENSITIVITY_LOW or END_SENSITIVITY_HIGH\nprefix_padding_ms\n=\n20\n,\nsilence_duration_ms\n=\n100\n,\n)\n),\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nMODEL_ID\n,\nconfig\n=\nconfig\n,\n)\nas\nsession\n:\naudio_bytes\n=\nPath\n(\n\"sample.pcm\"\n)\n.\nread_bytes\n()\nawait\nsession\n.\nsend_realtime_input\n(\nmedia\n=\nBlob\n(\ndata\n=\naudio_bytes\n,\nmime_type\n=\n\"audio/pcm;rate=16000\"\n)\n)\n# if stream gets paused, send:\n# await session.send_realtime_input(audio_stream_end=True)\nresponse\n=\n[]\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\nserver_content\n.\ninterrupted\nis\nTrue\n:\n# The model generation was interrupted\nresponse\n.\nappend\n(\n\"The session was interrupted\"\n)\nif\nmessage\n.\ntext\n:\nresponse\n.\nappend\n(\nmessage\n.\ntext\n)\ndisplay\n(\nMarkdown\n(\nf\n\"**Response >**\n{\n''\n.\njoin\n(\nresponse\n)\n}\n\"\n))\nExtend a session\nThe default maximum length of a conversation session is 10 minutes. A\ngoAway\nnotification\n(\nBidiGenerateContentServerMessage.goAway\n)\nis sent to the client 60 seconds before the session ends.\nYou can extend the session length in 10-minute increments using the\nGen AI SDK. There's no limit to the number of times you can extend\na session. For an example, see\nEnable and disable session\nresumption\n.\nContext window\nThe Live API context window is used to store real-time streamed data\n(25 tokens per second (TPS) for audio and 258 TPS for video) and other content,\nincluding text inputs and model outputs.\nIf the context window exceeds the maximum length (set using the\nMax content\nsize\nslider in Vertex AI Studio, or\ntrigger_tokens\nin the API), the\noldest turns are truncated using\ncontext window compression\nto prevent abrupt\nsession termination. Context window compression triggers once the context window\nhits its maximum length (set either in Vertex AI Studio using the\nTarget context size\nslider, or using\ntarget_tokens\nin the API) and deletes\nthe oldest parts of the conversation until the total token count is back down to\nthis target size.\nFor example, if your maximum context length is set to\n32000\ntokens and your\ntarget size is set to\n16000\ntokens:\nTurn 1:\nThe conversation starts. In this example, the request uses\n12,000 tokens.\nTotal context size: 12,000 tokens\nTurn 2:\nYou make another request. This request uses another 12,000\ntokens.\nTotal context size: 24,000 tokens\nTurn 3:\nYou make another request. This request uses 14,000 tokens.\nTotal context size: 38,000 tokens\nSince the total context size is now higher than the 32,000 token maximum,\ncontext window compression now triggers. The system goes back to the beginning\nof the conversation and starts deleting old turns until the total token size is\nless than the 16,000 token target:\nIt deletes\nTurn 1\n(12,000 tokens). The total is now 26,000 tokens, which\nis still higher than the 16,000 token target.\nIt deletes\nTurn 2\n(12,000 tokens). The total is now 14,000 tokens.\nThe final result is that only\nTurn 3\nremains in active memory, and the\nconversation continues from that point.\nThe minimum and maximum lengths for the context length and target size are:\nSetting (API flag)\nMinimum value\nMaximum value\nMaximum context length (\ntrigger_tokens\n)\n5,000\n128,000\nTarget context size (\ntarget_tokens\n)\n0\n128,000\nTo set the context window:\nConsole\nOpen\nVertex AI Studio > Stream realtime\n.\nClick to open the\nAdvanced\nmenu.\nIn the\nSession Context\nsection, use the\nMax context size\nslider to set the context size to a value between 5,000 and 128,000.\n(Optional) In the same section, use the\nTarget context size\nslider to set the target size to a value between 0 and 128,000.\nPython\nSet the\ncontext_window_compression.trigger_tokens\nand\ncontext_window_compression.sliding_window.target_tokens\nfields in the setup message:\nconfig\n=\ntypes\n.\nLiveConnectConfig\n(\ntemperature\n=\n0.7\n,\nresponse_modalities\n=\n[\n'TEXT'\n],\nsystem_instruction\n=\ntypes\n.\nContent\n(\nparts\n=\n[\ntypes\n.\nPart\n(\ntext\n=\n'test instruction'\n)],\nrole\n=\n'user'\n),\ncontext_window_compression\n=\ntypes\n.\nContextWindowCompressionConfig\n(\ntrigger_tokens\n=\n1000\n,\nsliding_window\n=\ntypes\n.\nSlidingWindow\n(\ntarget_tokens\n=\n10\n),\n),\n)\nConcurrent sessions\nYou can have up to 1,000 concurrent sessions per project.\nUpdate system instructions during a session\nThe Live API lets you update the system instructions during an active\nsession. Use this to adapt the model's responses, such as changing the response\nlanguage or modifying the tone.\nTo update the system instructions mid-session, you can send text content with\nthe\nsystem\nrole. The updated system instruction will remain in effect for the\nremaining session.\nPython\nsession\n.\nsend_client_content\n(\nturns\n=\ntypes\n.\nContent\n(\nrole\n=\n\"system\"\n,\nparts\n=\n[\ntypes\n.\nPart\n(\ntext\n=\n\"new system instruction\"\n)]\n),\nturn_complete\n=\nFalse\n)\nEnable and disable session resumption\nSession resumption lets you reconnect to a previous session within 24 hours.\nThis is achieved by storing cached data, including text, video, audio prompts,\nand model outputs. Project-level privacy is enforced for this cached data.\nBy default, session resumption is disabled.\nTo enable the session resumption feature, set the\nsessionResumption\nfield of\nthe\nBidiGenerateContentSetup\nmessage. If enabled, the server will periodically take a snapshot of the current\ncached session contexts, and store it in the internal storage.\nWhen a snapshot is successfully taken, a\nresumptionUpdate\nis returned with\nthe handle ID that you can record and use later to resume the session from the\nsnapshot.\nHere's an example of enabling session resumption and retrieving the handle ID:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\nasync\ndef\nmain\n():\nprint\n(\nf\n\"Connecting to the service with handle\n{\nprevious_session_handle\n}\n...\"\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\ntypes\n.\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nsession_resumption\n=\ntypes\n.\nSessionResumptionConfig\n(\n# The handle of the session to resume is passed here,\n# or else None to start a new session.\nhandle\n=\nprevious_session_handle\n),\n),\n)\nas\nsession\n:\nwhile\nTrue\n:\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\ntypes\n.\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\ntypes\n.\nPart\n(\ntext\n=\n\"Hello world!\"\n)]\n)\n)\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\n# Periodically, the server will send update messages that may\n# contain a handle for the current state of the session.\nif\nmessage\n.\nsession_resumption_update\n:\nupdate\n=\nmessage\n.\nsession_resumption_update\nif\nupdate\n.\nresumable\nand\nupdate\n.\nnew_handle\n:\n# The handle should be retained and linked to the session.\nreturn\nupdate\n.\nnew_handle\n# For the purposes of this example, placeholder input is continually fed\n# to the model. In non-sample code, the model inputs would come from\n# the user.\nif\nmessage\n.\nserver_content\nand\nmessage\n.\nserver_content\n.\nturn_complete\n:\nbreak\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nTo achieve seamless session resumption, enable\ntransparent mode\n:\nPython\ntypes\n.\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nsession_resumption\n=\ntypes\n.\nSessionResumptionConfig\n(\ntransparent\n=\nTrue\n,\n),\n)\nAfter transparent mode is enabled, the index of the client message that\ncorresponds with the context snapshot is explicitly returned. This helps\nidentify which client message you need to send again, when you resume the\nsession from the resumption handle.\nMore information\nFor more information on using the Live API, see:\nLive API overview\nLive API reference\nguide\nBuilt-in tools\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Interactive conversations with the Live API  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:26:24"
  }
}