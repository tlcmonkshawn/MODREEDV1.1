{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
  "title": "Introduction to function calling  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nIntroduction to function calling\nStay organized with collections\nSave and categorize content based on your preferences.\nFunction calling\n, also known as\ntool use\n, provides the LLM with definitions of external tools (for example, a\nget_current_weather\nfunction). When processing a prompt, the model intelligently determines if a tool is needed and, if so, outputs structured data specifying the tool to call and its parameters (for example,\nget_current_weather(location='Boston')\n). Your application then executes this tool, feeds the result back to the model, allowing it to complete its response with dynamic, real-world information or the outcome of an action. This effectively bridges the LLM with your systems and extends its capabilities.\nFunction calling enables two primary use cases:\nFetching data\n: Retrieve up-to-date information for model responses, such as current weather, currency conversion, or specific data from knowledge bases and APIs (RAG).\nTaking action\n: Perform external operations like submitting forms, updating application state, or orchestrating agentic workflows (e.g., conversation handoffs).\nFor more use cases and examples that are powered by function calling, see\nUse cases\n.\nFeatures and limitations\nThe following models support function calling:\nGemini models:\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nGemini 2.0 Flash-Lite\nOpen models:\nDeepSeek R1-0528\nLlama 4 Maverick\nLlama 4 Scout\nLlama 3.3\nYou can specify up to 512\nFunctionDeclarations\nDefine your functions in the\nOpenAPI schema\nformat.\nFor best practices related to the function declarations, including tips for names and descriptions, see\nBest practices\n.\nFor Open Models, follow this\nuser guide\n.\nHow to create a function calling application\nTo use function calling, perform the following tasks:\nSubmit function declarations and prompt to the model\n.\nProvide the API output to the model\n.\nStep 1: Submit the prompt and function declarations to the model\nDeclare a\nTool\nin a schema format that's compatible with the\nOpenAPI schema\n. For more information, see\nSchema examples\n.\nThe following examples submit a prompt and function declaration to the Gemini models.\nREST\nPROJECT_ID\n=\nmyproject\nLOCATION\n=\nus-central1\nMODEL_ID\n=\ngemini-2.0-flash-001\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"What is the weather in Boston?\"\n}]\n}],\n\"tools\": [{\n\"functionDeclarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city name of the location for which to get the weather.\",\n\"default\": {\n\"string_value\": \"Boston, MA\"\n}\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}]\n}'\nPython\nYou can specify the schema either manually using a Python dictionary or automatically with the\nfrom_func\nhelper function. The following example demonstrates how to declare a function manually.\nimport\nvertexai\nfrom\nvertexai.generative_models\nimport\n(\nContent\n,\nFunctionDeclaration\n,\nGenerationConfig\n,\nGenerativeModel\n,\nPart\n,\nTool\n,\nToolConfig\n)\n# Initialize Vertex AI\n# TODO(developer): Update the project\nvertexai\n.\ninit\n(\nproject\n=\n\"PROJECT_ID\"\n,\nlocation\n=\n\"us-central1\"\n)\n# Initialize Gemini model\nmodel\n=\nGenerativeModel\n(\nmodel_name\n=\n\"gemini-2.0-flash\"\n)\n# Manual function declaration\nget_current_weather_func\n=\nFunctionDeclaration\n(\nname\n=\n\"get_current_weather\"\n,\ndescription\n=\n\"Get the current weather in a given location\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city name of the location for which to get the weather.\"\n,\n\"default\"\n:\n{\n\"string_value\"\n:\n\"Boston, MA\"\n}\n}\n},\n},\n)\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston?\"\n),\n],\n)\n],\ngeneration_config\n=\nGenerationConfig\n(\ntemperature\n=\n0\n),\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n]\n)\nAlternatively, you can declare the function automatically with the\nfrom_func\nhelper function as shown in the following example:\ndef\nget_current_weather\n(\nlocation\n:\nstr\n=\n\"Boston, MA\"\n):\n\"\"\"\nGet the current weather in a given location\nArgs:\nlocation: The city name of the location for which to get the weather.\n\"\"\"\n# This example uses a mock implementation.\n# You can define a local function or import the requests library to call an API\nreturn\n{\n\"location\"\n:\n\"Boston, MA\"\n,\n\"temperature\"\n:\n38\n,\n\"description\"\n:\n\"Partly Cloudy\"\n,\n\"icon\"\n:\n\"partly-cloudy\"\n,\n\"humidity\"\n:\n65\n,\n\"wind\"\n:\n{\n\"speed\"\n:\n10\n,\n\"direction\"\n:\n\"NW\"\n}\n}\nget_current_weather_func\n=\nFunctionDeclaration\n.\nfrom_func\n(\nget_current_weather\n)\nNode.js\nThis example demonstrates a text scenario with one function and one\nprompt.\nNode.js\nBefore trying this sample, follow the\nNode.js\nsetup instructions in the\nVertex AI quickstart using\nclient libraries\n.\nFor more information, see the\nVertex AI\nNode.js\nAPI\nreference documentation\n.\nTo authenticate to Vertex AI, set up Application Default Credentials.\nFor more information, see\nSet up authentication for a local development environment\n.\nconst\n{\nVertexAI\n,\nFunctionDeclarationSchemaType\n,\n}\n=\nrequire\n(\n'\n@google-cloud/vertexai\n'\n);\nconst\nfunctionDeclarations\n=\n[\n{\nfunction_declarations\n:\n[\n{\nname\n:\n'get_current_weather'\n,\ndescription\n:\n'get weather in a given location'\n,\nparameters\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nOBJECT\n,\nproperties\n:\n{\nlocation\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n},\nunit\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n,\nenum\n:\n[\n'celsius'\n,\n'fahrenheit'\n],\n},\n},\nrequired\n:\n[\n'location'\n],\n},\n},\n],\n},\n];\nconst\nfunctionResponseParts\n=\n[\n{\nfunctionResponse\n:\n{\nname\n:\n'get_current_weather'\n,\nresponse\n:\n{\nname\n:\n'get_current_weather'\n,\ncontent\n:\n{\nweather\n:\n'super nice'\n}},\n},\n},\n];\n/**\n* TODO(developer): Update these variables before running the sample.\n*/\nasync\nfunction\nfunctionCallingStreamContent\n(\nprojectId\n=\n'PROJECT_ID'\n,\nlocation\n=\n'us-central1'\n,\nmodel\n=\n'gemini-2.0-flash-001'\n)\n{\n// Initialize Vertex with your Cloud project and location\nconst\nvertexAI\n=\nnew\nVertexAI\n({\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n});\n// Instantiate the model\nconst\ngenerativeModel\n=\nvertexAI\n.\ngetGenerativeModel\n({\nmodel\n:\nmodel\n,\n});\nconst\nrequest\n=\n{\ncontents\n:\n[\n{\nrole\n:\n'user'\n,\nparts\n:\n[{\ntext\n:\n'What is the weather in Boston?'\n}]},\n{\nrole\n:\n'ASSISTANT'\n,\nparts\n:\n[\n{\nfunctionCall\n:\n{\nname\n:\n'get_current_weather'\n,\nargs\n:\n{\nlocation\n:\n'Boston'\n},\n},\n},\n],\n},\n{\nrole\n:\n'USER'\n,\nparts\n:\nfunctionResponseParts\n},\n],\ntools\n:\nfunctionDeclarations\n,\n};\nconst\nstreamingResp\n=\nawait\ngenerativeModel\n.\ngenerateContentStream\n(\nrequest\n);\nfor\nawait\n(\nconst\nitem\nof\nstream\ningResp\n.\nstream\n)\n{\nconsole\n.\nlog\n(\nitem\n.\ncandidates\n[\n0\n].\ncontent\n.\nparts\n[\n0\n].\ntext\n);\n}\n}\nGo\nThis example demonstrates a text scenario with one function and one prompt.\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithFuncCall\nshows\nhow\nto\nsubmit\na\nprompt\nand\na\nfunction\ndeclaration\nto\nthe\nmodel\n,\n//\nallowing\nit\nto\nsuggest\na\ncall\nto\nthe\nfunction\nto\nfetch\nexternal\ndata\n.\nReturning\nthis\ndata\n//\nenables\nthe\nmodel\nto\ngenerate\na\ntext\nresponse\nthat\nincorporates\nthe\ndata\n.\nfunc\ngenerateWithFuncCall\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nweatherFunc\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nDescription\n:\n\"Returns the current weather in a location.\"\n,\nName\n:\n\"getCurrentWeather\"\n,\nParameters\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"object\"\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"location\"\n:\n{\nType\n:\n\"string\"\n},\n},\nRequired\n:\n[]\nstring\n{\n\"location\"\n},\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nTools\n:\n[]\n*\ngenai\n.\nTool\n{\n{\nFunctionDeclarations\n:\n[]\n*\ngenai\n.\nFunctionDeclaration\n{\nweatherFunc\n}},\n},\nTemperature\n:\ngenai\n.\nPtr\n(\nfloat32\n(\n0.0\n)),\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is the weather like in Boston?\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nvar\nfuncCall\n*\ngenai\n.\nFunctionCall\nfor\n_\n,\np\n:=\nrange\nresp\n.\nCandidates\n[\n0\n]\n.\nContent\n.\nParts\n{\nif\np\n.\nFunctionCall\n!=\nnil\n{\nfuncCall\n=\np\n.\nFunctionCall\nfmt\n.\nFprint\n(\nw\n,\n\"The model suggests to call the function \"\n)\nfmt\n.\nFprintf\n(\nw\n,\n\"%q with args: %v\n\\n\n\"\n,\nfuncCall\n.\nName\n,\nfuncCall\n.\nArgs\n)\n//\nExample\nresponse\n:\n//\nThe\nmodel\nsuggests\nto\ncall\nthe\nfunction\n\"getCurrentWeather\"\nwith\nargs\n:\nmap\n[\nlocation\n:\nBoston\n]\n}\n}\nif\nfuncCall\n==\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"model did not suggest a function call\"\n)\n}\n//\nUse\nsynthetic\ndata\nto\nsimulate\na\nresponse\nfrom\nthe\nexternal\nAPI\n.\n//\nIn\na\nreal\napplication\n,\nthis\nwould\ncome\nfrom\nan\nactual\nweather\nAPI\n.\nfuncResp\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\n\"getCurrentWeather\"\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"location\"\n:\n\"Boston\"\n,\n\"temperature\"\n:\n\"38\"\n,\n\"temperature_unit\"\n:\n\"F\"\n,\n\"description\"\n:\n\"Cold and cloudy\"\n,\n\"humidity\"\n:\n\"65\"\n,\n\"wind\"\n:\n`\n{\n\"speed\"\n:\n\"10\"\n,\n\"direction\"\n:\n\"NW\"\n}\n`\n,\n},\n}\n//\nReturn\nconversation\nturns\nand\nAPI\nresponse\nto\ncomplete\nthe\nmodel\n's response.\ncontents\n=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is the weather like in Boston?\"\n},\n},\nRole\n:\n\"user\"\n},\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nFunctionCall\n:\nfuncCall\n},\n}},\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nFunctionResponse\n:\nfuncResp\n},\n}},\n}\nresp\n,\nerr\n=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThe\nweather\nin\nBoston\nis\ncold\nand\ncloudy\nwith\na\ntemperature\nof\n38\ndegrees\nFahrenheit\n.\nThe\nhumidity\nis\n...\nreturn\nnil\n}\nC#\nThis example demonstrates a text scenario with one function and one prompt.\nC#\nBefore trying this sample, follow the\nC#\nsetup instructions in the\nVertex AI quickstart using\nclient libraries\n.\nFor more information, see the\nVertex AI\nC#\nAPI\nreference documentation\n.\nTo authenticate to Vertex AI, set up Application Default Credentials.\nFor more information, see\nSet up authentication for a local development environment\n.\nusing\nGoogle.Cloud.AIPlatform.V1\n;\nusing\nSystem\n;\nusing\nSystem.Threading.Tasks\n;\nusing\nType\n=\nGoogle\n.\nCloud\n.\nAIPlatform\n.\nV1\n.\nType\n;\nusing\nValue\n=\nGoogle\n.\nProtobuf\n.\nWellKnownTypes\n.\nValue\n;\npublic\nclass\nFunctionCalling\n{\npublic\nasync\nTask<string>\nGenerateFunctionCall\n(\nstring\nprojectId\n=\n\"your-project-id\"\n,\nstring\nlocation\n=\n\"us-central1\"\n,\nstring\npublisher\n=\n\"google\"\n,\nstring\nmodel\n=\n\"gemini-2.0-flash-001\"\n)\n{\nvar\npredictionServiceClient\n=\nnew\nPredictionServiceClientBuilder\n{\nEndpoint\n=\n$\"{location}-aiplatform.googleapis.com\"\n}.\nBuild\n();\n// Define the user's prompt in a Content object that we can reuse in\n// model calls\nvar\nuserPromptContent\n=\nnew\nContent\n{\nRole\n=\n\"USER\"\n,\nParts\n=\n{\nnew\nPart\n{\nText\n=\n\"What is the weather like in Boston?\"\n}\n}\n};\n// Specify a function declaration and parameters for an API request\nvar\nfunctionName\n=\n\"get_current_weather\"\n;\nvar\ngetCurrentWeatherFunc\n=\nnew\nFunctionDeclaration\n{\nName\n=\nfunctionName\n,\nDescription\n=\n\"Get the current weather in a given location\"\n,\nParameters\n=\nnew\nOpenApiSchema\n{\nType\n=\nType\n.\nObject\n,\nProperties\n=\n{\n[\"location\"]\n=\nnew\n()\n{\nType\n=\nType\n.\nString\n,\nDescription\n=\n\"Get the current weather in a given location\"\n},\n[\"unit\"]\n=\nnew\n()\n{\nType\n=\nType\n.\nString\n,\nDescription\n=\n\"The unit of measurement for the temperature\"\n,\nEnum\n=\n{\n\"celsius\"\n,\n\"fahrenheit\"\n}\n}\n},\nRequired\n=\n{\n\"location\"\n}\n}\n};\n// Send the prompt and instruct the model to generate content using the tool that you just created\nvar\ngenerateContentRequest\n=\nnew\nGenerateContentRequest\n{\nModel\n=\n$\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\"\n,\nGenerationConfig\n=\nnew\nGenerationConfig\n{\nTemperature\n=\n0f\n},\nContents\n=\n{\nuserPromptContent\n},\nTools\n=\n{\nnew\nTool\n{\nFunctionDeclarations\n=\n{\ngetCurrentWeatherFunc\n}\n}\n}\n};\nGenerateContentResponse\nresponse\n=\nawait\npredictionServiceClient\n.\nGenerateContentAsync\n(\ngenerateContentRequest\n);\nvar\nfunctionCall\n=\nresponse\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n].\nFunctionCall\n;\nConsole\n.\nWriteLine\n(\nfunctionCall\n);\nstring\napiResponse\n=\n\"\"\n;\n// Check the function name that the model responded with, and make an API call to an external system\nif\n(\nfunctionCall\n.\nName\n==\nfunctionName\n)\n{\n// Extract the arguments to use in your API call\nstring\nlocationCity\n=\nfunctionCall\n.\nArgs\n.\nFields\n[\n\"location\"\n].\nStringValue\n;\n// Here you can use your preferred method to make an API request to\n// fetch the current weather\n// In this example, we'll use synthetic data to simulate a response\n// payload from an external API\napiResponse\n=\n@\"{ \"\"location\"\": \"\"Boston, MA\"\",\n\"\"temperature\"\": 38, \"\"description\"\": \"\"Partly Cloudy\"\"}\"\n;\n}\n// Return the API response to Gemini so it can generate a model response or request another function call\ngenerateContentRequest\n=\nnew\nGenerateContentRequest\n{\nModel\n=\n$\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\"\n,\nContents\n=\n{\nuserPromptContent\n,\n// User prompt\nresponse\n.\nCandidates\n[\n0\n].\nContent\n,\n// Function call response,\nnew\nContent\n{\nParts\n=\n{\nnew\nPart\n{\nFunctionResponse\n=\nnew\n()\n{\nName\n=\nfunctionName\n,\nResponse\n=\nnew\n()\n{\nFields\n=\n{\n{\n\"content\"\n,\nnew\nValue\n{\nStringValue\n=\napiResponse\n}\n}\n}\n}\n}\n}\n}\n}\n},\nTools\n=\n{\nnew\nTool\n{\nFunctionDeclarations\n=\n{\ngetCurrentWeatherFunc\n}\n}\n}\n};\nresponse\n=\nawait\npredictionServiceClient\n.\nGenerateContentAsync\n(\ngenerateContentRequest\n);\nstring\nresponseText\n=\nresponse\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n].\nText\n;\nConsole\n.\nWriteLine\n(\nresponseText\n);\nreturn\nresponseText\n;\n}\n}\nJava\nJava\nBefore trying this sample, follow the\nJava\nsetup instructions in the\nVertex AI quickstart using\nclient libraries\n.\nFor more information, see the\nVertex AI\nJava\nAPI\nreference documentation\n.\nTo authenticate to Vertex AI, set up Application Default Credentials.\nFor more information, see\nSet up authentication for a local development environment\n.\nimport\ncom.google.cloud.vertexai.\nVertexAI\n;\nimport\ncom.google.cloud.vertexai.api.\nContent\n;\nimport\ncom.google.cloud.vertexai.api.\nFunctionDeclaration\n;\nimport\ncom.google.cloud.vertexai.api.\nGenerateContentResponse\n;\nimport\ncom.google.cloud.vertexai.api.\nSchema\n;\nimport\ncom.google.cloud.vertexai.api.\nTool\n;\nimport\ncom.google.cloud.vertexai.api.\nType\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nChatSession\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nContentMaker\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nGenerativeModel\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nPartMaker\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nResponseHandler\n;\nimport\njava.io.IOException\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\npublic\nclass\nFunctionCalling\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\nthrows\nIOException\n{\n// TODO(developer): Replace these variables before running the sample.\nString\nprojectId\n=\n\"your-google-cloud-project-id\"\n;\nString\nlocation\n=\n\"us-central1\"\n;\nString\nmodelName\n=\n\"gemini-2.0-flash-001\"\n;\nString\npromptText\n=\n\"What's the weather like in Paris?\"\n;\nwhatsTheWeatherLike\n(\nprojectId\n,\nlocation\n,\nmodelName\n,\npromptText\n);\n}\n// A request involving the interaction with an external tool\npublic\nstatic\nString\nwhatsTheWeatherLike\n(\nString\nprojectId\n,\nString\nlocation\n,\nString\nmodelName\n,\nString\npromptText\n)\nthrows\nIOException\n{\n// Initialize client that will be used to send requests.\n// This client only needs to be created once, and can be reused for multiple requests.\ntry\n(\nVertexAI\nvertexAI\n=\nnew\nVertexAI\n(\nprojectId\n,\nlocation\n))\n{\nFunctionDeclaration\nfunctionDeclaration\n=\nFunctionDeclaration\n.\nnewBuilder\n()\n.\nsetName\n(\n\"getCurrentWeather\"\n)\n.\nsetDescription\n(\n\"Get the current weather in a given location\"\n)\n.\nsetParameters\n(\nSchema\n.\nnewBuilder\n()\n.\nsetType\n(\nType\n.\nOBJECT\n)\n.\nputProperties\n(\n\"location\"\n,\nSchema\n.\nnewBuilder\n()\n.\nsetType\n(\nType\n.\nSTRING\n)\n.\nsetDescription\n(\n\"location\"\n)\n.\nbuild\n()\n)\n.\naddRequired\n(\n\"location\"\n)\n.\nbuild\n()\n)\n.\nbuild\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Function declaration:\"\n);\nSystem\n.\nout\n.\nprintln\n(\nfunctionDeclaration\n);\n// Add the function to a \"tool\"\nTool\ntool\n=\nTool\n.\nnewBuilder\n()\n.\naddFunctionDeclarations\n(\nfunctionDeclaration\n)\n.\nbuild\n();\n// Start a chat session from a model, with the use of the declared function.\nGenerativeModel\nmodel\n=\nnew\nGenerativeModel\n(\nmodelName\n,\nvertexAI\n)\n.\nwithTools\n(\nArrays\n.\nasList\n(\ntool\n));\nChatSession\nchat\n=\nmodel\n.\nstartChat\n();\nSystem\n.\nout\n.\nprintln\n(\nString\n.\nformat\n(\n\"Ask the question: %s\"\n,\npromptText\n));\nGenerateContentResponse\nresponse\n=\nchat\n.\nsendMessage\n(\npromptText\n);\n// The model will most likely return a function call to the declared\n// function `getCurrentWeather` with \"Paris\" as the value for the\n// argument `location`.\nSystem\n.\nout\n.\nprintln\n(\n\"\\nPrint response: \"\n);\nSystem\n.\nout\n.\nprintln\n(\nResponseHandler\n.\ngetContent\n(\nresponse\n));\n// Provide an answer to the model so that it knows what the result\n// of a \"function call\" is.\nContent\ncontent\n=\nContentMaker\n.\nfromMultiModalData\n(\nPartMaker\n.\nfromFunctionResponse\n(\n\"getCurrentWeather\"\n,\nCollections\n.\nsingletonMap\n(\n\"currentWeather\"\n,\n\"sunny\"\n)));\nSystem\n.\nout\n.\nprintln\n(\n\"Provide the function response: \"\n);\nSystem\n.\nout\n.\nprintln\n(\ncontent\n);\nresponse\n=\nchat\n.\nsendMessage\n(\ncontent\n);\n// See what the model replies now\nSystem\n.\nout\n.\nprintln\n(\n\"Print response: \"\n);\nString\nfinalAnswer\n=\nResponseHandler\n.\ngetText\n(\nresponse\n);\nSystem\n.\nout\n.\nprintln\n(\nfinalAnswer\n);\nreturn\nfinalAnswer\n;\n}\n}\n}\nIf the model determines that it needs the output of a particular function, the\nresponse that the application receives from the model contains the function name\nand the parameter values that the function should be called with.\nThe following is an example of a model response to the user prompt \"What is the weather like in Boston?\". The model proposes calling\nthe\nget_current_weather\nfunction with the parameter\nBoston, MA\n.\ncandidates {\ncontent {\nrole: \"model\"\nparts {\nfunction_call {\nname: \"get_current_weather\"\nargs {\nfields {\nkey: \"location\"\nvalue {\nstring_value: \"Boston, MA\"\n}\n}\n}\n}\n}\n}\n...\n}\nStep 2: Provide the API output to the model\nInvoke the external API and pass the API output back to the model.\nThe following example uses synthetic data to simulate a response payload from an\nexternal API and submits the output back to the model.\nREST\nPROJECT_ID\n=\nmyproject\nMODEL_ID\n=\ngemini-2.0-flash\nLOCATION\n=\n\"us-central1\"\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [\n{\n\"role\": \"user\",\n\"parts\": {\n\"text\": \"What is the weather in Boston?\"\n}\n},\n{\n\"role\": \"model\",\n\"parts\": [\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"Boston, MA\"\n}\n}\n}\n]\n},\n{\n\"role\": \"user\",\n\"parts\": [\n{\n\"functionResponse\": {\n\"name\": \"get_current_weather\",\n\"response\": {\n\"temperature\": 20,\n\"unit\": \"C\"\n}\n}\n}\n]\n}\n],\n\"tools\": [\n{\n\"function_declarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a specific location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city name of the location for which to get the weather.\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}\n]\n}'\nPython\nfunction_response_contents\n=\n[]\nfunction_response_parts\n=\n[]\n# Iterates through the function calls in the response in case there are parallel function call requests\nfor\nfunction_call\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\nfunction_calls\n:\nprint\n(\nf\n\"Function call:\n{\nfunction_call\n.\nname\n}\n\"\n)\n# In this example, we'll use synthetic data to simulate a response payload from an external API\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"Boston, MA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"Boston, MA\"\n,\n\"temperature\"\n:\n38\n,\n\"description\"\n:\n\"Partly Cloudy\"\n}\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"San Francisco, CA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"San Francisco, CA\"\n,\n\"temperature\"\n:\n58\n,\n\"description\"\n:\n\"Sunny\"\n}\nfunction_response_parts\n.\nappend\n(\nPart\n.\nfrom_function_response\n(\nname\n=\nfunction_call\n.\nname\n,\nresponse\n=\n{\n\"contents\"\n:\napi_response\n}\n)\n)\n# Add the function call response to the contents\nfunction_response_contents\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\nfunction_response_parts\n)\n# Submit the User's prompt, model's response, and API output back to the model\nresponse\n=\nmodel\n.\ngenerate_content\n(\n[\nContent\n(\n# User prompt\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston?\"\n),\n],\n),\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n,\n# Function call response\nfunction_response_contents\n# API output\n],\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n],\n)\n# Get the model summary response\nprint\n(\nresponse\n.\ntext\n)\nFor best practices related to API invocation, see\nBest practices - API invocation\n.\nIf the model had proposed several parallel function calls, the application must\nprovide all of the responses back to the model. To learn more, see\nParallel function calling example\n.\nThe model may determine that the\noutput of another function is necessary for responding to the prompt. In this case,\nthe response that the application receives from the model contains another\nfunction name and another set of parameter values.\nIf the model determines that the API response is sufficient for responding to\nthe user's prompt, it creates a natural language response and returns it to the\napplication. In this case, the application must pass the response back to the\nuser. The following is an example of a natural language response:\nIt is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies.\nFunction calling with thoughts\nWhen calling functions with\nthinking\nenabled, you'll\nneed to get the\nthought_signature\nfrom\nthe model response object and return it when you send the result of the function\nexecution back to the model. For example:\nPython\n# Call the model with function declarations\n# ...Generation config, Configure the client, and Define user prompt (No changes)\n# Send request with declarations (using a thinking model)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\nconfig\n=\nconfig\n,\ncontents\n=\ncontents\n)\n# See thought signatures\nfor\npart\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n.\nparts\n:\nif\nnot\npart\n.\ntext\n:\ncontinue\nif\npart\n.\nthought\nand\npart\n.\nthought_signature\n:\nprint\n(\n\"Thought signature:\"\n)\nprint\n(\npart\n.\nthought_signature\n)\nViewing thought signatures isn't required, but you will need to adjust\nStep\n2\nto return them along with the result of the function execution\nso it can incorporate the thoughts into its final response:\nPython\n# Create user friendly response with function result and call the model again\n# ...Create a function response part (No change)\n# Append thought signatures, function call and result of the function execution to contents\nfunction_call_content\n=\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n# Append the model's function call message, which includes thought signatures\ncontents\n.\nappend\n(\nfunction_call_content\n)\ncontents\n.\nappend\n(\ntypes\n.\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nfunction_response_part\n]))\n# Append the function response\nfinal_response\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\nconfig\n=\nconfig\n,\ncontents\n=\ncontents\n,\n)\nprint\n(\nfinal_response\n.\ntext\n)\nWhen returning thought signatures, follow these guidelines:\nThe model returns signatures within other parts in the response,\nfor example function calling or text, text, or thought summaries parts.\nReturn the entire response with all parts back to the model in\nsubsequent turns.\nDon't merge part with one signature with another part which also contains a\nsignature. Signatures can't be concatenated together.\nDon't merge one part with a signature with another part without a signature.\nThis breaks the correct positioning of the thought represented by the\nsignature.\nLearn more about limitations and usage of thought signatures, and about thinking\nmodels in general, on the\nThinking\npage.\nParallel function calling\nFor prompts such as \"Get weather details in Boston and San Francisco?\",\nthe model may propose several parallel function calls. For a list of models that\nsupport parallel function calling, see\nSupported models\n.\nREST\nThis example demonstrates a scenario with one\nget_current_weather\nfunction.\nThe user prompt is \"Get weather details in Boston and San Francisco?\". The\nmodel proposes two parallel\nget_current_weather\nfunction calls: one with the\nparameter\nBoston\nand the other with the parameter\nSan Francisco\n.\nTo learn more about the request parameters, see\nGemini API\n.\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"Boston\"\n}\n}\n},\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"San Francisco\"\n}\n}\n}\n]\n},\n...\n}\n],\n...\n}\nThe following command demonstrates how you can provide the function output to\nthe model. Replace\nmy-project\nwith the name of your Google Cloud project.\nModel request\nPROJECT_ID=\nmy-project\nMODEL_ID=gemini-2.0-flash\nLOCATION=\"us-central1\"\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent \\\n-d '{\n\"contents\": [\n{\n\"role\": \"user\",\n\"parts\": {\n\"text\": \"What is difference in temperature in Boston and San Francisco?\"\n}\n},\n{\n\"role\": \"model\",\n\"parts\": [\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"Boston\"\n}\n}\n},\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"San Francisco\"\n}\n}\n}\n]\n},\n{\n\"role\": \"user\",\n\"parts\": [\n{\n\"functionResponse\": {\n\"name\": \"get_current_weather\",\n\"response\": {\n\"temperature\": 30.5,\n\"unit\": \"C\"\n}\n}\n},\n{\n\"functionResponse\": {\n\"name\": \"get_current_weather\",\n\"response\": {\n\"temperature\": 20,\n\"unit\": \"C\"\n}\n}\n}\n]\n}\n],\n\"tools\": [\n{\n\"function_declarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a specific location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city name of the location for which to get the weather.\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}\n]\n}'\nThe natural language response created by the model is similar to the following:\nModel response\n[\n{\n\"candidates\": [\n{\n\"content\": {\n\"parts\": [\n{\n\"text\": \"The temperature in Boston is 30.5C and the temperature in San Francisco is 20C. The difference is 10.5C. \\n\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n...\n}\n]\n...\n}\n]\nPython\nThis example demonstrates a scenario with one\nget_current_weather\nfunction.\nThe user prompt is \"What is the weather like in Boston and San Francisco?\".\nReplace\nmy-project\nwith the name of your Google Cloud project.\nimport\nvertexai\nfrom\nvertexai.generative_models\nimport\n(\nContent\n,\nFunctionDeclaration\n,\nGenerationConfig\n,\nGenerativeModel\n,\nPart\n,\nTool\n,\nToolConfig\n)\n# Initialize Vertex AI\n# TODO(developer): Update the project\nvertexai\n.\ninit\n(\nproject\n=\n\"my-project\"\n,\nlocation\n=\n\"us-central1\"\n)\n# Initialize Gemini model\nmodel\n=\nGenerativeModel\n(\nmodel_name\n=\n\"gemini-2.0-flash\"\n)\n# Manual function declaration\nget_current_weather_func\n=\nFunctionDeclaration\n(\nname\n=\n\"get_current_weather\"\n,\ndescription\n=\n\"Get the current weather in a given location\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city name of the location for which to get the weather.\"\n,\n\"default\"\n:\n{\n\"string_value\"\n:\n\"Boston, MA\"\n}\n}\n},\n},\n)\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston and San Francisco?\"\n),\n],\n)\n],\ngeneration_config\n=\nGenerationConfig\n(\ntemperature\n=\n0\n),\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n]\n)\nThe following command demonstrates how you can provide the function output to\nthe model.\nfunction_response_contents\n=\n[]\nfunction_response_parts\n=\n[]\n# You can have parallel function call requests for the same function type.\n# For example, 'location_to_lat_long(\"London\")' and 'location_to_lat_long(\"Paris\")'\n# In that case, collect API responses in parts and send them back to the model\nfor\nfunction_call\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\nfunction_calls\n:\nprint\n(\nf\n\"Function call:\n{\nfunction_call\n.\nname\n}\n\"\n)\n# In this example, we'll use synthetic data to simulate a response payload from an external API\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"Boston, MA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"Boston, MA\"\n,\n\"temperature\"\n:\n38\n,\n\"description\"\n:\n\"Partly Cloudy\"\n}\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"San Francisco, CA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"San Francisco, CA\"\n,\n\"temperature\"\n:\n58\n,\n\"description\"\n:\n\"Sunny\"\n}\nfunction_response_parts\n.\nappend\n(\nPart\n.\nfrom_function_response\n(\nname\n=\nfunction_call\n.\nname\n,\nresponse\n=\n{\n\"contents\"\n:\napi_response\n}\n)\n)\n# Add the function call response to the contents\nfunction_response_contents\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\nfunction_response_parts\n)\nfunction_response_contents\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston and San Francisco?\"\n),\n],\n),\n# User prompt\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n,\n# Function call response\nfunction_response_contents\n,\n# Function response\n],\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n]\n)\n# Get the model summary response\nprint\n(\nresponse\n.\ntext\n)\nGo\nimport\n(\n\"context\"\n\"encoding/json\"\n\"errors\"\n\"fmt\"\n\"io\"\n\"cloud.google.com/go/vertexai/genai\"\n)\n// parallelFunctionCalling shows how to execute multiple function calls in parallel\n// and return their results to the model for generating a complete response.\nfunc\nparallelFunctionCalling\n(\nw\nio\n.\nWriter\n,\nprojectID\n,\nlocation\n,\nmodelName\nstring\n)\nerror\n{\n// location = \"us-central1\"\n// modelName = \"gemini-2.0-flash-001\"\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\nprojectID\n,\nlocation\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create GenAI client: %w\"\n,\nerr\n)\n}\ndefer\nclient\n.\nClose\n()\nmodel\n:=\nclient\n.\nGenerativeModel\n(\nmodelName\n)\n// Set temperature to 0.0 for maximum determinism in function calling.\nmodel\n.\nSetTemperature\n(\n0.0\n)\nfuncName\n:=\n\"getCurrentWeather\"\nfuncDecl\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nName\n:\nfuncName\n,\nDescription\n:\n\"Get the current weather in a given location\"\n,\nParameters\n:\n&\ngenai\n.\nSchema\n{\nType\n:\ngenai\n.\nTypeObject\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"location\"\n:\n{\nType\n:\ngenai\n.\nTypeString\n,\nDescription\n:\n\"The location for which to get the weather. \"\n+\n\"It can be a city name, a city name and state, or a zip code. \"\n+\n\"Examples: 'San Francisco', 'San Francisco, CA', '95616', etc.\"\n,\n},\n},\nRequired\n:\n[]\nstring\n{\n\"location\"\n},\n},\n}\n// Add the weather function to our model toolbox.\nmodel\n.\nTools\n=\n[]\n*\ngenai\n.\nTool\n{\n{\nFunctionDeclarations\n:\n[]\n*\ngenai\n.\nFunctionDeclaration\n{\nfuncDecl\n},\n},\n}\nprompt\n:=\ngenai\n.\nText\n(\n\"Get weather details in New Delhi and San Francisco?\"\n)\nresp\n,\nerr\n:=\nmodel\n.\nGenerateContent\n(\nctx\n,\nprompt\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"got empty response from model\"\n)\n}\nelse\nif\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nFunctionCalls\n())\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"got no function call suggestions from model\"\n)\n}\n// In a production environment, consider adding validations for function names and arguments.\nfor\n_\n,\nfnCall\n:=\nrange\nresp\n.\nCandidates\n[\n0\n].\nFunctionCalls\n()\n{\nfmt\n.\nFprintf\n(\nw\n,\n\"The model suggests to call the function %q with args: %v\\n\"\n,\nfnCall\n.\nName\n,\nfnCall\n.\nArgs\n)\n// Example response:\n// The model suggests to call the function \"getCurrentWeather\" with args: map[location:New Delhi]\n// The model suggests to call the function \"getCurrentWeather\" with args: map[location:San Francisco]\n}\n// Use synthetic data to simulate responses from the external API.\n// In a real application, this would come from an actual weather API.\nmockAPIResp1\n,\nerr\n:=\njson\n.\nMarshal\n(\nmap\n[\nstring\n]\nstring\n{\n\"location\"\n:\n\"New Delhi\"\n,\n\"temperature\"\n:\n\"42\"\n,\n\"temperature_unit\"\n:\n\"C\"\n,\n\"description\"\n:\n\"Hot and humid\"\n,\n\"humidity\"\n:\n\"65\"\n,\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to marshal function response to JSON: %w\"\n,\nerr\n)\n}\nmockAPIResp2\n,\nerr\n:=\njson\n.\nMarshal\n(\nmap\n[\nstring\n]\nstring\n{\n\"location\"\n:\n\"San Francisco\"\n,\n\"temperature\"\n:\n\"36\"\n,\n\"temperature_unit\"\n:\n\"F\"\n,\n\"description\"\n:\n\"Cold and cloudy\"\n,\n\"humidity\"\n:\n\"N/A\"\n,\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to marshal function response to JSON: %w\"\n,\nerr\n)\n}\n// Note, that the function calls don't have to be chained. We can obtain both responses in parallel\n// and return them to Gemini at once.\nfuncResp1\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\nfuncName\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"content\"\n:\nmockAPIResp1\n,\n},\n}\nfuncResp2\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\nfuncName\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"content\"\n:\nmockAPIResp2\n,\n},\n}\n// Return both API responses to the model allowing it to complete its response.\nresp\n,\nerr\n=\nmodel\n.\nGenerateContent\n(\nctx\n,\nprompt\n,\nfuncResp1\n,\nfuncResp2\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n||\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"got empty response from model\"\n)\n}\nfmt\n.\nFprintln\n(\nw\n,\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n])\n// Example response:\n// The weather in New Delhi is hot and humid with a humidity of 65 and a temperature of 42°C. The weather in San Francisco ...\nreturn\nnil\n}\nFunction calling modes\nYou can control how the model uses the provided tools (function declarations) by setting the mode within the\nfunction_calling_config\n.\nMode\nDescription\nAUTO\nThe default model behavior. The model decides whether to predict function calls or respond with natural language based on the context. This is the most flexible mode and recommended for most scenarios.\nVALIDATED\n(Preview)\nThe model is constrained to predict either function calls or natural language, and ensures function schema adherence. If\nallowed_function_names\nis not provided, the model picks from all of the available function declarations. If\nallowed_function_names\nis provided, the model picks from the set of allowed functions.\nANY\nThe model is constrained to always predict one or more function calls and ensures function schema adherence. If\nallowed_function_names\nis not provided, the model picks from all of the available function declarations. If\nallowed_function_names\nis provided, the model picks from the set of allowed functions. Use this mode when you require a function call response to every prompt (if applicable).\nNONE\nThe model is\nprohibited\nfrom making function calls. This is equivalent to sending a request without any function declarations. Use this mode to temporarily disable function calling without removing your tool definitions.\nForced function calling\nInstead of allowing the model to choose between a natural language response and a function call, you can force it to only predict function calls. This is known as\nforced function calling\n. You can also choose to provide the model with a full set of function declarations, but restrict its responses to a subset of these functions.\nThe following example is forced to predict only\nget_weather\nfunction calls.\nPython\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston?\"\n),\n],\n)\n],\ngeneration_config\n=\nGenerationConfig\n(\ntemperature\n=\n0\n),\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_weather_func\n,\nsome_other_function\n],\n)\n],\ntool_config\n=\nToolConfig\n(\nfunction_calling_config\n=\nToolConfig\n.\nFunctionCallingConfig\n(\n# ANY mode forces the model to predict only function calls\nmode\n=\nToolConfig\n.\nFunctionCallingConfig\n.\nMode\n.\nANY\n,\n# Allowed function calls to predict when the mode is ANY. If empty, any of\n# the provided function calls will be predicted.\nallowed_function_names\n=\n[\n\"get_weather\"\n],\n)\n)\n)\nFunction schema examples\nFunction declarations are compatible with the\nOpenAPI schema\n. We support the following attributes:\ntype\n,\nnullable\n,\nrequired\n,\nformat\n,\ndescription\n,\nproperties\n,\nitems\n,\nenum\n,\nanyOf\n,\n$ref\n, and\n$defs\n. Remaining attributes are not supported.\nFunction with object and array parameters\nThe following example uses a Python dictionary to declare a function that takes both object and array parameters:\nextract_sale_records_func\n=\nFunctionDeclaration\n(\nname\n=\n\"extract_sale_records\"\n,\ndescription\n=\n\"Extract sale records from a document.\"\n,\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"records\"\n:\n{\n\"type\"\n:\n\"array\"\n,\n\"description\"\n:\n\"A list of sale records\"\n,\n\"items\"\n:\n{\n\"description\"\n:\n\"Data for a sale record\"\n,\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"id\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"The unique id of the sale.\"\n},\n\"date\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Date of the sale, in the format of MMDDYY, e.g., 031023\"\n},\n\"total_amount\"\n:\n{\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"The total amount of the sale.\"\n},\n\"customer_name\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The name of the customer, including first name and last name.\"\n},\n\"customer_contact\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The phone number of the customer, e.g., 650-123-4567.\"\n},\n},\n\"required\"\n:\n[\n\"id\"\n,\n\"date\"\n,\n\"total_amount\"\n],\n},\n},\n},\n\"required\"\n:\n[\n\"records\"\n],\n},\n)\nFunction with enum parameter\nThe following example uses a Python dictionary to declare a function that takes an integer\nenum\nparameter:\nset_status_func\n=\nFunctionDeclaration\n(\nname\n=\n\"set_status\"\n,\ndescription\n=\n\"set a ticket's status field\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"status\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"enum\"\n:\n[\n\"10\"\n,\n\"20\"\n,\n\"30\"\n],\n# Provide integer (or any other type) values as strings.\n}\n},\n},\n)\nFunction with ref and def\nThe following JSON function declaration uses the\nref\nand\ndefs\nattributes:\n{\n\"contents\"\n:\n...\n,\n\"tools\"\n:\n[\n{\n\"function_declarations\"\n:\n[\n{\n\"name\"\n:\n\"get_customer\"\n,\n\"description\"\n:\n\"Search for a customer by name\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"first_name\"\n:\n{\n\"ref\"\n:\n\"#/defs/name\"\n},\n\"last_name\"\n:\n{\n\"ref\"\n:\n\"#/defs/name\"\n}\n},\n\"defs\"\n:\n{\n\"name\"\n:\n{\n\"type\"\n:\n\"string\"\n}\n}\n}\n}\n]\n}\n]\n}\nUsage notes:\nUnlike, the OpenAPI schema, specify\nref\nand\ndefs\nwithout the\n$\nsymbol.\nref\nmust refer to direct child of\ndefs\n; no\nexternal references.\nThe maximum depth of nested schema is 32.\nRecursion depth in\ndefs\n(self-reference) is limited to two.\nfrom_func\nwith array parameter\nThe following code sample declares a function that multiplies an array of numbers and uses\nfrom_func\nto generate the\nFunctionDeclaration\nschema.\nfrom\ntyping\nimport\nList\n# Define a function. Could be a local function or you can import the requests library to call an API\ndef\nmultiply_numbers\n(\nnumbers\n:\nList\n[\nint\n]\n=\n[\n1\n,\n1\n])\n->\nint\n:\n\"\"\"\nCalculates the product of all numbers in an array.\nArgs:\nnumbers: An array of numbers to be multiplied.\nReturns:\nThe product of all the numbers. If the array is empty, returns 1.\n\"\"\"\nif\nnot\nnumbers\n:\n# Handle empty array\nreturn\n1\nproduct\n=\n1\nfor\nnum\nin\nnumbers\n:\nproduct\n*=\nnum\nreturn\nproduct\nmultiply_number_func\n=\nFunctionDeclaration\n.\nfrom_func\n(\nmultiply_numbers\n)\n\"\"\"\nmultiply_number_func contains the following schema:\n{'name': 'multiply_numbers',\n'description': 'Calculates the product of all numbers in an array.',\n'parameters': {'properties': {'numbers': {'items': {'type': 'INTEGER'},\n'description': 'list of numbers',\n'default': [1.0, 1.0],\n'title': 'Numbers',\n'type': 'ARRAY'}},\n'description': 'Calculates the product of all numbers in an array.',\n'title': 'multiply_numbers',\n'property_ordering': ['numbers'],\n'type': 'OBJECT'}}\n\"\"\"\nBest practices for function calling\nWrite clear and detailed function names, parameter descriptions, and instructions\nFunction names should start with a letter or an underscore and contain only characters a-z, A-Z, 0-9, underscores, dots or dashes with a maximum length of 64.\nBe extremely clear and specific in your function and parameter descriptions.\nThe model relies on these to choose the correct function and provide\nappropriate arguments. For example, a\nbook_flight_ticket\nfunction could\nhave the description\nbook flight tickets after confirming users' specific requirements, such as time, departure, destination, party size and preferred airline\nUse strong typed parameters\nIf the parameter values are from a finite set, add an\nenum\nfield instead of putting the set of values into the description. If the parameter value is always an integer, set the type to\ninteger\nrather than\nnumber\n.\nTool Selection\nWhile the model can use an arbitrary number of tools, providing too many can\nincrease the risk of selecting an incorrect or suboptimal tool. For best\nresults, aim to provide only the relevant tools for the context or task,\nideally keeping the active set to a maximum of 10-20. If you have a large\ntotal number of tools, consider dynamic tool selection based on\nconversation context.\nIf you provide generic, low-level tools (like\nbash\n) the model might use the tool\nmore often, but with less accuracy. If you provide a specific, high-level tool\n(like\nget_weather\n), the model will be able to use the tool more accurately, but\nthe tool might not be used as often.\nUse system instructions\nWhen using functions with date, time, or location parameters, include the\ncurrent date, time, or relevant location information (for example, city and\ncountry) in the system instruction. This provides the model with the necessary\ncontext to process the request accurately, even if the user's prompt lacks\ndetails.\nPrompt engineering\nFor best results, prepend the user prompt with the following details:\nAdditional context for the model-for example,\nYou are a flight API assistant to help with searching flights based on user preferences.\nDetails or instructions on how and when to use the functions-for example,\nDon't make assumptions on the departure or destination airports. Always use a future date for the departure or destination time.\nInstructions to ask clarifying questions if user queries are ambiguous-for example,\nAsk clarifying questions if not enough information is available.\nUse generation configuration\nFor the temperature parameter, use\n0\nor another low value. This instructs\nthe model to generate more confident results and reduces hallucinations.\nUse structured output\nFunction calling can be used together with\nstructured output\nto let the model always predict function calls or outputs that adheres to a specific schema, so that you receive consistently formatted responses when model does not generate function calls.\nValidate the API call\nIf the model proposes the invocation of a function that would send an order,\nupdate a database, or otherwise have significant consequences, validate the\nfunction call with the user before executing it.\nUse thought signatures\nThought signatures\nshould always be used\nwith function calling for best results.\nPricing\nThe pricing for function calling is based on the number of characters within the\ntext inputs and outputs. To learn more, see\nVertex AI pricing\n.\nHere, text input (prompt)\nrefers to the user prompt for the current conversation turn, the function\ndeclarations for the current conversation turn, and the history of the\nconversation. The history of the conversation includes the queries, the function\ncalls, and the function responses of previous conversation turns.\nVertex AI truncates the history of the conversation at 32,000 characters.\nText output (response) refers to the function calls and the text responses\nfor the current conversation turn.\nUse cases of function calling\nYou can use function calling for the following tasks:\nUse Case\nExample description\nExample link\nIntegrate with external APIs\nGet weather information using a meteorological API\nNotebook tutorial\nConvert addresses to latitude/longitude coordinates\nNotebook tutorial\nConvert currencies using a currency exchange API\nCodelab\nBuild advanced chatbots\nAnswer customer questions about products and services\nNotebook tutorial\nCreate an assistant to answer financial and news questions about companies\nNotebook tutorial\nStructure and control function calls\nExtract structured entities from raw log data\nNotebook tutorial\nExtract single or multiple parameters from user input\nNotebook tutorial\nHandle lists and nested data structures in function calls\nNotebook tutorial\nHandle function calling behavior\nHandle parallel function calls and responses\nNotebook tutorial\nManage when and which functions the model can call\nNotebook tutorial\nQuery databases with natural language\nConvert natural language questions into SQL queries for BigQuery\nSample app\nMultimodal function calling\nUse images, videos, audio, and PDFs as input to trigger function calls\nNotebook tutorial\nHere are some more use cases:\nInterpret voice commands\n: Create functions that correspond with\nin-vehicle tasks. For example, you can create functions that turn on the\nradio or activate the air conditioning. Send audio files of the user's voice\ncommands to the model, and ask the model to convert the audio into text and\nidentify the function that the user wants to call.\nAutomate workflows based on environmental triggers\n: Create functions to\nrepresent processes that can be automated. Provide the model with data from\nenvironmental sensors and ask it to parse and process the data to determine\nwhether one or more of the workflows should be activated. For example, a\nmodel could process temperature data in a warehouse and choose to activate a\nsprinkler function.\nAutomate the assignment of support tickets\n: Provide the model with\nsupport tickets, logs, and context-aware rules. Ask the model to process all\nof this information to determine who the ticket should be assigned to. Call\na function to assign the ticket to the person suggested by the model.\nRetrieve information from a knowledge base\n: Create functions that\nretrieve academic articles on a given subject and summarize them. Enable the\nmodel to answer questions about academic subjects and provide citations for\nits answers.\nWhat's next\nSee the\nAPI reference for function calling\n.\nLearn about\nVertex AI Agent Engine\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Introduction to function calling  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:25:56"
  }
}