{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live",
  "title": "Live API reference  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nAPI reference\nSend feedback\nLive API reference\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Live API enables low-latency bidirectional voice and video\ninteractions with Gemini. Using the Live API, you can\nprovide end users with the experience of natural, human-like voice\nconversations, and with the ability to interrupt the model's responses using\nvoice commands. The Live API\ncan process text, audio, and video input, and it can provide text and audio\noutput.\nFor more information about the Live API, see\nLive API\n.\nCapabilities\nLive API includes the following key capabilities:\nMultimodality\n: The model can see, hear, and speak.\nLow-latency realtime interaction\n: The model can provide fast responses.\nSession memory\n: The model retains memory of all interactions\nwithin a single session, recalling previously heard or seen information.\nSupport for function calling, code execution, and Search as a Tool\n:\nYou can integrate the model with external services and data sources.\nLive API is designed for server-to-server communication.\nFor web and mobile apps, we recommend using the integration from our partners\nat\nDaily\n.\nSupported models\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGet started\nTo try the Live API, go to the\nVertex AI Studio\n,\nand then click\nStart Session\n.\nLive API is a stateful API that uses\nWebSockets\n.\nThis section shows an example of how to use Live API\nfor text-to-text generation, using Python 3.9+.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nContent\n,\nHttpOptions\n,\nLiveConnectConfig\n,\nModality\n,\nPart\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1beta1\"\n))\nmodel_id\n=\n\"gemini-2.0-flash-live-preview-04-09\"\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel_id\n,\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\nModality\n.\nTEXT\n]),\n)\nas\nsession\n:\ntext_input\n=\n\"Hello? Gemini, are you there?\"\nprint\n(\n\"> \"\n,\ntext_input\n,\n\"\n\\n\n\"\n)\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n(\ntext\n=\ntext_input\n)])\n)\nresponse\n=\n[]\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\ntext\n:\nresponse\n.\nappend\n(\nmessage\n.\ntext\n)\nprint\n(\n\"\"\n.\njoin\n(\nresponse\n))\n# Example output:\n# >  Hello? Gemini, are you there?\n# Yes, I'm here. What would you like to talk about?\nIntegration guide\nThis section describes how integration works with Live API.\nSessions\nA WebSocket connection establishes a session between the client and the\nGemini server.\nAfter a client initiates a new connection the session can exchange messages\nwith the server to:\nSend text, audio, or video to the Gemini server.\nReceive audio, text, or function call requests from the\nGemini server.\nThe session configuration is sent in the first message after connection.\nA session configuration includes the model, generation parameters,\nsystem instructions, and tools.\nSee the following example configuration:\n{\n\"model\": string,\n\"generationConfig\": {\n\"candidateCount\": integer,\n\"maxOutputTokens\": integer,\n\"temperature\": number,\n\"topP\": number,\n\"topK\": integer,\n\"presencePenalty\": number,\n\"frequencyPenalty\": number,\n\"responseModalities\": [string],\n\"speechConfig\": object\n},\n\"systemInstruction\": string,\n\"tools\": [object]\n}\nFor more information, see\nBidiGenerateContentSetup\n.\nSend messages\nMessages are JSON-formatted objects exchanged over the WebSocket connection.\nTo send a message the client must send a JSON object over an open WebSocket\nconnection. The JSON object must have\nexactly one\nof the fields from the\nfollowing object set:\n{\n\"setup\": BidiGenerateContentSetup,\n\"clientContent\": BidiGenerateContentClientContent,\n\"realtimeInput\": BidiGenerateContentRealtimeInput,\n\"toolResponse\": BidiGenerateContentToolResponse\n}\nSupported client messages\nSee the supported client messages in the following table:\nMessage\nDescription\nBidiGenerateContentSetup\nSession configuration to be sent in the first message\nBidiGenerateContentClientContent\nIncremental content update of the current conversation delivered from the client\nBidiGenerateContentRealtimeInput\nReal time audio or video input\nBidiGenerateContentToolResponse\nResponse to a\nToolCallMessage\nreceived from the server\nReceive messages\nTo receive messages from Gemini, listen for the WebSocket\n'message' event, and then parse the result according to the definition of the\nsupported server messages.\nSee the following:\nws\n.\naddEventListener\n(\n\"message\"\n,\nasync\n(\nevt\n)\n=>\n{\nif\n(\nevt\n.\ndata\ninstanceof\nBlob\n)\n{\n//\nProcess\nthe\nreceived\ndata\n(\naudio\n,\nvideo\n,\netc\n.\n)\n}\nelse\n{\n//\nProcess\nJSON\nresponse\n}\n});\nServer messages will have\nexactly one\nof the fields from the following object\nset:\n{\n\"setupComplete\": BidiGenerateContentSetupComplete,\n\"serverContent\": BidiGenerateContentServerContent,\n\"toolCall\": BidiGenerateContentToolCall,\n\"toolCallCancellation\": BidiGenerateContentToolCallCancellation\n\"usageMetadata\": UsageMetadata\n\"goAway\": GoAway\n\"sessionResumptionUpdate\": SessionResumptionUpdate\n\"inputTranscription\": BidiGenerateContentTranscription\n\"outputTranscription\": BidiGenerateContentTranscription\n}\nSupported server messages\nSee the supported server messages in the following table:\nMessage\nDescription\nBidiGenerateContentSetupComplete\nA\nBidiGenerateContentSetup\nmessage from the client, sent when setup is complete\nBidiGenerateContentServerContent\nContent generated by the model in response to a client message\nBidiGenerateContentToolCall\nRequest for the client to run the function calls and return the responses with the matching IDs\nBidiGenerateContentToolCallCancellation\nSent when a function call is canceled due to the user interrupting model output\nUsageMetadata\nA report of the number of tokens used by the session so far\nGoAway\nA signal that the current connection will soon be terminated\nSessionResumptionUpdate\nA session checkpoint, which can be resumed\nBidiGenerateContentTranscription\nA transcription of either the user's or model's speech\nIncremental content updates\nUse incremental updates to send text input, establish session context, or\nrestore session context. For short contexts you can send turn-by-turn\ninteractions to represent the exact sequence of events. For longer contexts\nit's recommended to provide a single message summary to free up the\ncontext window for the follow up interactions.\nSee the following example context message:\n{\n\"clientContent\"\n:\n{\n\"turns\"\n:\n[\n{\n\"parts\"\n:[\n{\n\"text\"\n:\n\"\"\n}\n],\n\"role\"\n:\n\"user\"\n},\n{\n\"parts\"\n:[\n{\n\"text\"\n:\n\"\"\n}\n],\n\"role\"\n:\n\"model\"\n}\n],\n\"turnComplete\"\n:\ntrue\n}\n}\nNote that while content parts can be of a\nfunctionResponse\ntype,\nBidiGenerateContentClientContent\nshouldn't be used to provide a response\nto the function calls issued by the model.\nBidiGenerateContentToolResponse\nshould be used instead.\nBidiGenerateContentClientContent\nshould only be\nused to establish previous context or provide text input to the conversation.\nStreaming audio and video\nCode execution\nTo learn more about code execution, see\nCode execution\n.\nFunction calling\nAll functions must be declared at the start of the session by sending tool\ndefinitions as part of the\nBidiGenerateContentSetup\nmessage.\nYou define functions by using JSON, specifically with a\nselect subset\nof the\nOpenAPI schema format\n.\nA single function declaration can include the following parameters:\nname\n(string): The unique identifier for the function\nwithin the API call.\ndescription\n(string): A comprehensive explanation\nof the function's purpose and capabilities.\nparameters\n(object): Defines the input data required\nby the function.\ntype\n(string): Specifies the overall data type,\nsuch as object.\nproperties\n(object): Lists individual parameters,\neach with:\ntype\n(string): The data type of the parameter,\nsuch as string, integer, boolean.\ndescription\n(string): A clear explanation of\nthe parameter's purpose and expected format.\nrequired\n(array): An array of strings listing\nthe parameter names that are mandatory for the function to operate.\nFor code examples of a function declaration using curl commands, see\nFunction calling with the Gemini\nAPI\n.\nFor examples of how to create function declarations using the\nGemini API SDKs, see the\nFunction calling\ntutorial\n.\nFrom a single prompt, the model can generate multiple function calls and the\ncode necessary to chain their outputs. This code executes in a\nsandbox environment, generating subsequent\nBidiGenerateContentToolCall\nmessages. The execution pauses until the results of each function call\nare available, which ensures sequential processing.\nThe client should respond with\nBidiGenerateContentToolResponse\n.\nTo learn more, see\nIntroduction to function calling\n.\nAudio formats\nSee the list of\nsupported audio formats\n.\nSystem instructions\nYou can provide system instructions to better control the model's output\nand specify the tone and sentiment of audio responses.\nSystem instructions are added to the prompt before the interaction begins\nand remain in effect for the entire session.\nSystem instructions can only be set at the beginning of a session,\nimmediately following the initial connection. To provide further input\nto the model during the session, use incremental content updates.\nInterruptions\nUsers can interrupt the model's output at any time. When\nVoice activity detection (VAD) detects an interruption, the ongoing\ngeneration is canceled and discarded. Only the information already sent\nto the client is retained in the session history. The server then\nsends a\nBidiGenerateContentServerContent\nmessage to report the interruption.\nIn addition, the Gemini server discards any pending\nfunction calls and sends a\nBidiGenerateContentServerContent\nmessage with the\nIDs of the canceled calls.\nVoices\nTo specify a voice, set the\nvoiceName\nwithin the\nspeechConfig\nobject,\nas part of your\nsession configuration\n.\nSee the following JSON representation of a\nspeechConfig\nobject:\n{\n\"voiceConfig\"\n:\n{\n\"prebuiltVoiceConfig\"\n:\n{\n\"voiceName\"\n:\n\"\nVOICE_NAME\n\"\n}\n}\n}\nTo see the list of supported voices, see\nChange voice and language settings\n.\nLimitations\nConsider the following limitations of Live API and\nGemini 2.0 when you plan your project.\nClient authentication\nLive API only provides server to server authentication\nand isn't recommended for direct client use. Client input should be routed\nthrough an intermediate application server for secure authentication with\nthe Live API.\nMaximum session duration\nThe default maximum length of a conversation session is 10 minutes. For\nmore information, see\nSession length\n.\nVideo frame rate\nWhen you stream video to the model, it is processed at 1 frame per second (FPS). This makes the API unsuitable for use cases that require analyzing fast-changing video, such as play-by-play in high-speed sports.\nVoice activity detection (VAD)\nBy default, the model automatically performs voice activity detection (VAD) on\na continuous audio input stream. VAD can be configured with the\nRealtimeInputConfig.AutomaticActivityDetection\nfield of the\nsetup message\n.\nWhen the audio stream is paused for more than a second (for example,\nwhen the user switches off the microphone), an\nAudioStreamEnd\nevent is sent to flush any cached audio. The client can resume sending\naudio data at any time.\nAlternatively, the automatic VAD can be turned off by setting\nRealtimeInputConfig.AutomaticActivityDetection.disabled\nto\ntrue\nin the setup\nmessage. In this configuration the client is responsible for detecting user\nspeech and sending\nActivityStart\nand\nActivityEnd\nmessages at the appropriate times. An\nAudioStreamEnd\nisn't sent in\nthis configuration. Instead, any interruption of the stream is marked by\nan\nActivityEnd\nmessage.\nAdditional limitations\nManual endpointing isn't supported.\nAudio inputs and audio outputs negatively impact the model's ability to\nuse function calling.\nToken count\nToken count isn't supported.\nRate limits\nThe following rate limits apply:\n5,000 concurrent sessions per project\n4M tokens per minute\nMessages and events\nBidiGenerateContentClientContent\nIncremental update of the current conversation delivered from the client. All the content here is unconditionally appended to the conversation history and used as part of the prompt to the model to generate content.\nA message here will interrupt any current model generation.\nFields\nturns[]\nContent\nOptional. The content appended to the current conversation with the model.\nFor single-turn queries, this is a single instance. For multi-turn queries, this is a repeated field that contains conversation history and latest request.\nturn_complete\nbool\nOptional. If true, indicates that the server content generation should start with the currently accumulated prompt. Otherwise, the server will await additional messages before starting generation.\nBidiGenerateContentRealtimeInput\nUser input that is sent in real time.\nThis is different from\nClientContentUpdate\nin a few ways:\nCan be sent continuously without interruption to model generation.\nIf there is a need to mix data interleaved across the\nClientContentUpdate\nand the\nRealtimeUpdate\n, server attempts to  optimize for best response, but there are no guarantees.\nEnd of turn is not explicitly specified, but is rather derived from user  activity (for example, end of speech).\nEven before the end of turn, the data is processed incrementally  to optimize for a fast start of the response from the model.\nIs always assumed to be the user's input (cannot be used to populate  conversation history).\nFields\nmedia_chunks[]\nBlob\nOptional. Inlined bytes data for media input.\nactivity_start\nActivityStart\nOptional. Marks the start of user activity. This can only be sent if automatic (i.e. server-side) activity detection is disabled.\nactivity_end\nActivityEnd\nOptional. Marks the end of user activity. This can only be sent if automatic (i.e. server-side) activity detection is disabled.\nActivityEnd\nThis type has no fields.\nMarks the end of user activity.\nActivityStart\nThis type has no fields.\nOnly one of the fields in this message must be set at a time. Marks the start of user activity.\nBidiGenerateContentServerContent\nIncremental server update generated by the model in response to client messages.\nContent is generated as quickly as possible, and not in realtime. Clients may choose to buffer and play it out in realtime.\nFields\nturn_complete\nbool\nOutput only. If true, indicates that the model is done generating. Generation will only start in response to additional client messages. Can be set alongside\ncontent\n, indicating that the\ncontent\nis the last in the turn.\ninterrupted\nbool\nOutput only. If true, indicates that a client message has interrupted current model generation. If the client is playing out the content in realtime, this is a good signal to stop and empty the current queue. If the client is playing out the content in realtime, this is a good signal to stop and empty the current playback queue.\ngeneration_complete\nbool\nOutput only. If true, indicates that the model is done generating.\nWhen model is interrupted while generating there will be no 'generation_complete' message in interrupted turn, it will go through 'interrupted > turn_complete'.\nWhen model assumes realtime playback there will be delay between generation_complete and turn_complete that is caused by model waiting for playback to finish.\ngrounding_metadata\nGroundingMetadata\nOutput only. Metadata specifies sources used to ground generated content.\ninput_transcription\nTranscription\nOptional. Input transcription. The transcription is independent to the model turn which means it doesn't imply any ordering between transcription and model turn.\noutput_transcription\nTranscription\nOptional. Output transcription. The transcription is independent to the model turn which means it doesn't imply any ordering between transcription and model turn.\nmodel_turn\nContent\nOutput only. The content that the model has generated as part of the current conversation with the user.\nTranscription\nAudio transcription message.\nFields\ntext\nstring\nOptional. Transcription text.\nfinished\nbool\nOptional. The bool indicates the end of the transcription.\nBidiGenerateContentSetup\nMessage to be sent in the first and only first client message. Contains configuration that will apply for the duration of the streaming session.\nClients should wait for a\nBidiGenerateContentSetupComplete\nmessage before sending any additional messages.\nFields\nmodel\nstring\nRequired. The fully qualified name of the publisher model.\nPublisher model format:\nprojects/{project}/locations/{location}/publishers/\\*/models/\\*\ngeneration_config\nGenerationConfig\nOptional. Generation config.\nThe following fields aren't supported:\nresponse_logprobs\nresponse_mime_type\nlogprobs\nresponse_schema\nstop_sequence\nrouting_config\naudio_timestamp\nsystem_instruction\nContent\nOptional. The user provided system instructions for the model. Note: only text should be used in parts and content in each part will be in a separate paragraph.\ntools[]\nTool\nOptional. A list of\nTools\nthe model may use to generate the next response.\nA\nTool\nis a piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of knowledge and scope of the model.\nsession_resumption\nSessionResumptionConfig\nOptional. Configures session resumption mechanism. If included, the server will send periodical\nSessionResumptionUpdate\nmessages to the client.\ncontext_window_compression\nContextWindowCompressionConfig\nOptional. Configures context window compression mechanism.\nIf included, server will compress context window to fit into given length.\nrealtime_input_config\nRealtimeInputConfig\nOptional. Configures the handling of realtime input.\ninput_audio_transcription\nAudioTranscriptionConfig\nOptional. The transcription of the input aligns with the input audio language.\noutput_audio_transcription\nAudioTranscriptionConfig\nOptional. The transcription of the output aligns with the language code specified for the output audio.\nAudioTranscriptionConfig\nThis type has no fields.\nThe audio transcription configuration.\nBidiGenerateContentSetupComplete\nSent in response to a\nBidiGenerateContentSetup\nmessage from the client.\nFields\nsession_id\nstring\nThe session id of the session.\nBidiGenerateContentToolCall\nRequest for the client to execute the\nfunction_calls\nand return the responses with the matching\nid\ns.\nFields\nfunction_calls[]\nFunctionCall\nOutput only. The function call to be executed.\nBidiGenerateContentToolCallCancellation\nNotification for the client that a previously issued\nToolCallMessage\nwith the specified\nid\ns should have been not executed and should be cancelled. If there were side-effects to those tool calls, clients may attempt to undo the tool calls. This message occurs only in cases where the clients interrupt server turns.\nFields\nids[]\nstring\nOutput only. The ids of the tool calls to be cancelled.\nBidiGenerateContentToolResponse\nClient generated response to a\nToolCall\nreceived from the server. Individual\nFunctionResponse\nobjects are matched to the respective\nFunctionCall\nobjects by the\nid\nfield.\nNote that in the unary and server-streaming GenerateContent APIs function calling happens by exchanging the\nContent\nparts, while in the bidi GenerateContent APIs function calling happens over these dedicated set of messages.\nFields\nfunction_responses[]\nFunctionResponse\nOptional. The response to the function calls.\nRealtimeInputConfig\nConfigures the realtime input behavior in\nBidiGenerateContent\n.\nFields\nautomatic_activity_detection\nAutomaticActivityDetection\nOptional. If not set, automatic activity detection is enabled by default. If automatic voice detection is disabled, the client must send activity signals.\nactivity_handling\nActivityHandling\nOptional. Defines what effect activity has.\nturn_coverage\nTurnCoverage\nOptional. Defines which input is included in the user's turn.\nActivityHandling\nThe different ways of handling user activity.\nEnums\nACTIVITY_HANDLING_UNSPECIFIED\nIf unspecified, the default behavior is\nSTART_OF_ACTIVITY_INTERRUPTS\n.\nSTART_OF_ACTIVITY_INTERRUPTS\nIf true, start of activity will interrupt the model's response (also called \"barge in\"). The model's current response will be cut-off in the moment of the interruption. This is the default behavior.\nNO_INTERRUPTION\nThe model's response will not be interrupted.\nAutomaticActivityDetection\nConfigures automatic detection of activity.\nFields\nstart_of_speech_sensitivity\nStartSensitivity\nOptional. Determines how likely speech is to be detected.\nend_of_speech_sensitivity\nEndSensitivity\nOptional. Determines how likely detected speech is ended.\nprefix_padding_ms\nint32\nOptional. The required duration of detected speech before start-of-speech is committed. The lower this value the more sensitive the start-of-speech detection is and the shorter speech can be recognized. However, this also increases the probability of false positives.\nsilence_duration_ms\nint32\nOptional. The required duration of detected silence (or non-speech) before end-of-speech is committed. The larger this value, the longer speech gaps can be without interrupting the user's activity but this will increase the model's latency.\ndisabled\nbool\nOptional. If enabled, detected voice and text input count as activity. If disabled, the client must send activity signals.\nEndSensitivity\nEnd of speech sensitivity.\nEnums\nEND_SENSITIVITY_UNSPECIFIED\nThe default is END_SENSITIVITY_LOW.\nEND_SENSITIVITY_HIGH\nAutomatic detection ends speech more often.\nEND_SENSITIVITY_LOW\nAutomatic detection ends speech less often.\nStartSensitivity\nStart of speech sensitivity.\nEnums\nSTART_SENSITIVITY_UNSPECIFIED\nThe default is START_SENSITIVITY_LOW.\nSTART_SENSITIVITY_HIGH\nAutomatic detection will detect the start of speech more often.\nSTART_SENSITIVITY_LOW\nAutomatic detection will detect the start of speech less often.\nTurnCoverage\nOptions about which input is included in the user's turn.\nEnums\nTURN_COVERAGE_UNSPECIFIED\nIf unspecified, the default behavior is\nTURN_INCLUDES_ALL_INPUT\n.\nTURN_INCLUDES_ONLY_ACTIVITY\nThe users turn only includes activity since the last turn, excluding inactivity (e.g. silence on the audio stream).\nTURN_INCLUDES_ALL_INPUT\nThe users turn includes all realtime input since the last turn, including inactivity (e.g. silence on the audio stream). This is the default behavior.\nUsageMetadata\nMetadata on the usage of the cached content.\nFields\ntotal_token_count\nint32\nTotal number of tokens that the cached content consumes.\ntext_count\nint32\nNumber of text characters.\nimage_count\nint32\nNumber of images.\nvideo_duration_seconds\nint32\nDuration of video in seconds.\naudio_duration_seconds\nint32\nDuration of audio in seconds.\nGoAway\nServer will not be able to service client soon.\nFields\ntime_left\nDuration\nThe remaining time before the connection will be terminated as ABORTED. The minimal time returned here is specified differently together with the rate limits for a given model.\nSessionResumptionUpdate\nUpdate of the session resumption state.\nOnly sent if\nBidiGenerateContentSetup.session_resumption\nwas set.\nFields\nnew_handle\nstring\nNew handle that represents state that can be resumed. Empty if\nresumable\n=false.\nresumable\nbool\nTrue if session can be resumed at this point.\nIt might be not possible to resume session at some points. In that case we send update empty new_handle and resumable=false. Example of such case could be model executing function calls or just generating. Resuming session (using previous session token) in such state will result in some data loss.\nlast_consumed_client_message_index\nint64\nIndex of last message sent by client that is included in state represented by this SessionResumptionToken. Only sent when\nSessionResumptionConfig.transparent\nis set.\nPresence of this index allows users to transparently reconnect and avoid issue of losing some part of realtime audio input/video. If client wishes to temporarily disconnect (for example as result of receiving GoAway) they can do it without losing state by buffering messages sent since last\nSessionResmumptionTokenUpdate\n. This field will enable them to limit buffering (avoid keeping all requests in RAM).\nIt will not be used for 'resumption to restore state' some time later -- in those cases partial audio and video frames are likely not needed.\nWhat's next\nLearn more about\nfunction\ncalling\n.\nSee the\nFunction calling\nreference\nfor examples.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Live API reference  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:27:37"
  }
}