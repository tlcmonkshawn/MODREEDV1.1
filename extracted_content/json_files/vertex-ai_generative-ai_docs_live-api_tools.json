{
  "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/tools",
  "title": "Built-in tools for the Live API  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
  "text_content": "Home\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nBuilt-in tools for the Live API\nStay organized with collections\nSave and categorize content based on your preferences.\nSeveral tools are compatible with various versions of\nLive API-supported models, including:\nFunction calling\nGrounding with Google Search\nGrounding with Vertex AI RAG Engine (Preview)\nTo enable a particular tool for usage in returned responses, include the name of\nthe tool in the\ntools\nlist when you initialize the model. The following\nsections provide examples of how to use each of the built-in tools in your code.\nSupported models\nYou can use the Live API with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash\nPrivate GA\n*\ngemini-live-2.5-flash-preview-native-audio\nPublic preview\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\n*\nReach out to your Google account team representative to request\naccess.\nFunction calling\nUse\nfunction calling\nto create a description of a\nfunction, then pass that description to the model in a request. The response\nfrom the model includes the name of a function that matches the description and\nthe arguments to call it with.\nAll functions must be declared at the start of the session by sending tool\ndefinitions as part of the\nLiveConnectConfig\nmessage.\nTo enable function calling, include\nfunction_declarations\nin the\ntools\nlist:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\n# Simple function definitions\nturn_on_the_lights\n=\n{\n\"name\"\n:\n\"turn_on_the_lights\"\n}\nturn_off_the_lights\n=\n{\n\"name\"\n:\n\"turn_off_the_lights\"\n}\ntools\n=\n[{\n\"function_declarations\"\n:\n[\nturn_on_the_lights\n,\nturn_off_the_lights\n]}]\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"TEXT\"\n],\n\"tools\"\n:\ntools\n}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nprompt\n=\n\"Turn on the lights please\"\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\n{\n\"parts\"\n:\n[{\n\"text\"\n:\nprompt\n}]})\nasync\nfor\nchunk\nin\nsession\n.\nreceive\n():\nif\nchunk\n.\nserver_content\n:\nif\nchunk\n.\ntext\nis\nnot\nNone\n:\nprint\n(\nchunk\n.\ntext\n)\nelif\nchunk\n.\ntool_call\n:\nfunction_responses\n=\n[]\nfor\nfc\nin\ntool_call\n.\nfunction_calls\n:\nfunction_response\n=\ntypes\n.\nFunctionResponse\n(\nname\n=\nfc\n.\nname\n,\nresponse\n=\n{\n\"result\"\n:\n\"ok\"\n}\n# simple, hard-coded function response\n)\nfunction_responses\n.\nappend\n(\nfunction_response\n)\nawait\nsession\n.\nsend_tool_response\n(\nfunction_responses\n=\nfunction_responses\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nPython\nFor examples using function calling in system instructions, see our\nbest practices example\n.\nGrounding with Google Search\nYou can use\nGrounding with Google Search\nwith\nthe Live API by including\ngoogle_search\nin the\ntools\nlist:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\ntools\n=\n[{\n'google_search'\n:\n{}}]\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"TEXT\"\n],\n\"tools\"\n:\ntools\n}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nprompt\n=\n\"When did the last Brazil vs. Argentina soccer match happen?\"\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\n{\n\"parts\"\n:\n[{\n\"text\"\n:\nprompt\n}]})\nasync\nfor\nchunk\nin\nsession\n.\nreceive\n():\nif\nchunk\n.\nserver_content\n:\nif\nchunk\n.\ntext\nis\nnot\nNone\n:\nprint\n(\nchunk\n.\ntext\n)\n# The model might generate and execute Python code to use Search\nmodel_turn\n=\nchunk\n.\nserver_content\n.\nmodel_turn\nif\nmodel_turn\n:\nfor\npart\nin\nmodel_turn\n.\nparts\n:\nif\npart\n.\nexecutable_code\nis\nnot\nNone\n:\nprint\n(\npart\n.\nexecutable_code\n.\ncode\n)\nif\npart\n.\ncode_execution_result\nis\nnot\nNone\n:\nprint\n(\npart\n.\ncode_execution_result\n.\noutput\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nGrounding with Vertex AI RAG Engine (Preview)\nYou can use Vertex AI RAG Engine with the Live API for grounding,\nstoring, and retrieving contexts:\nPython\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nfrom\ngoogle.genai.types\nimport\n(\nContent\n,\nLiveConnectConfig\n,\nHttpOptions\n,\nModality\n,\nPart\n)\nfrom\nIPython\nimport\ndisplay\nPROJECT_ID\n=\nYOUR_PROJECT_ID\nLOCATION\n=\nYOUR_LOCATION\nTEXT_INPUT\n=\nYOUR_TEXT_INPUT\nMODEL_NAME\n=\n\"gemini-live-2.5-flash\"\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nPROJECT_ID\n,\nlocation\n=\nLOCATION\n,\n)\nrag_store\n=\ntypes\n.\nVertexRagStore\n(\nrag_resources\n=\n[\ntypes\n.\nVertexRagStoreRagResource\n(\nrag_corpus\n=\n# Use memory corpus if you want to store context.\n)\n],\n# Set `store_context` to true to allow Live API sink context into your memory corpus.\nstore_context\n=\nTrue\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nMODEL_NAME\n,\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\nModality\n.\nTEXT\n],\ntools\n=\n[\ntypes\n.\nTool\n(\nretrieval\n=\ntypes\n.\nRetrieval\n(\nvertex_rag_store\n=\nrag_store\n))]),\n)\nas\nsession\n:\ntext_input\n=\nTEXT_INPUT\nprint\n(\n\"> \"\n,\ntext_input\n,\n\"\n\\n\n\"\n)\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n(\ntext\n=\ntext_input\n)])\n)\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\ntext\n:\ndisplay\n.\ndisplay\n(\ndisplay\n.\nMarkdown\n(\nmessage\n.\ntext\n))\ncontinue\nFor more information, see\nUse Vertex AI RAG Engine in Gemini\nLive API\n.\n(Public preview) Native audio\nGemini 2.5 Flash with Live API\nintroduces native audio capabilities, enhancing the standard Live API\nfeatures. Native audio provides richer and more natural voice interactions\nthrough\n30 HD voices\nin\n24\nlanguages\n. It also\nincludes two new features exclusive to native audio:\nProactive Audio\nand\nAffective Dialog\n.\nUse Affective Dialog\nAffective Dialog\nallows models using Live API\nnative audio to better understand and respond appropriately to users' emotional\nexpressions, leading to more nuanced conversations.\nTo enable Affective Dialog, set\nenable_affective_dialog\nto\ntrue\nin the setup message:\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nenable_affective_dialog\n=\nTrue\n,\n)\nMore information\nFor more information on using the Live API, see:\nLive API overview\nLive API reference\nguide\nInteractive conversations\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
  "metadata": {
    "title": "Built-in tools for the Live API  |  Generative AI on Vertex AI  |  Google Cloud Documentation",
    "extracted_at": "2025-11-11 14:26:07"
  }
}