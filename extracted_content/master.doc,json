{
  "metadata": {
    "created_at": "2025-11-11T14:45:17.219623",
    "source": "Gemini 2.5 Flash Live API Documentation",
    "main_page": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-live-api",
    "total_pages": 27,
    "corrections_applied": {
      "concurrent_sessions": "Corrected from 5,000 to 1,000",
      "code_execution": "Removed (not supported)",
      "rag_engine": "Removed (not supported)"
    }
  },
  "pages": [
    {
      "filename": "vertex-ai_generative-ai_docs.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\ndeployed_code_account\nBuild, deploy, and connect agents\nBuild agents with an open approach and deploy them with enterprise-grade controls. Connect agents across your enterprise ecosystem.\nenterprise\nEnterprise ready\nDeploy your generative AI agents and applications at scale with enterprise-grade security, data residency and privacy, access transparency, and low latency.\nlightbulb_tips\nState-of-the-art features\nExpand the capabilities of your applications by using the 2 million token context window that Gemini supports, and the built-in multimodality and thinking capabilities from Gemini 2.5 models.\npsychiatry\nOpen and flexible platform\nVertex AI Model Garden provides a library of over 200 enterprise-ready models and Vertex AI Model Builder helps you test, customize, deploy, and monitor Google proprietary and select third-party models, including Anthropic's Claude 3.7 Sonnet, Meta's Llama 4, Mistral's AI Mixtral 8x7B, and AI21 Labs's Jamba 1.5.\nGenerate text using the Gemini API in Vertex AI\nUse the Gen AI SDK to send requests to the Gemini API in Vertex AI\nBrowse the Vertex AI Studio Prompt Gallery\nTest out prompts with no setup required\nGenerate images and verify watermarks using Imagen\nCreate a watermarked image using Imagen on Vertex AI\nAgent Builder\nA suite of features for building and deploying AI agents\nLive API\nUse the Live API to provide your end users with the experience of natural, human-like voice conversations\nThinking\nSolve complex requests and show the model's thought process behind its response using Gemini's built-in reasoning capabilities\nGrounding\nGround your responses using Google Search, Maps, or even your own data\nGenerate embeddings\nGenerate embeddings to perform tasks such as search, classification, clustering, and outlier detection\nTuning\nAdapt your model to perform specific tasks with greater precision and accuracy\nImage generation\nBuild next-generation AI products that transform your user's imagination into high quality visual assets using AI generation in seconds\nVideo generation\nUse Veo on Vertex AI to generate new videos from a text prompt or an image prompt\nGenerative AI evaluation service\nVertex AI lets you evaluate any generative model or application and benchmark the evaluation results against your own judgment, using your own evaluation criteria\nUse the Gen AI SDK to send requests to the Gemini API in Vertex AI\nJava\nPython\nNode.js\nGo\nExplore various Gemini use cases with our example Jupyter notebooks\nRun in Colab\nRun in Colab Enterprise\nOpen in Vertex AI Workbench\nView on GitHub\nLearn how to design prompts to improve the quality of your responses from the model\nRun in Colab\nRun in Colab Enterprise\nOpen in Vertex AI Workbench\nView on GitHub\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-06-27 UTC.",
      "content_length": 3357,
      "line_count": 69
    },
    {
      "filename": "vertex-ai_generative-ai_docs_dynamic-shared-quota.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/dynamic-shared-quota",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/dynamic-shared-quota\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nDynamic shared quota (DSQ)\nStay organized with collections\nSave and categorize content based on your preferences.\nDynamic shared quota (DSQ) was introduced to serve your pay-as-you-go (PayGo)\nrequests with greater flexibility to adapt to your workload needs without having\nto manage quotas and quota increase requests (QIR). With DSQ, there are no predefined\nquota limits on your usage. Instead, DSQ provides access to a large, shared pool of\nresources, dynamically allocated based on real-time availability of resources and\nreal-time demand across all customers of that model. When more customers are active,\neach customer gets a lower amount of throughput. Similarly, if there are fewer customers,\neach customer might get higher throughput.\nSupported models\nThe following Gemini models and their\nsupervised fine-tuned\nmodels support DSQ:\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash Image\nGemini 2.5 Flash-Lite\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.0 Flash with image generation\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nGemini 2.0 Flash-Lite\nThe following Imagen models support DSQ:\nImagen 4\nImagen 4 Fast\nImagen 4 Ultra\nHow DSQ works\nDynamic shared quota (DSQ) adapts to your traffic patterns and needs and\nminimizes usage frictions. Your project's access to resources under DSQ is not\ncapped by an arbitrary number we set. Instead, it's determined by the overall\ncapacity of the shared pool and the current collective demand from all customers.\nThis model is designed to offer significant flexibility, allowing your workloads\nto burst and consume more resources when available. Conversely, it also allows\nall customers of the shared pool to have a chance to access resources when\navailable without requiring to configure per customer quota.\nTo ensure a fair and stable experience for all users in the shared resource\nenvironment, Dynamic Shared Quota intelligently manages how requests are handled,\nespecially during periods of very high demand from isolated sources. Rather than\na fixed cap, DSQ employs a dynamic prioritization approach. This means that while\nthe system is designed to accommodate bursts, unusually large and rapid spikes in\ntraffic from a single source may be handled with a different priority than more\nconsistent, steady traffic. This sophisticated management ensures that broad user\nactivity and regular workloads are protected from transient, extreme spikes,\npromoting overall system stability and equitable access.\nGemini requests with multi-modal inputs are subject to the\ncorresponding system rate limits that include\nimage\n,\naudio\n,\nvideo\n, and\ndocument\n.\nTo help ensure high availability for your application and to get predictable\nservice levels for your production workloads, see\nProvisioned Throughput\n.\nUnderstanding Resource Exhaustion 429 errors under DSQ\nWe understand that encountering a 'resource exhausted' 429 error can be\nfrustrating and might lead you to suspect you are hitting some sort of quota\nlimit. However, with DSQ, this is not the case. These errors indicate that the\noverall shared pool of resources for that specific type (e.g., a particular\nmodel in a specific region) at a specific time is experiencing extremely high\ndemand from many users simultaneously. Think of it like trying to get on a very\npopular train during peak rush hour. There isn't a 'ticket limit' specifically\nfor you, but the train itself might be momentarily full. It's a temporary state\nof contention for resources, not a fixed limit imposed on your project.\nDSQ is constantly working to manage and distribute the available capacity fairly\nand efficiently. When you receive such an error, it means instantaneous demand\nhas outstripped the available supply in that shared pool. Unlike a hard\nquota where you'd be blocked even if resources were idle elsewhere, DSQ aims to\ngive you access whenever resources are free. The exhaustion error is a reflection\nof the entire system's current load, not a ceiling on your account.\nWe recommend implementing retry mechanisms, as availability in this dynamic\nenvironment can change quickly. For more tactics of handling Resource Exhaustion\nerrors, see\nA guide to handling 429 errors\nor\nError code 429\n.\nWhat's next\nTo learn about quotas and limits for Vertex AI, see\nVertex AI quotas and limits\n.\nTo learn more about Google Cloud quotas and system limits, see the\nCloud Quotas documentation\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 5004,
      "line_count": 112
    },
    {
      "filename": "vertex-ai_generative-ai_docs_error-code-429.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/error-code-429",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/error-code-429\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nError code 429\nStay organized with collections\nSave and categorize content based on your preferences.\nIf the number of your requests exceeds the capacity allocated to process\nrequests, then error code\n429\nis returned. The following table displays the\nerror message generated by each type of quota framework:\nQuota framework\nMessage\nPay-as-you-go\nResource exhausted, please try again later.\nProvisioned Throughput\nToo many requests. Exceeded the Provisioned Throughput.\nWith a Provisioned Throughput subscription, you can reserve an\namount of throughput for specific generative AI models. If you don't have a\nProvisioned Throughput subscription and resources aren't available\nto your application, then an error code\n429\nis returned. Although you don't\nhave reserved capacity, you can try your request again. However, the request\nisn't counted against your error rate as described in your\nservice level\nagreement (SLA)\n.\nFor projects that have purchased Provisioned Throughput,\nVertex AI measures a project's throughput and reserves the purchased\namount of throughput for the project's actual usage.\nFor standard Provisioned Throughput, when you use less than your\npurchased amount, errors that might otherwise be\n429\nare returned as\n5XX\nand\ncount toward the SLA error rate. For Single Zone Provisioned Throughput,\nwhen you use less than your purchased amount, capacity-related\n429\nerrors are\ntreated as\n5XX\nbut don't count toward the SLA error rate. When you exceed your\npurchased amount, the additional requests are processed on-demand as pay-as-you-go.\nPay-as-you-go\nOn the pay-as-you-go quota framework, you have the following options to\nresolving\n429\nerrors:\nUse the\nglobal endpoint\ninstead of a regional endpoint whenever possible.\nImplement a retry strategy by using\ntruncated exponential backoff\n.\nIf your model uses quotas, you can submit a Quota Increase Request (QIR). If\nyour model uses\nDynamic shared\nquota\n, smoothing traffic\nand reducing large spikes can help. For more information, see\nDynamic shared\nquota (DSQ)\n.\nSubscribe to Provisioned Throughput for a more consistent level of service.\nFor more information, see\nProvisioned Throughput\n.\nProvisioned Throughput\nTo correct the 429 error generated by Provisioned Throughput, do the\nfollowing:\nUse the\nDefault behavior\nexample\n, which doesn't set a\nheader in prediction requests. Any overages are processed on-demand and billed\nas pay-as-you-go.\nIncrease the number of GSUs in your Provisioned Throughput\nsubscription.\nWhat's next\nTo learn more about dynamic shared quota, see\nDynamic shared\nquota\n.\nTo learn more about Provisioned Throughput, see\nProvisioned Throughput\n.\nTo learn about quotas and limits for Vertex AI, see\nVertex AI quotas and limits\n.\nTo learn more about Google Cloud quotas and system limits, see the\nCloud Quotas documentation\n.\nTo learn more about API errors, see\nAPI errors\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 3442,
      "line_count": 111
    },
    {
      "filename": "vertex-ai_generative-ai_docs_grounding_grounding-with-elasticsearch.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-elasticsearch",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-elasticsearch\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGrounding with Elasticsearch\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains how you can use your Elasticsearch instance to ground on your\ndata.\nGrounding Gemini with Elasticsearch\nGrounding\ninvolves using public and private\ndatasets to provide context and facts to ground Large Language Model (LLM)\nresponses. By grounding with Elasticsearch, you can take advantage of your\nexisting Elasticsearch indexes to help enhance the quality and reliability of\nGemini's output, reducing hallucinations and helping to ensure\nresponses are relevant to your data. This lets you build powerful RAG\napplications such as:\nGenerative search summaries\nQuestion-and-answer chatbots with enterprise data\nAgents grounded in your data\nYou can ground an answer on up to 10 data sources at one time. You can combine\ngrounding with Elasticsearch with\nGrounding with\nGoogle Search\nto connect the model with world knowledge, a wide possible range of topics, or\nup-to-date information on the internet.\nSupported models\nThe following\nmodels support grounding with Elasticsearch\nwith text input only:\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nSet up a search template in Elasticsearch\nThis section explains how you can use your Elasticsearch instance to ground on\nyour data stored in the instance.\nBest practices\nFor the best grounding responses, use these principles when creating a search\ntemplate:\nInclude only relevant and useful data. For example, in a product catalog,\nspecifying an image URL might not help the LLM answer prompts about product\nproperties unless the prompt specifically asks for a URL. Similarly, avoid\noutputting embedding vectors.\nGrounding removes Elasticsearch results with low relevance to your prompts.\nYou should provide a higher number of Elasticsearch results to capture all\nrelevant context.\nResults data can be in one field or spread across multiple fields.\nSample templates\nYou can use your search templates. However, we recommend that you use the\ngeneric kNN search template with Elasticsearch grounding. For additional search\ntemplates, see the\nGitHub repository\n.\nThis semantic search with Vertex AI is a generic kNN search.\nPUT\n_scrip\nts\n/google\n-\nte\nmpla\nte\n-\nk\nnn\n-\nmul\nt\niou\nt\npu\nt\n{\n\"script\"\n:\n{\n\"lang\"\n:\n\"mustache\"\n,\n\"source\"\n:\n{\n\"_source\"\n:\n{\n\"excludes\"\n:\n[\n\"title_embedding\"\n,\n\"description_embedding\"\n,\n\"images\"\n]\n},\n\"size\"\n:\n\"num_hits\"\n,\n\"knn\"\n:\n[\n{\n\"field\"\n:\n\"description_embedding\"\n,\n\"k\"\n:\n5\n,\n\"num_candidates\"\n:\n10\n,\n\"query_vector_builder\"\n:\n{\n\"text_embedding\"\n:\n{\n\"model_id\"\n:\n\"googlevertexai_embeddings_004\"\n,\n\"model_text\"\n:\n\"query\"\n}\n},\n\"boost\"\n:\n0.4\n},\n{\n\"field\"\n:\n\"title_embedding\"\n,\n\"k\"\n:\n5\n,\n\"num_candidates\"\n:\n10\n,\n\"query_vector_builder\"\n:\n{\n\"text_embedding\"\n:\n{\n\"model_id\"\n:\n\"googlevertexai_embeddings_004\"\n,\n\"model_text\"\n:\n\"query\"\n}\n},\n\"boost\"\n:\n0.6\n}\n]\n}\n}\n}\nGenerate grounded responses with Elasticsearch\nThis section explains how you use the Vertex AI API to ground your LLM\nresponses.\nPrerequisites\nBefore you can ground LLM responses with Elasticsearch, you must complete the\nfollowing:\nActivate the Vertex AI API\n: Ensure that both the\nVertex AI API is enabled for your Google Cloud project.\nInstall and sign in to the Google Cloud CLI CLI\n: Install and initialize\nthe gcloud CLI\ncommand-line tool\n.\nElasticsearch setup\n: Use an existing Elasticsearch cluster and index that\nyou want to use for grounding. Obtain the following information from your\nElasticsearch setup:\nEndpoint\n: The URL of your Elasticsearch cluster.\nIndex Name\n: The name of the index you want to search such as\nmy-data-index\n.\nAPI Key\n: An API key that allows access to your Elasticsearch cluster.\nThe API key must start with the prefix\nApiKey\n.\nCreate an Elasticsearch search template\n: Use an Elasticsearch data source\nthat uses a reference\ntemplate\nthat returns result data\nfor grounding.\nGenerate grounded responses\nConsole\nTo ground with Elasticsearch in the Google Cloud console, do the following:\nGo to Vertex AI Studio to the\nCreate prompt\npage.\nGo to Create prompt\nIn the\nSettings\npanel, to ground your data, click the\nGrounding: Your\ndata\ntoggle.\nIn the\nCustomize Grounding\npane, select\nElasticsearch\n.\nEnter the endpoint in the\nElasticsearch endpoint\nfield.\nEnter\nApiKey YOUR_API_KEY\nin the\nElasticsearch API Key\nfield.\nEnter the index in the\nElasticsearch index\nfield.\nEnter the search template in the\nElasticsearch search template\nfield.\nAdjust the number of hits by sliding the\nNumber of hits\nslider.\nClick\nSave\n.\nEnter your prompt.\nClick\nSubmit\n.\nUnderstand your response\nIf your model prompt successfully grounds to Elasticsearch data stores using the\nVertex AI Studio or the API, then the model's responses include\nmetadata with citations and source content. If low-source relevance or\nincomplete information occurs within the model's response, then metadata might\nnot be provided, and the prompt response won't be grounded.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nGenerateContentConfig\n,\nElasticsearch\n,\nRetrieval\n,\nTool\n,\nHttpOptions\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\n# Replace with your Elasticsearch details\nELASTIC_SEARCH_ENDPOINT\n=\n\"YOUR_ELASTICSEARCH_ENDPOINT\"\nELASTIC_SEARCH_API_KEY\n=\n\"YOUR_ELASTICSEARCH_API_KEY\"\nINDEX_NAME\n=\n\"YOUR_INDEX_NAME\"\nSEARCH_TEMPLATE_NAME\n=\n\"YOUR_SEARCH_TEMPLATE_NAME\"\nNUM_HITS\n=\n5\ntool\n=\nTool\n(\nretrieval\n=\nRetrieval\n(\nexternal_api\n=\nElasticsearch\n(\napi_spec\n=\n\"ELASTIC_SEARCH\"\n,\nendpoint\n=\nELASTIC_SEARCH_ENDPOINT\n,\napi_auth\n=\n{\n\"apiKeyConfig\"\n:\n{\n\"apiKeyString\"\n:\nf\n\"ApiKey\n{\nELASTIC_SEARCH_API_KEY\n}\n\"\n}\n},\nelastic_search_params\n=\n{\n\"index\"\n:\nINDEX_NAME\n,\n\"searchTemplate\"\n:\nSEARCH_TEMPLATE_NAME\n,\n\"numHits\"\n:\nNUM_HITS\n,\n},\n)\n)\n)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\n# Or another supported model\ncontents\n=\n\"What are the main features of product X?\"\n,\n# Your query\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\ntool\n],\n),\n)\nprint\n(\nresponse\n.\ntext\n)\nREST\nTo send a text prompt and ground it with Elasticsearch, send a POST request to\nthe Vertex AI API. At a minimum, you must provide the request\nbody. Make sure to do the following replacements:\nPROMPT\n: The text prompt to ground.\nELASTIC_SEARCH_ENDPOINT\n: The absolute endpoint path for\nthe Elasticsearch resource to use.\nELASTIC_SEARCH_API_KEY\n: The API key for the Elasticsearch\ndata endpoint.\nINDEX_NAME\n: The name of the Elasticsearch index used for\ngrounding.\nSEARCH_TEMPLATE_NAME\n: The Elasticsearch search template\nused for grounding.\nNUM_HITS\n: The number of results returned from the\nElasticsearch data source and used for grounding.\nHTTP method and URL:\nPOST\nhttps://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\nRequest JSON body:\n{\n\"contents\"\n:\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[\n{\n\"text\"\n:\n\"\nPROMPT\n\"\n}\n]\n}\n],\n\"tools\"\n:\n[{\n\"retrieval\"\n:\n{\n\"externalApi\"\n:\n{\n\"api_spec\"\n:\n\"ELASTIC_SEARCH\"\n,\n\"endpoint\"\n:\n\"\nELASTIC_SEARCH_ENDPOINT\n\"\n,\n\"apiAuth\"\n:\n{\n\"apiKeyConfig\"\n:\n{\n\"apiKeyString\"\n:\n\"ApiKey\nELASTIC_SEARCH_API_KEY\n\"\n}\n},\n\"elasticSearchParams\"\n:\n{\n\"index\"\n:\n\"\nINDEX_NAME\n\"\n,\n\"searchTemplate\"\n:\n\"\nSEARCH_TEMPLATE_NAME\n\"\n,\n\"numHits\"\n:\n\"\nNUM_HITS\n\"\n,\n}\n}\n}\n}]\n}\nFor more information on other API fields such as system instructions and\nmulti-turn chats, see\nGenerative AI beginner's guide\n.\nSend the API request\nYou can save the request body in a file named\nrequest.json\n.\nThen execute the POST API request, and do the following replacements:\nLOCATION\n: The region to process the request. To use the\nglobal endpoint\n, exclude the location from the endpoint name, and configure the location of the resource to\nglobal\n.\nPROJECT_ID\n: Your Google Cloud project ID. For more information on project IDs, see\nCreating and managing projects\n.\nMODEL_ID\n: The model ID of the multimodal model.\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json; charset=utf-8\"\n\\\n-d\n@request.json\n\\\n\"https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\"\nYou should receive a JSON response similar to the following:\n{\n\"candidates\"\n:\n[\n{\n\"content\"\n:\n{\n\"role\"\n:\n\"model\"\n,\n\"parts\"\n:\n[\n{\n\"text\"\n:\n\"Based on the information ...\"\n}\n]\n},\n\"finishReason\"\n:\n\"STOP\"\n,\n\"safetyRatings\"\n:\n[\n\"...\"\n],\n\"groundingMetadata\"\n:\n{\n\"groundingChunks\"\n:\n[\n{\n\"retrievedContext\"\n:\n{\n\"text\"\n:\n\"ipsum lorem ...\"\n}\n},\n{\n...\n},\n{\n...\n},\n],\n\"groundingSupports\"\n:\n[\n{\n\"segment\"\n:\n{\n\"startIndex\"\n:\n25\n,\n\"endIndex\"\n:\n147\n,\n\"text\"\n:\n\"ipsum lorem ...\"\n},\n\"groundingChunkIndices\"\n:\n[\n1\n,\n2\n],\n\"confidenceScores\"\n:\n[\n0.6626542\n,\n0.82018316\n],\n},\n],\n},\n}\n],\n}\nUnderstand your response\nThe response from both APIs include the LLM-generated text, which is called a\ncandidate\n. If your model prompt successfully grounds to your Elasticsearch\ndata source, then the responses include grounding metadata, which identifies the\nparts of the response that were derived from your Elasticsearch data. However,\nthere are several reasons this metadata might not be provided, and the prompt\nresponse won't be grounded. These reasons include low-source relevance or\nincomplete information within the model's response.\nThe following is a breakdown of the output data:\nRole\n: Indicates the sender of the grounded answer. Because the response\nalways contains grounded text, the role is always\nmodel\n.\nText\n: The grounded answer generated by the LLM.\nGrounding metadata\n: Information about the grounding source, which contains\nthe following elements:\nGrounding chunks\n: A list of results from your Elasticsearch index that\nsupport the answer.\nGrounding supports\n: Information about a specific claim within the answer\nthat can be used to show citations:\nSegment\n: The part of the model's answer that is substantiated by a\ngrounding chunk.\nGrounding chunk index\n: The index of the grounding chunks in the\ngrounding chunks list that corresponds to this claim.\nConfidence scores\n: A number from 0 to 1 that indicates how grounded\nthe claim is in the provided set of grounding chunks. Not available for\nGemini 2.5 Pro and Gemini 2.5 Flash and later.\nWhat's next\nTo learn how to send chat prompt requests, see\nMultiturn chat\n.\nTo learn about responsible AI best practices and Vertex AI's safety filters,\nsee\nSafety best practices\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 11573,
      "line_count": 736
    },
    {
      "filename": "vertex-ai_generative-ai_docs_grounding_grounding-with-google-maps.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-maps",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-maps\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGrounding with Google Maps in Vertex AI\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page describes how Grounding with Google Maps with Vertex AI can help to enhance your\ngenerative AI applications by providing geospatial context.\nOverview\nGrounding with Google Maps with Vertex AI is a service that connects Gemini models with\ngeospatial data from Google Maps. Google Maps has access to\ninformation on millions of locations, including businesses, landmarks, and\npoints of interest. This data gives you access to information on over 250\nmillion places that can be used to ground your model's responses, enabling your\nAI applications and agents to provide local data and geospatial context.\nYou can also enable concurrent grounding with Google Maps,\nGoogle Search, and grounding with your data sources.\nUses of Grounding with Google Maps\nYou can use Grounding with Google Maps for various applications, such as:\nConversational assistants that can answer questions about nearby places, such\nas \"Where's the nearest place to get an Italian espresso?\"\nPersonalized descriptions and community insights, such as\"Can you tell me more about family-friendly restaurants that are within a walkable distance?\"\nSummaries of areas around specific locations, such as an EV charging station or\na hotel.\nThis can be beneficial for use cases in real estate, travel, mobility, and\nsocial-media apps.\nTry the Grounding with Google Maps demo\nopen_in_new\nSupported models\nThis section lists the models that support Grounding with Google Maps.\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nFor more information about the Gemini models, see\nGemini models\n.\nUse Grounding with Google Maps to ground your model's responses\nThis code sample demonstrates how to use Grounding with Google Maps to ground\nyour model's responses.\nSearch results can be customized for a specific geographic location by using the\nlatitude and longitude coordinates. For more information, see the\nGrounding\nAPI.\nConsole\nTo use Grounding with Google Maps with Vertex AI, follow these steps:\nIn the Google Cloud console, go to the\nVertex AI Studio\npage.\nGo to Vertex AI Studio\nIn the\nTools\nsection, click\nGrounding: Google\n. A configuration pane\nappears.\nTo use Google Maps, click\nGoogle Maps\ntoggle.\nClick\nApply\n.\nEnter your prompt in the field, and click\nSubmit\n. Your prompt responses\nground to Google Maps.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nGenerateContentConfig\n,\nGoogleMaps\n,\nHttpOptions\n,\nTool\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"Where can I get the best espresso near me?\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\n# Use Google Maps Tool\nTool\n(\ngoogle_maps\n=\nGoogleMaps\n(\nenable_widget\n=\nFalse\n# Optional: return Maps widget token\n))\n],\ntool_config\n=\ntypes\n.\nToolConfig\n(\nretrieval_config\n=\ntypes\n.\nRetrievalConfig\n(\nlat_lng\n=\ntypes\n.\nLatLng\n(\n# Pass geo coordinates for location-aware grounding\nlatitude\n=\n40.7128\n,\nlongitude\n=-\n74.006\n),\nlanguage_code\n=\n\"en_US\"\n,\n# Optional: localize Maps results\n),\n),\n),\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# 'Here are some of the top-rated places to get espresso near you: ...'\nREST\nBefore using any of the request data,\nmake the following replacements:\nLOCATION\n: The region to process the request. To use the\nglobal endpoint\n, exclude the location from the endpoint name and configure the location of the resource to global.\nPROJECT_ID\n: Your\nproject ID\n.\nMODEL_ID\n: The model ID of the multimodal model.\nPROMPT\n: The prompt to send to the model.\nLATITUDE\n: The latitude of the location. For example, a latitude of\n37.7749\nrepresents San Francisco. You can obtain latitude and longitude coordinates using services like Google Maps or other geocoding tools.\nLONGITUDE\n: The longitude of the location. For example, a longitude of\n-122.4194\nrepresents San Francisco.\nENABLE_WIDGET\n: Whether to return a token and enable the Google Maps widget (default is\nfalse\n).\nHTTP method and URL:\nPOST https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\nRequest JSON body:\n{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"\nPROMPT\n\"\n}]\n}],\n\"tools\": [{\n\"googleMaps\": {\n\"enableWidget\": \"\nENABLE_WIDGET\n\"\n}\n}],\n\"toolConfig\": {\n\"retrievalConfig\": {\n\"latLng\": {\n\"latitude\":\nLATITUDE\n,\n\"longitude\":\nLONGITUDE\n},\n\"languageCode\":\n\"en_US\"\n}\n},\n\"model\": \"projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n\"\n}\nTo send your request, expand one of these options:\ncurl (Linux, macOS, or Cloud Shell)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\"\nPowerShell (Windows)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\" | Select-Object -Expand Content\nYou should receive a JSON response similar to the following:\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"text\": \"\\\"The Italian Place\\\" in Alexandria, VA, is good for children and offers takeout. It has a rating of 4.2 stars based on 411 reviews.\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n\"groundingMetadata\": {\n\"groundingChunks\": [\n{\n\"maps\": {\n\"uri\": \"https://maps.google.com/?cid=9001322937822692826\",\n\"title\": \"The Italian Place\",\n\"placeId\": \"places/ChIJOTRDf_qwt4kR2kV_WYUf63w\"\n}\n},\n{\n\"maps\": {\n\"uri\": \"https://maps.google.com/?cid=9001322937822692826\",\n\"title\": \"Hank's Pasta Bar\",\n\"placeId\": \"places/MMVtPzn9FGcevML89\",\n\"placeAnswerSources\": {\n\"reviewSnippets\": [\n{\n\"id\": \"places/ChIJOTRDf_qwt4kR2kV_WYUf63w\",\n\"title\": \"Google Maps Review\",\n\"uri\": \"https://maps.google.com/?cid=9001322937822692826\"\n},\n]\n}\n}\n},\n...\n],\n\"groundingSupports\": [\n{\n\"segment\": {\n\"endIndex\": 79,\n\"text\": \"\\\"The Italian Place\\\" in Alexandria, VA, is good for children and offers takeout.\"\n},\n\"groundingChunkIndices\": [\n0\n]\n},\n],\n\"googleMapsWidgetContextToken\": \"widgetcontent/...\"\n}\n}\n],\n...\n}\nPlace properties\nThis section lists place properties that are used to describe locations and used\nby Grounding with Google Maps to generate responses. These properties are\nused to determine the types of questions that Grounding with Google Maps can\nanswer.\nSample place properties\nThis list provides an alphabetized sampling of properties about places that can\nbe used by your model to generate responses.\nAddress\nCurbside pickup\nDebit card\nDistance\nFree parking lot\nLive music\nMenu for children\nOpening hours\nPayment options (such as\ncash\nor\ncredit card\n)\nPlace answer\nPet friendly\nServes beer\nServes vegetarian food\nWheelchair accessible\nWifi\nPlace answers\nare a response from Grounding with Google Maps based on\ninformation derived from user reviews.\nExamples of using place properties\nThe following examples use\nplace properties\nin questions\nabout different types of places. Grounding with Google Maps uses the\nproperties to understand your intent and then provides relevant answers based on\nthe data associated with places in Google Maps.\nPlan a family dinner\n: You might ask,\nIs \"The Italian Place\" good for\nchildren, and do they offer takeout? What is their rating?\nAnswers to these\nquestions help you to determine if a restaurant is suitable for a family and\nif the restaurant offers a convenient service.\nCheck accessibility for a friend\n: You might ask,\nI need a restaurant that\nhas a wheelchair accessible entrance.\nA response to this prompt might help\nyou to determine if the location meets specific accessibility needs.\nFind a location for a late-night snack\n: You might ask,\nIs \"Burger Joint\"\nopen now? Do they serve dinner? What are their opening hours for Friday?\nAnswers to these questions help you to find an open establishment serving a\nspecific meal during a particular time.\nMeet a client for coffee\n: You might ask,\nDoes \"Cafe Central\" have Wifi?\nDo they serve coffee? What is their price level, and do they accept credit\ncards?\nAnswers to these questions help you to assess the suitability of a\ncafe for a business meeting based on amenities, offerings, and payment\noptions.\nInformation in the\nGoogle Maps Grounded Results\nmight differ from actual\nconditions of the road.\nUnderstand your response\nGoogle Maps sources are returned in\ngroundingMetadata\nwithin\ngroundingChunks\n. Sources are returned for places and for user reviews,\nwhich have been used to help generate the\nGoogle Maps Grounded Result\n.\nThis code sample demonstrates a place source and a place answer source in the\nresponse:\n\"groundingChunks\"\n:\n[\n{\n\"maps\"\n:\n{\n\"uri\"\n:\n\"{Link to Maps Content}\"\n,\n\"title\"\n:\n\"{Name of Maps Place}\"\n,\n\"placeId\"\n:\n\"{Place ID}\"\n,\n\"placeAnswerSources\"\n:\n{\n\"reviewSnippets\"\n:\n[\n{\n\"reviewId\"\n:\n\"{Review ID}\"\n,\n\"googleMapsUri\"\n:\n\"{Link to Maps Content}\"\n,\n\"title\"\n:\n\"{Title of review}\"\n}\n]\n}\n},\n}\n],\nService usage requirements\nThis section describes the service usage requirements for Grounding with Google Maps.\nInform the user about the use of Google Maps sources\nWith each\nGoogle Maps Grounded Result\n, you'll receive sources in\ngroundingChunks\nthat support each response. The following metadata is also\nreturned:\nsource uri\ntitle\nID\nWhen presenting results from Grounding with Google Maps with Vertex AI, you must specify the\nassociated Google Maps sources, and inform your users of the following:\nThe Google Maps sources must immediately follow the generated content that\nthe sources support. This generated content is also referred to as\nGoogle Maps Grounded Result\n.\nThe Google Maps sources must be viewable within one user interaction.\nDisplay Google Maps sources with Google Maps links\nFor each source in\ngroundingChunks\nand in\ngrounding_chunks.maps.placeAnswerSources.reviewSnippets\n, a link preview must\nbe generated following these requirements:\nAttribute each source to Google Maps following the\nGoogle Maps text\nattribution guidelines\n.\nDisplay the source title provided in the response.\nLink to the source using the\nuri\nor\ngoogleMapsUri\nfrom the response.\nThese images show the minimum requirements for displaying the sources and\nGoogle Maps links.\nYou can collapse the view of the sources.\nOptional: Enhance the link preview with additional content, such as:\nA\nGoogle Maps favicon\nis inserted\nbefore the Google Maps text attribution.\nA photo from the source URL (\nog:image\n).\nFor more information about some of our Google Maps data providers and their\nlicense terms, see the\nGoogle Maps and Google Earth legal\nnotices\n.\nGoogle Maps text attribution guidelines\nWhen you attribute sources to Google Maps in text, follow these guidelines:\nDon't modify the text\nGoogle Maps\nin any way:\nDon't change the capitalization of\nGoogle Maps\n.\nDon't wrap\nGoogle Maps\nonto multiple lines.\nDon't localize\nGoogle Maps\ninto another language.\nPrevent browsers from translating\nGoogle Maps\nby using the HTML\nattribute\ntranslate=\"no\"\n.\nStyle\nGoogle Maps\ntext as described in the following table:\nProperty\nStyle\nFont family\nRoboto. Loading the font is optional.\nFallback font family\nAny sans serif body font already used in your product or \"Sans-Serif\" to invoke the default system font\nFont style\nNormal\nFont weight\n400\nFont color\nWhite, black (#1F1F1F), or gray (#5E5E5E). Maintain accessible (4.5:1) contrast against the background.\nFont size\nMinimum font size: 12sp\nMaximum font size: 16sp\nTo learn about sp, see Font size units on the\nMaterial Design website.\nLetter spacing\nNormal\nExample CSS\nThe following CSS renders\nGoogle Maps\nwith the appropriate typographic\nstyle and color on a white or light background.\n@\nimport\nurl\n(\n'https://fonts.googleapis.com/css2?family=Roboto&display=swap'\n)\n;\n.\nGMP-attribution\n{\nfont-family\n:\nRoboto\n,\nSans-Serif\n;\nfont-style\n:\nnormal\n;\nfont-weight\n:\n400\n;\nfont-size\n:\n1\nrem\n;\nletter-spacing\n:\nnormal\n;\nwhite-space\n:\nnowrap\n;\ncolor\n:\n#5e5e5e\n;\n}\nContext token, place ID, and review ID\nThe Google Maps data includes context token, place ID, and review ID. You\nmight cache, store, and export the following response data:\ngoogleMapsWidgetContextToken\nplaceId\nreviewId\nThe restrictions against caching in the Grounding with Google Maps Terms\ndon't apply.\nProhibited Territory\nGrounding with Google Maps has restrictions for certain content and\nactivities to maintain a safe and reliable platform. Customer will\nnot distribute or market a Customer Application that offers\nGrounding with Google Maps in a Prohibited Territory. The current Prohibited\nTerritories are:\nChina\nCrimea\nCuba\nDonetsk People's Republic\nIran\nLuhansk People's Republic\nNorth Korea\nSyria\nVietnam\nThis list may be updated from time to time.\nOptional: Google Maps contextual widget\nThe contextual widget is a Google Maps Pre-GA Offering that's a visual\ncontainer used to support or supplement other Google Maps content. The\nGoogle Maps contextual widget lets you integrate Grounding with Google Maps\ninto your applications to create a conversational LLM-powered chat experience.\nThe contextual widget is rendered using the context token,\ngoogleMapsWidgetContextToken\n, which is returned in the\nVertex AI API response and can be used to render visual content.\nThe contextual widget serves different functions depending on your scenario:\nIt displays user reviews and photos, which is user-generated content (UGC), in\nthe scenario where Google Maps prompting is used for answer generation.\nIt helps to enrich results with map visualizations and data when\nVertex AI generates just a text response.\nFor more information on the contextual widget, see\nMaps grounding\nwidget\n.\nRender the Google Maps contextual widget\nTo render and use the Google Maps contextual widget, use the alpha\nversion of the Google Maps JavaScript API on the page that displays the\nwidget. For more information, see\nLoad\nthe Maps JavaScript API\n.\nThe following code samples demonstrate how to use a contextual widget:\nCreate a contextual widget.\n<body>\n<gmp-place-contextual id=\"widget\"></gmp-place-contextual>\n</body>\nIn any response that is grounded with Google Maps, there's a\ncorresponding\ngoogleMapsWidgetContextToken\nthat's used to render the\ncontextual widget and placed in close proximity to the generated response.\nTo update the context token, set the\nwidget.contextToken property\n.\n\"googleMapsWidgetContextToken\"\n:\n\"widgetcontent/AcBXPQdpWQWbap9H-OH8sEKmOXxmEKAYvff0tvthhneMQC3VrqWCjpnPBl4-Id98FGiA_S_t8aeAeJj0T6JkWFX56Bil8oBSR0W8JH3C_RSYLbTjxKdpxc9yNn6JcZTtolIRZon9xi6WpNGuSyjcIxWu2S0hwpasNOpUlWrG1RxVCB4WD1fsz_pwR236mG36lMxevXTQ_JnfdYNuQwQ4Lc3vn...<snip>...\nTs5VJE_b3IC5eE_6wez0nh61r7foTUZpP7BXMwxR-7Wyfcj6x1v6mIWsFGr1o0p_HSAMNqWPg-aFVnkPLhAkOR6MaNZOfezTva-gxHlu7z_haFvYxcUE1qfNVQ\"\n,\nfunction\nupdateWidget\n(\ncontextToken\n)\n{\nlet\nwidget\n=\ndocument\n.\nquerySelector\n(\n'#widget'\n);\nwidget\n.\ncontextToken\n=\ncontextToken\n;\n}\nOptional: Specify the list layout. Valid values include the following:\nCompact layout\n:\n<gmp-place-contextual-list-config layout=\"compact\">\nVertical layout\n:\n<gmp-place-contextual-list-config layout=\"vertical\">\nThis code sample demonstrates changing the list layout to a compact layout.\n<gmp-place-contextual id=\"widget\">\n<gmp-place-contextual-list-config layout=\"compact\">\n</gmp-place-contextual-list-config>\n</gmp-place-contextual>\nOptional: Change the map mode. Valid values include the following:\n2D roadmap map\n:\nmap-mode=\"roadmap\"\n3D hybrid map\n:\nmap-mode=\"hybrid\"\nNo map\n:\nmap-mode=\"none\"\nThis code sample demonstrates changing the map mode to a 2D map.\n<gmp-place-contextual id=\"widget\">\n<gmp-place-contextual-list-config map-mode=\"roadmap\">\n</gmp-place-contextual-list-config>\n</gmp-place-contextual>\nWhat's next\nTo learn more about how to ground Gemini models to your data, see\nGrounding with\nyour data\n.\nTo learn more about responsible AI best practices and Vertex AI's\nsafety filters, see\nResponsible AI\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 17617,
      "line_count": 780
    },
    {
      "filename": "vertex-ai_generative-ai_docs_grounding_grounding-with-google-search.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGrounding with Google Search\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains how to ground a model's responses using\nGoogle Search, which uses publicly-available web data. Also,\nSearch suggestions are explained, which are included in\nyour responses.\nGrounding with Google Search\nIf you want to connect your model with world knowledge, a wide possible range of\ntopics, or up-to-date information on the Internet, then use Grounding with Google Search.\nTo learn more about model grounding in Vertex AI,\nsee the\nGrounding overview\n.\nSupported models\nThis section lists the models that support grounding with\nSearch.\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nSupported languages\nFor a list of supported languages, see\nLanguages\n.\nGround your model with Google Search\nUse the following instructions to ground a model with publicly available web\ndata.\nConsiderations\nTo use grounding with Google Search, you must enable\nGoogle Search Suggestions. For more information, see\nUse Google Search suggestions\n.\nFor ideal results, use a temperature of\n1.0\n. To learn more about setting\nthis configuration, see the\nGemini API request\nbody\nfrom the model reference.\nGrounding with Google Search has a limit of one million queries per day. If\nyou require more queries, contact\nGoogle Cloud support\nfor assistance.\nSearch results can be customized for a specific geographic location of the end\nuser by using the latitude and longitude coordinates. For more information, see\nthe\nGrounding\nAPI.\nConsole\nTo use Grounding with Google Search with the Vertex AI Studio, follow these steps:\nIn the Google Cloud console, go to the\nVertex AI Studio\npage.\nGo to\nVertex AI Studio\nClick the\nFreeform\ntab.\nIn the side panel, click the\nGround model responses\ntoggle.\nClick\nCustomize\nand set Google Search as the source.\nEnter your prompt in the text box and click\nSubmit\n.\nYour prompt responses now ground to Google Search.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nGenerateContentConfig\n,\nGoogleSearch\n,\nHttpOptions\n,\nTool\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"When is the next total solar eclipse in the United States?\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\n# Use Google Search Tool\nTool\n(\ngoogle_search\n=\nGoogleSearch\n())\n],\n),\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# 'The next total solar eclipse in the United States will occur on ...'\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithGoogleSearch\nshows\nhow\nto\ngenerate\ntext\nusing\nGoogle\nSearch\n.\nfunc\ngenerateWithGoogleSearch\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"When is the next total solar eclipse in the United States?\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nTools\n:\n[]\n*\ngenai\n.\nTool\n{\n{\nGoogleSearch\n:\n&\ngenai\n.\nGoogleSearch\n{}},\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThe\nnext\ntotal\nsolar\neclipse\nin\nthe\nUnited\nStates\nwill\noccur\non\nMarch\n30\n,\n2033\n,\nbut\nit\nwill\nonly\n...\nreturn\nnil\n}\nJava\nLearn how to install or update the\nJava\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.GenerateContentConfig\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.GoogleSearch\n;\nimport\ncom.google.genai.types.HttpOptions\n;\nimport\ncom.google.genai.types.Tool\n;\npublic\nclass\nToolsGoogleSearchWithText\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\nmodelId\n=\n\"gemini-2.5-flash\"\n;\ngenerateContent\n(\nmodelId\n);\n}\n//\nGenerates\ntext\nwith\nGoogle\nSearch\ntool\npublic\nstatic\nString\ngenerateContent\n(\nString\nmodelId\n)\n{\n//\nInitialize\nclient\nthat\nwill\nbe\nused\nto\nsend\nrequests\n.\nThis\nclient\nonly\nneeds\nto\nbe\ncreated\n//\nonce\n,\nand\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nhttpOptions\n(\nHttpOptions\n.\nbuilder\n()\n.\napiVersion\n(\n\"v1\"\n)\n.\nbuild\n())\n.\nbuild\n())\n{\n//\nCreate\na\nGenerateContentConfig\nand\nset\nGoogle\nSearch\ntool\nGenerateContentConfig\ncontentConfig\n=\nGenerateContentConfig\n.\nbuilder\n()\n.\ntools\n(\nTool\n.\nbuilder\n()\n.\ngoogleSearch\n(\nGoogleSearch\n.\nbuilder\n()\n.\nbuild\n())\n.\nbuild\n())\n.\nbuild\n();\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\n\"When is the next total solar eclipse in the United States?\"\n,\ncontentConfig\n);\nSystem\n.\nout\n.\nprint\n(\nresponse\n.\ntext\n());\n//\nExample\nresponse\n:\n//\nThe\nnext\ntotal\nsolar\neclipse\nin\nthe\nUnited\nStates\nwill\noccur\non\n...\nreturn\nresponse\n.\ntext\n();\n}\n}\n}\nREST\nBefore using any of the request data,\nmake the following replacements:\nLOCATION\n: The region to process the request. To use the\nglobal endpoint\n, exclude the location from the endpoint name and configure the location of the resource to global.\nPROJECT_ID\n: Your\nproject ID\n.\nMODEL_ID\n: The model ID of the multimodal model.\nTEXT\n:\nThe text instructions to include in the prompt.\nEXCLUDE_DOMAINS\n: Optional: List of domains that aren't to be used for grounding.\nLATITUDE\n: Optional: The latitude of the end user's location. For example, a latitude of\n37.7749\nrepresents San Francisco. You can obtain latitude and longitude coordinates using services like Google Maps or other geocoding tools.\nLONGITUDE\n: Optional: The longitude of the end user's location. For example, a longitude of\n-122.4194\nrepresents San Francisco.\nHTTP method and URL:\nPOST https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\nRequest JSON body:\n{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"\nTEXT\n\"\n}]\n}],\n\"tools\": [{\n\"googleSearch\": {\n\"exclude_domains\": [ \"domain.com\", \"domain2.com\" ]\n}\n}],\n\"toolConfig\": {\n\"retrievalConfig\": {\n\"latLng\": {\n\"latitude\":\nLATITUDE\n,\n\"longitude\":\nLONGITUDE\n}\n}\n},\n\"model\": \"projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n\"\n}\nTo send your request, expand one of these options:\ncurl (Linux, macOS, or Cloud Shell)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\"\nPowerShell (Windows)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\" | Select-Object -Expand Content\nYou should receive a JSON response similar to the following:\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"text\": \"The weather in Chicago this weekend, will be partly cloudy. The temperature will be between 49°F (9°C) and 55°F (13°C) on Saturday and between 51°F (11°C) and 56°F (13°C) on Sunday. There is a slight chance of rain on both days.\\n\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n\"groundingMetadata\": {\n\"webSearchQueries\": [\n\"weather in Chicago this weekend\"\n],\n\"searchEntryPoint\": {\n\"renderedContent\": \"...\"\n},\n\"groundingChunks\": [\n{\n\"web\": {\n\"uri\": \"https://www.google.com/search?q=weather+in+Chicago,+IL\",\n\"title\": \"Weather information for locality: Chicago, administrative_area: IL\",\n\"domain\": \"google.com\"\n}\n},\n{\n\"web\": {\n\"uri\": \"...\",\n\"title\": \"weatherbug.com\",\n\"domain\": \"weatherbug.com\"\n}\n}\n],\n\"groundingSupports\": [\n{\n\"segment\": {\n\"startIndex\": 85,\n\"endIndex\": 214,\n\"text\": \"The temperature will be between 49°F (9°C) and 55°F (13°C) on Saturday and between 51°F (11°C) and 56°F (13°C) on Sunday.\"\n},\n\"groundingChunkIndices\": [\n0\n],\n\"confidenceScores\": [\n0.8662828\n]\n},\n{\n\"segment\": {\n\"startIndex\": 215,\n\"endIndex\": 261,\n\"text\": \"There is a slight chance of rain on both days.\"\n},\n\"groundingChunkIndices\": [\n1,\n0\n],\n\"confidenceScores\": [\n0.62836814,\n0.6488607\n]\n}\n],\n\"retrievalMetadata\": {}\n}\n}\n],\n\"usageMetadata\": {\n\"promptTokenCount\": 10,\n\"candidatesTokenCount\": 98,\n\"totalTokenCount\": 108,\n\"trafficType\": \"ON_DEMAND\",\n\"promptTokensDetails\": [\n{\n\"modality\": \"TEXT\",\n\"tokenCount\": 10\n}\n],\n\"candidatesTokensDetails\": [\n{\n\"modality\": \"TEXT\",\n\"tokenCount\": 98\n}\n]\n},\n\"modelVersion\": \"gemini-2.0-flash\",\n\"createTime\": \"2025-05-19T14:42:55.000643Z\",\n\"responseId\": \"b0MraIMFoqnf-Q-D66G4BQ\"\n}\nUnderstand your response\nIf your model prompt successfully grounds to Google Search from the\nVertex AI Studio or from the API, then the responses include metadata with\nsource links (web URLs). However, there are several reasons this metadata might\nnot be provided, and the prompt response won't be grounded. These reasons\ninclude low source relevance or incomplete information within the model's\nresponse.\nGrounding support\nDisplaying grounding support is required, because it aids you in validating\nresponses from the publishers and adds avenues for further learning.\nGrounding support for responses from Google Search sources should be shown\nboth inline and in aggregate. For example, see the following image as a\nsuggestion on how to do this.\nUse of alternative search engine options\nWhen using Grounding with Google Search a Customer Application can:\nOffer alternative search engine options,\nMake other search engines the default option,\nDisplay their own or third-party search suggestions or search results as long\nas: any non-Google results must be displayed separately from Google's Grounded\nResults and Search Suggestions, and shown in a way that does not confuse\nusers or suggest they are from Google.\nBenefits\nThe following complex prompts and workflows that require planning, reasoning,\nand thinking can be done when you use Google Search as a tool:\nYou can ground to help ensure responses are based on the latest and most\naccurate information.\nYou can retrieve artifacts from the web to do analysis.\nYou can find relevant images, videos, or other media to assist in multimodal\nreasoning or task generation.\nYou can perform coding, technical troubleshooting, and other specialized\ntasks.\nYou can find region-specific information, or assist in translating content\naccurately.\nYou can find relevant websites for browsing.\nUse Google Search suggestions\nWhen you use grounding with Google Search, and you receive\nSearch suggestions in your response, you must display the\nSearch suggestions in production and in your applications.\nFor more information on grounding with Google Search, see\nGrounding with Google Search\n.\nSpecifically, you must display the search queries that are included in the\ngrounded response's metadata. The response includes:\n\"content\"\n: LLM-generated response.\n\"webSearchQueries\"\n: The queries to be used for\nSearch suggestions.\nFor example, in the following code snippet, Gemini responds to a\nSearch grounded prompt, which is asking about a type of\ntropical plant.\n\"predictions\"\n:\n[\n{\n\"content\"\n:\n\"Monstera is a type of vine that thrives in bright indirect light…\"\n,\n\"groundingMetadata\"\n:\n{\n\"webSearchQueries\"\n:\n[\n\"What's a monstera?\"\n],\n}\n}\n]\nYou can take this output, and display it by using Search\nsuggestions.\nRequirements for Search suggestions\nThe following are requirements for suggestions:\nRequirement\nDescription\nDo\nWhile complying with the\ndisplay\nrequirements\n, the Search suggestion is displayed exactly as provided without any changes.\nWhen you interact with the Search suggestion, you are\ntaken directly to the Search results page (SRP).\nDon't\nInclude any screens or additional steps between the user's tap and the display\nof the SRP.\nDisplay any other search results or suggestions next to the\nSearch suggestion or the associated grounded LLM\nresponse.\nDisplay requirements\nThe following are the display requirements:\nDisplay the Search suggestion exactly as provided, and\ndon't make any modifications to colors, fonts, or appearance. Ensure the\nSearch suggestion renders as specified in the following\nmocks such as light and dark mode:\nWhenever a grounded response is shown, its corresponding\nSearch suggestion should remain visible.\nFor branding, you must strictly follow Google's guidelines for third-party use\nof Google brand features at the\nWelcome to our Brand Resource\nCenter\n.\nWhen you use grounding with Search,\nSearch suggestion chips display. The field that contains\nthe suggestion chips must be the same width as the grounded response from the\nLLM.\nBehavior on tap\nWhen a user taps the chip, they are taken directly to a\nSearch results page (SRP) for the search term displayed in\nthe chip. The SRP can open either within your in-application browser or in a\nseparate browser application. It's important to not minimize, remove, or\nobstruct the SRP's display in any way. The following animated mockup illustrates\nthe tap-to-SRP interaction.\nCode to implement a Search suggestion\nWhen you use the API to ground a response to search, the model response provides\ncompliant HTML and CSS styling in the\nrenderedContent\nfield, which you\nimplement to display Search suggestions in your\napplication.\nWhat's next\nTo learn more about grounding, see\nGrounding overview\n.\nTo learn how to send chat prompt requests, see\nMultiturn chat\n.\nTo learn about responsible AI best practices and Vertex AI's safety filters,\nsee\nSafety best practices\n.\nLearn how to\nsend chat prompt requests\n.\nLearn about\nresponsible AI best practices and Vertex AI safety filters\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 16207,
      "line_count": 1007
    },
    {
      "filename": "vertex-ai_generative-ai_docs_grounding_grounding-with-vertex-ai-search.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-vertex-ai-search",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-vertex-ai-search\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGrounding with Vertex AI Search\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains how you can ground responses by using your data from\nVertex AI Search.\nGrounding Gemini to your data\nIf you want to do retrieval-augmented generation (RAG), connect your model to\nyour website data or your sets of documents, then use\nGrounding with\nVertex AI Search\n.\nGrounding to your data supports a maximum of 10 Vertex AI Search\ndata sources and can be combined with\nGrounding with\nGoogle Search\n.\nSupported models\nThis section lists the models that support grounding with your data.\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nPrerequisites\nBefore you can ground model output to your data, do the following:\nIn the Google Cloud console, go to the\nIAM\npage, and search for the\ndiscoveryengine.servingConfigs.search\npermission, which is required for the\ngrounding service to work.\nGo to IAM\nEnable AI Applications\nand activate the API.\nCreate a AI Applications data source\nand\napplication.\nSee the\nIntroduction to Vertex AI Search\nfor more.\nEnable AI Applications\nTo use Vertex AI Search to ground your responses, you must activate the\nVertex AI Search service by following these steps:\nIn the Google Cloud console, go to the\nAI Applications\npage.\nGo to AI Applications\nOptional: Review the\nterms for data use\n.\nAI Applications is available in the\nglobal\nlocation or the\neu\nand\nus\nmulti-region. To learn more, see\nAI Applications\nlocations\n.\nCreate a data store in AI Applications\nTo create a data store in AI Applications, you can choose to\nground with website data or documents.\nWebsite\nOpen the\nCreate Data\nStore\npage from the Google Cloud console.\nIn\nWebsite Content\nbox, click\nSelect\n.\nSpecify the\nwebsites for your data store\npane displays.\nIf\nAdvanced website indexing\nisn't checked, then select the\nAdvanced\nwebsite indexing\ncheckbox to turn it on.\nConfigure your data store\npane displays.\nIn the\nSpecify URL patterns to index\nsection, do the following:\nAdd URLs for\nSites to include\n.\nOptional: Add URLs for\nSites to exclude\n.\nClick\nContinue\n.\nIn the\nConfigure your data store\npane,\nSelect a value from the\nLocation of your data store\nlist.\nEnter a name in the\nYour data store name\nfield. The ID is\ngenerated. Use this ID when you generate your grounded responses with\nyour data store. For more information, see\nGenerate grounded responses\nwith your data store\n.\nClick\nCreate\n.\nDocuments\nOpen the\nCreate Data\nStore\npage from the Google Cloud console.\nIn\nCloud Storage\nbox, click\nSelect\n.\nImport data from\nCloud Storage\npane displays.\nIn the\nUnstructured documents (PDF, HTML, TXT and more)\nsection, select\nUnstructured documents (PDF, HTML, TXT and more)\n.\nSelect a\nSynchronization frequency\noption.\nSelect a\nSelect a folder or a file you want to import\noption, and\nenter the path in the field.\nClick\nContinue\n.\nConfigure your data store\npane displays.\nIn the\nConfigure your data store\npane,\nSelect a value from the\nLocation of your data store\nlist.\nEnter a name in the\nYour data store name\nfield. The ID is\ngenerated.\nTo select parsing and chunking options for your documents, expand the\nDocument Processing Options\nsection. For more information about\ndifferent parsers, see\nParse\ndocuments\n.\nClick\nCreate\n.\nClick\nCreate\n.\nGenerate grounded responses with your data store\nUse the following instructions to ground a model with your data. A maximum\nof 10 data stores is supported.\nIf you don't know your data store ID, follow these steps:\nIn the Google Cloud console, go to the\nAI Applications\npage and\nin the navigation menu, click\nData stores\n.\nGo to the Data stores page\nClick the name of your data store.\nOn the\nData\npage for your data store, get the data store ID.\nConsole\nTo ground your model output to AI Applications by using Vertex AI Studio in the\nGoogle Cloud console, follow these steps:\nIn the Google Cloud console, go to the\nVertex AI Studio Freeform\npage.\nGo to\nFreeform\nTo turn on grounding, click the\nGrounding: your data\ntoggle.\nClick\nCustomize\n.\nSelect\nVertex AI Search\nas your source.\nUsing this path format, replace your data store's Project ID and\nthe ID of the data store:\nprojects/\nPROJECT_ID\n/locations/global/collections/default_collection/dataStores/\nDATASTORE_ID\n.\nClick\nSave\n.\nEnter your prompt in the text box, and click\nSubmit\n.\nYour prompt responses are grounded to AI Applications.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nGenerateContentConfig\n,\nVertexAISearch\n,\nRetrieval\n,\nTool\n,\nHttpOptions\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\n# Replace with your Vertex AI Search data store details\nDATASTORE_PATH\n=\n\"projects/\nPROJECT_ID\n/locations/global/collections/default_collection/dataStores/DATASTORE_ID\"\ntool\n=\nTool\n(\nretrieval\n=\nRetrieval\n(\nvertex_ai_search\n=\nVertexAISearch\n(\ndatastore\n=\nDATASTORE_PATH\n)\n)\n)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\n# Or another supported model\ncontents\n=\n\"What information can you find about topic X in the provided documents?\"\n,\n# Your query\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\ntool\n],\n),\n)\nprint\n(\nresponse\n.\ntext\n)\nREST\nTo test a text prompt by using the Vertex AI API, send a POST request to the\npublisher model endpoint.\nBefore using any of the request data,\nmake the following replacements:\nLOCATION\n: The region to process the request. To use the\nglobal\nendpoint\n, exclude the location from the endpoint name, and configure the location of the resource to\nglobal\n.\nPROJECT_ID\n: Your\nproject ID\n.\nMODEL_ID\n: The model ID of the multimodal model.\nPROMPT\n: The prompt to send to the model.\nHTTP method and URL:\nPOST https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\nRequest JSON body:\n{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"\nPROMPT\n\"\n}]\n}],\n\"tools\": [{\n\"retrieval\": {\n\"vertexAiSearch\": {\n\"datastore\": projects/\nPROJECT_ID\n/locations/global/collections/default_collection/dataStores/\nDATASTORE_ID\n}\n}\n}],\n\"model\": \"projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n\"\n}\nTo send your request, expand one of these options:\ncurl (Linux, macOS, or Cloud Shell)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\"\nPowerShell (Windows)\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://\nLOCATION\n-aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent\" | Select-Object -Expand Content\nYou should receive a JSON response similar to the following:\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"text\": \"You can make an appointment on the website https://dmv.gov/\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n\"safetyRatings\": [\n\"...\"\n],\n\"groundingMetadata\": {\n\"retrievalQueries\": [\n\"How to make appointment to renew driving license?\"\n],\n\"groundingChunks\": [\n{\n\"retrievedContext\": {\n\"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AXiHM.....QTN92V5ePQ==\",\n\"title\": \"dmv\"\n}\n}\n],\n\"groundingSupport\": [\n{\n\"segment\": {\n\"startIndex\": 25,\n\"endIndex\": 147\n},\n\"segment_text\": \"ipsum lorem ...\",\n\"supportChunkIndices\": [1, 2],\n\"confidenceScore\": [0.9541752, 0.97726375]\n},\n{\n\"segment\": {\n\"startIndex\": 294,\n\"endIndex\": 439\n},\n\"segment_text\": \"ipsum lorem ...\",\n\"supportChunkIndices\": [1],\n\"confidenceScore\": [0.9541752, 0.9325467]\n}\n]\n}\n}\n],\n\"usageMetadata\": {\n\"...\"\n}\n}\nUnderstand your response\nThe response from both APIs include the LLM-generated text, which is called a\ncandidate\n. If your model prompt successfully grounds to your data source, then\nthe responses include grounding metadata, which identifies the parts of the\nresponse that were derived from your data. However, there are several reasons\nthis metadata may not be provided, and the prompt response won't be grounded.\nThese reasons include low-source relevance or incomplete information within the\nmodel's response.\nThe following is a breakdown of the output data:\nRole\n: Indicates the sender of the grounded answer. Because the response\nalways contains grounded text, the role is always\nmodel\n.\nText\n: The grounded answer generated by the LLM.\nGrounding metadata\n: Information about the grounding source, which contains\nthe following elements:\nGrounding chunks\n: A list of results from your index that support the\nanswer.\nGrounding supports\n: Information about a specific claim within the answer\nthat can be used to show citations:\nSegment\n: The part of the model's answer that is substantiated by a\ngrounding chunk.\nGrounding chunk index\n: The index of the grounding chunks in the\ngrounding chunks list that corresponds to this claim.\nConfidence scores\n: A number from 0 to 1 that indicates how grounded\nthe claim is in the provided set of grounding chunks. Not available for\nGemini 2.5 and later.\nWhat's next\nTo learn how to send chat prompt requests, see\nMultiturn chat\n.\nTo learn about responsible AI best practices and Vertex AI's safety filters,\nsee\nSafety best practices\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 10794,
      "line_count": 546
    },
    {
      "filename": "vertex-ai_generative-ai_docs_grounding_overview.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/grounding/overview\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGrounding overview\nStay organized with collections\nSave and categorize content based on your preferences.\nIn generative AI, grounding is the ability to connect model output to verifiable\nsources of information. If you provide models with access to specific data\nsources, then grounding tethers their output to these data and reduces the\nchances of inventing content. This is particularly important in situations where\naccuracy and reliability are significant.\nGrounding provides the following benefits:\nReduces model hallucinations, which are instances where the model generates\ncontent that isn't factual.\nAnchors model responses to your data sources.\nProvides auditability by\nproviding grounding support, which are links to sources.\nYou can ground supported-model output in Vertex AI in the following ways:\nGrounding type\nDescription\nGrounding with Google Search\nConnect your model to world knowledge and a wide possible range of topics using Google Search.\nGrounding with Google Maps\nUse Google Maps data with your model to provide more accurate and context-aware responses to your prompts, including geospatial context.\nGrounding with Vertex AI Search\nUse retrieval-augmented generation (RAG) to connect your model to your website data or your sets of documents stored in Vertex AI Search.\nGrounding with Elasticsearch\nUse retrieval-augmented generation with your existing Elasticsearch indexes and Gemini.\nGrounding with your search API\nConnect Gemini to your external data sources by grounding with any search API.\nWeb Grounding for Enterprise\nUse a web index suitable for highly-regulated industries to generate grounded responses with compliance controls.\nFor language support, see\nSupported languages for prompts\n.\nWhat's next\nTo learn more about responsible AI best practices and Vertex AI's\nsafety filters, see\nResponsible AI\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 2420,
      "line_count": 54
    },
    {
      "filename": "vertex-ai_generative-ai_docs_live-api.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nLive API\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Live API enables low-latency, two-way voice and video interactions\nwith Gemini. Use the Live API to provide end users with\nnatural, human-like voice conversations, including the ability to interrupt the\nmodel's responses with voice commands.\nThis document covers the basics of using Live API, including its\ncapabilities, starter examples, and basic use case code examples. If you're\nlooking for information on how to start an interactive conversation using the\nLive API, see\nInteractive conversations with the Live API\n. If you're looking for information\non what tools the Live API can use, see\nBuilt-in\ntools\n.\nTry in Vertex AI\nSupported models\nThe Live API is supported for use in both the Google Gen AI SDK and\nusing Vertex AI Studio. Some features (like text input and output) are\nonly available using the Gen AI SDK.\nYou can use the Live API with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash\nPrivate GA\n*\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\ngemini-live-2.5-flash-preview-native-audio\nPublic preview; Discontinuation date: October 18, 2025\n*\nReach out to your Google account team representative to request\naccess.\nFor more information including technical specifications and limitations, see the\nLive API reference\nguide\n.\nLive API capabilities\nReal-time multimodal\nunderstanding:\nConverse\nwith Gemini about what it sees on a video feed or through screen\nsharing, using built-in support for streaming audio and video.\nBuilt-in tool usage:\nSeamlessly integrate\ntools like\nfunction calling\nand\nGrounding with Google Search\ninto your\nconversations for more practical and dynamic interactions.\nLow latency interactions:\nHave low latency, human-like interactions with\nGemini.\nMultilingual support:\nConverse in 24 supported languages.\n(GA versions only)\nSupport for\nProvisioned Throughput\n:\nUse fixed-cost, fixed-term subscription\navailable in several term-lengths that reserves throughput for supported\ngenerative AI models on Vertex AI, including Live API.\nHigh-quality transcription:\nLive API supports\ntext transcription\nfor both input and output audio.\nGemini 2.5 Flash with Live API also includes\nnative audio\nas a public preview offering. Native audio introduces:\nAffective Dialog:\nLive API\nunderstands and responds to the user's tone of voice. The same words spoken\nin different ways can lead to vastly different and more nuanced\nconversations.\nProactive Audio and context awareness:\nLive API intelligently disregards ambient conversations and other\nirrelevant audio, understanding when to listen and when to remain silent.\nFor more information on native audio, see\nBuilt-in\ntools\n.\nSupported audio formats\nThe Live API supports the following audio formats:\nInput audio:\nRaw 16-bit PCM audio at 16kHz, little-endian\nOutput audio:\nRaw 16-bit PCM audio at 24kHz, little-endian\nSupported video formats\nLive API supports video frames input at 1FPS. For best results, use\nnative 768x768 resolution at 1FPS.\nStarter examples\nYou can get started using the Live API with one of the following\nnotebook tutorials, demo applications, or guides.\nNotebook tutorials\nDownload these notebook tutorials from GitHub, or open the notebook tutorials in\nthe environment of your choice.\nUse WebSockets with the Live API\nStreaming audio and video\nDemo applications and guides\nWebSocket demo app\nAgent Development Kit: Custom Audio Streaming\nProject Livewire\nAdditional examples\nTo get even more utility from Live API, try these examples that use\nLive API's audio processing, transcription, and voice response\ncapabilities.\nGet text responses from audio input\nYou can send audio and receive text responses by converting the audio to a\n16-bit PCM, 16kHz, mono format. The following example reads a WAV file and sends\nit in the correct format:\nPython\n# Test file: https://storage.googleapis.com/generativeai-downloads/data/16000.wav\n# Install helpers for converting files: pip install librosa soundfile\nimport\nasyncio\nimport\nio\nfrom\npathlib\nimport\nPath\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nimport\nsoundfile\nas\nsf\nimport\nlibrosa\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"TEXT\"\n]}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nbuffer\n=\nio\n.\nBytesIO\n()\ny\n,\nsr\n=\nlibrosa\n.\nload\n(\n\"sample.wav\"\n,\nsr\n=\n16000\n)\nsf\n.\nwrite\n(\nbuffer\n,\ny\n,\nsr\n,\nformat\n=\n\"RAW\"\n,\nsubtype\n=\n\"PCM_16\"\n)\nbuffer\n.\nseek\n(\n0\n)\naudio_bytes\n=\nbuffer\n.\nread\n()\n# If already in correct format, you can use this:\n# audio_bytes = Path(\"sample.pcm\").read_bytes()\nawait\nsession\n.\nsend_realtime_input\n(\naudio\n=\ntypes\n.\nBlob\n(\ndata\n=\naudio_bytes\n,\nmime_type\n=\n\"audio/pcm;rate=16000\"\n)\n)\nasync\nfor\nresponse\nin\nsession\n.\nreceive\n():\nif\nresponse\n.\ntext\nis\nnot\nNone\n:\nprint\n(\nresponse\n.\ntext\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nGet voice responses from text input\nUse this example to send text input and receive synthesized speech responses:\nPython\nimport\nasyncio\nimport\nnumpy\nas\nnp\nfrom\nIPython.display\nimport\nAudio\n,\nMarkdown\n,\ndisplay\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nContent\n,\nLiveConnectConfig\n,\nHttpOptions\n,\nModality\n,\nPart\n,\nSpeechConfig\n,\nVoiceConfig\n,\nPrebuiltVoiceConfig\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nvoice_name\n=\n\"Aoede\"\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nspeech_config\n=\nSpeechConfig\n(\nvoice_config\n=\nVoiceConfig\n(\nprebuilt_voice_config\n=\nPrebuiltVoiceConfig\n(\nvoice_name\n=\nvoice_name\n,\n)\n),\n),\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\n\"gemini-live-2.5-flash\"\n,\nconfig\n=\nconfig\n,\n)\nas\nsession\n:\ntext_input\n=\n\"Hello? Gemini are you there?\"\ndisplay\n(\nMarkdown\n(\nf\n\"**Input:**\n{\ntext_input\n}\n\"\n))\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n(\ntext\n=\ntext_input\n)]))\naudio_data\n=\n[]\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\n(\nmessage\n.\nserver_content\n.\nmodel_turn\nand\nmessage\n.\nserver_content\n.\nmodel_turn\n.\nparts\n):\nfor\npart\nin\nmessage\n.\nserver_content\n.\nmodel_turn\n.\nparts\n:\nif\npart\n.\ninline_data\n:\naudio_data\n.\nappend\n(\nnp\n.\nfrombuffer\n(\npart\n.\ninline_data\n.\ndata\n,\ndtype\n=\nnp\n.\nint16\n)\n)\nif\naudio_data\n:\ndisplay\n(\nAudio\n(\nnp\n.\nconcatenate\n(\naudio_data\n),\nrate\n=\n24000\n,\nautoplay\n=\nTrue\n))\nFor more examples of sending text, see\nour Getting Started\nguide\n.\nTranscribe audio\nThe Live API can transcribe both input and output audio. Use the\nfollowing example to enable transcription:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"AUDIO\"\n],\n\"input_audio_transcription\"\n:\n{},\n\"output_audio_transcription\"\n:\n{}\n}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nmessage\n=\n\"Hello? Gemini are you there?\"\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\n{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[{\n\"text\"\n:\nmessage\n}]},\nturn_complete\n=\nTrue\n)\nasync\nfor\nresponse\nin\nsession\n.\nreceive\n():\nif\nresponse\n.\nserver_content\n.\nmodel_turn\n:\nprint\n(\n\"Model turn:\"\n,\nresponse\n.\nserver_content\n.\nmodel_turn\n)\nif\nresponse\n.\nserver_content\n.\ninput_transcription\n:\nprint\n(\n\"Input transcript:\"\n,\nresponse\n.\nserver_content\n.\ninput_transcription\n.\ntext\n)\nif\nresponse\n.\nserver_content\n.\noutput_transcription\n:\nprint\n(\n\"Output transcript:\"\n,\nresponse\n.\nserver_content\n.\noutput_transcription\n.\ntext\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nWebSockets\n# Set model generation_config\nCONFIG\n=\n{\n'response_modalities'\n:\n[\n'AUDIO'\n],\n}\nheaders\n=\n{\n\"Content-Type\"\n:\n\"application/json\"\n,\n\"Authorization\"\n:\nf\n\"Bearer\n{\nbearer_token\n[\n0\n]\n}\n\"\n,\n}\n# Connect to the server\nasync\nwith\nconnect\n(\nSERVICE_URL\n,\nadditional_headers\n=\nheaders\n)\nas\nws\n:\n# Setup the session\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\n{\n\"setup\"\n:\n{\n\"model\"\n:\n\"gemini-2.0-flash-live-preview-04-09\"\n,\n\"generation_config\"\n:\nCONFIG\n,\n'input_audio_transcription'\n:\n{},\n'output_audio_transcription'\n:\n{}\n}\n}\n)\n)\n# Receive setup response\nraw_response\n=\nawait\nws\n.\nrecv\n(\ndecode\n=\nFalse\n)\nsetup_response\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n(\n\"ascii\"\n))\n# Send text message\ntext_input\n=\n\"Hello? Gemini are you there?\"\ndisplay\n(\nMarkdown\n(\nf\n\"**Input:**\n{\ntext_input\n}\n\"\n))\nmsg\n=\n{\n\"client_content\"\n:\n{\n\"turns\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[{\n\"text\"\n:\ntext_input\n}]}],\n\"turn_complete\"\n:\nTrue\n,\n}\n}\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\nmsg\n))\nresponses\n=\n[]\ninput_transcriptions\n=\n[]\noutput_transcriptions\n=\n[]\n# Receive chucks of server response\nasync\nfor\nraw_response\nin\nws\n:\nresponse\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n())\nserver_content\n=\nresponse\n.\npop\n(\n\"serverContent\"\n,\nNone\n)\nif\nserver_content\nis\nNone\n:\nbreak\nif\n(\ninput_transcription\n:=\nserver_content\n.\nget\n(\n\"inputTranscription\"\n))\nis\nnot\nNone\n:\nif\n(\ntext\n:=\ninput_transcription\n.\nget\n(\n\"text\"\n))\nis\nnot\nNone\n:\ninput_transcriptions\n.\nappend\n(\ntext\n)\nif\n(\noutput_transcription\n:=\nserver_content\n.\nget\n(\n\"outputTranscription\"\n))\nis\nnot\nNone\n:\nif\n(\ntext\n:=\noutput_transcription\n.\nget\n(\n\"text\"\n))\nis\nnot\nNone\n:\noutput_transcriptions\n.\nappend\n(\ntext\n)\nmodel_turn\n=\nserver_content\n.\npop\n(\n\"modelTurn\"\n,\nNone\n)\nif\nmodel_turn\nis\nnot\nNone\n:\nparts\n=\nmodel_turn\n.\npop\n(\n\"parts\"\n,\nNone\n)\nif\nparts\nis\nnot\nNone\n:\nfor\npart\nin\nparts\n:\npcm_data\n=\nbase64\n.\nb64decode\n(\npart\n[\n\"inlineData\"\n][\n\"data\"\n])\nresponses\n.\nappend\n(\nnp\n.\nfrombuffer\n(\npcm_data\n,\ndtype\n=\nnp\n.\nint16\n))\n# End of turn\nturn_complete\n=\nserver_content\n.\npop\n(\n\"turnComplete\"\n,\nNone\n)\nif\nturn_complete\n:\nbreak\nif\ninput_transcriptions\n:\ndisplay\n(\nMarkdown\n(\nf\n\"**Input transcription >**\n{\n''\n.\njoin\n(\ninput_transcriptions\n)\n}\n\"\n))\nif\nresponses\n:\n# Play the returned audio message\ndisplay\n(\nAudio\n(\nnp\n.\nconcatenate\n(\nresponses\n),\nrate\n=\n24000\n,\nautoplay\n=\nTrue\n))\nif\noutput_transcriptions\n:\ndisplay\n(\nMarkdown\n(\nf\n\"**Output transcription >**\n{\n''\n.\njoin\n(\noutput_transcriptions\n)\n}\n\"\n))\nPricing for Live API transcription is determined by the number of text\noutput tokens. To learn more, please see the\nVertex AI pricing page\n.\nMore information\nFor more information on using the Live API, see:\nBest practices with the Live API\nLive API reference guide\nInteractive conversations\nBuilt-in tools\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 11120,
      "line_count": 1144
    },
    {
      "filename": "vertex-ai_generative-ai_docs_live-api_best-practices.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/best-practices",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/best-practices\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nBest practices with the Live API\nStay organized with collections\nSave and categorize content based on your preferences.\nBefore getting started with the Live API, the following are some best\npractices for getting improved results from the models:\nUse clear system instructions:\nModels perform best when their roles and\nideal outputs are well-defined, along with explicit instructions that tell the\nmodel how to behave in response to prompts.\nDefine personas and roles:\nGive the model a clear role in the interaction,\nwith skills and knowledge defined, such as telling the model that it is a ship's\ncaptain well-versed in seafaring and ship maintenance during the Age of\nDiscovery.\nUse clear prompts:\nProvide examples of what the models should and\nshouldn't do in the prompts, and try to limit prompts to one prompt per persona\nor role at a time. Instead of lengthy, multi-page prompts, consider using prompt\nchaining instead. The model performs best on tasks with single function calls.\nProvide starting commands and information:\nThe Live API expects\nuser input before it responds. To have the Live API initiate the\nconversation, include a prompt asking it to greet the user or begin the\nconversation. Include information about the user to have the Live API\npersonalize that greeting.\nGuidelines for language specification\nFor optimal performance on Live API cascaded\ngemini-live-2.5-flash\n,\nmake sure that the API's\nlanguage_code\nmatches the language spoken by the\nuser.\nIf the expectation is for the model to respond in a non-English language,\ninclude the following as part of your system instructions:\nRESPOND IN {OUTPUT_LANGUAGE}. YOU MUST RESPOND UNMISTAKABLY IN {OUTPUT_LANGUAGE}.\nGuidelines for system instruction design\nTo get the best performance out of the Live API, we recommend having a\nclearly-defined set of system instructions (SIs) that defines the agent persona,\nconversational rules, guardrails, and tool definitions, in this order.\nFor best results, separate each agent into a distinct SI.\nSpecify the agent persona:\nProvide detail on the agent's name, role, and\nany preferred characteristics. If you want to specify the accent, be sure to\nalso specify the preferred output language (such as a British accent for an\nEnglish speaker).\nSpecify the conversational rules:\nPut these rules in the order you expect\nthe model to follow. Delineate between one-time elements of the conversation\nand conversational loops. For example:\nOne-time element:\nGather a customer's details once (such as name,\nlocation, loyalty card number).\nConversational loop:\nThe user can discuss recommendations, pricing,\nreturns, and delivery, and may want to go from topic to topic. Let the\nmodel know that it's OK to engage in this conversational loop for as\nlong as the user wants.\nSpecify tool calls within a flow in distinct sentences:\nFor example, if a\none-time step to gather a customer's details requires invoking a\nget_user_info\nfunction, you might say:\nYour first step is to gather user information. First,\nask the user to provide their name, location, and loyalty card number. Then\ninvoke\nget_user_info\nwith these details.\nAdd any necessary guardrails:\nProvide any general conversational\nguardrails you don't want the model to do. Feel free to provide specific\nexamples of if\nx\nhappens, you want the model to do\ny\n. If you're still not\ngetting the preferred level of precision, use the word\nunmistakably\nto guide the model to be precise.\nUse precise tool definitions:\nBe specific in your tool definitions. Be\nsure to tell Gemini under what conditions a tool call should be\ninvoked.\nExample\nThis example combines both the best practices and\nguidelines for system instruction design\nto\nguide the model's performance as a career coach.\n**Persona:**\nYou are Laura, a career coach from Brooklyn, NY. You specialize in providing\ndata driven advice to give your clients a fresh perspective on the career\nquestions they're navigating. Your special sauce is providing quantitative,\ndata-driven insights to help clients think about their issues in a different\nway. You leverage statistics, research, and psychology as much as possible.\nYou only speak to your clients in English, no matter what language they speak\nto you in.\n**Conversational Rules:**\n1. **Introduce yourself:** Warmly greet the client.\n2. **Intake:** Ask for your client's full name, date of birth, and state they're\ncalling in from. Call `create_client_profile` to create a new patient profile.\n3. **Discuss the client's issue:** Get a sense of what the client wants to\ncover in the session. DO NOT repeat what the client is saying back to them in\nyour response. Don't ask more than a few questions here.\n4. **Reframe the client's issue with real data:** NO PLATITUDES. Start providing\ndata-driven insights for the client, but embed these as general facts within\nconversation. This is what they're coming to you for: your unique thinking on\nthe subjects that are stressing them out. Show them a new way of thinking about\nsomething. Let this step go on for as long as the client wants. As part of this,\nif the client mentions wanting to take any actions, update\n`add_action_items_to_profile` to remind the client later.\n5. **Next appointment:** Call `get_next_appointment` to see if another\nappointment has already been scheduled for the client. If so, then share the\ndate and time with the client and confirm if they'll be able to attend. If\nthere is no appointment, then call `get_available_appointments` to see openings.\nShare the list of openings with the client and ask what they would prefer. Save\ntheir preference with `schedule_appointment`. If the client prefers to schedule\noffline, then let them know that's perfectly fine and to use the patient portal.\n**General Guidelines:** You're meant to be a witty, snappy conversational\npartner. Keep your responses short and progressively disclose more information\nif the client requests it. Don't repeat back what the client says back to them.\nEach response you give should be a net new addition to the conversation, not a\nrecap of what the client said. Be relatable by bringing in your own background\ngrowing up professionally in Brooklyn, NY. If a client tries to get you off\ntrack, gently bring them back to the workflow articulated above.\n**Guardrails:** If the client is being hard on themselves, never encourage that.\nRemember that your ultimate goal is to create a supportive environment for your\nclients to thrive.\nTool definitions\nThis JSON defines the relevant functions called in the career coach example.\nFor best results when defining functions, include their names, descriptions,\nparameters, and invocation conditions.\n[\n{\n\"name\"\n:\n\"create_client_profile\"\n,\n\"description\"\n:\n\"Creates a new client profile with their personal details. Returns a unique client ID. \\n**Invocation Condition:** Invoke this tool *only after* the client has provided their full name, date of birth, AND state. This should only be called once at the beginning of the 'Intake' step.\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"full_name\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The client's full name.\"\n},\n\"date_of_birth\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The client's date of birth in YYYY-MM-DD format.\"\n},\n\"state\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The 2-letter postal abbreviation for the client's state (e.g., 'NY', 'CA').\"\n}\n},\n\"required\"\n:\n[\n\"full_name\"\n,\n\"date_of_birth\"\n,\n\"state\"\n]\n}\n},\n{\n\"name\"\n:\n\"add_action_items_to_profile\"\n,\n\"description\"\n:\n\"Adds a list of actionable next steps to a client's profile using their client ID. \\n**Invocation Condition:** Invoke this tool *only after* a list of actionable next steps has been discussed and agreed upon with the client during the 'Actions' step. Requires the `client_id` obtained from the start of the session.\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"client_id\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The unique ID of the client, obtained from create_client_profile.\"\n},\n\"action_items\"\n:\n{\n\"type\"\n:\n\"array\"\n,\n\"items\"\n:\n{\n\"type\"\n:\n\"string\"\n},\n\"description\"\n:\n\"A list of action items for the client (e.g., ['Update resume', 'Research three companies']).\"\n}\n},\n\"required\"\n:\n[\n\"client_id\"\n,\n\"action_items\"\n]\n}\n},\n{\n\"name\"\n:\n\"get_next_appointment\"\n,\n\"description\"\n:\n\"Checks if a client has a future appointment already scheduled using their client ID. Returns the appointment details or null. \\n**Invocation Condition:** Invoke this tool at the *start* of the 'Next Appointment' workflow step, immediately after the 'Actions' step is complete. This is used to check if an appointment *already exists*.\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"client_id\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The unique ID of the client.\"\n}\n},\n\"required\"\n:\n[\n\"client_id\"\n]\n}\n},\n{\n\"name\"\n:\n\"get_available_appointments\"\n,\n\"description\"\n:\n\"Fetches a list of the next available appointment slots. \\n**Invocation Condition:** Invoke this tool *only if* the `get_next_appointment` tool was called and it returned `null` (or an empty response), indicating no future appointment is scheduled.\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{}\n}\n},\n{\n\"name\"\n:\n\"schedule_appointment\"\n,\n\"description\"\n:\n\"Books a new appointment for a client at a specific date and time. \\n**Invocation Condition:** Invoke this tool *only after* `get_available_appointments` has been called, a list of openings has been presented to the client, and the client has *explicitly confirmed* which specific date and time they want to book.\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"client_id\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The unique ID of the client.\"\n},\n\"appointment_datetime\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The chosen appointment slot in ISO 8601 format (e.g., '2025-10-30T14:30:00').\"\n}\n},\n\"required\"\n:\n[\n\"client_id\"\n,\n\"appointment_datetime\"\n]\n}\n}\n]\nMore information\nFor more information on using the Live API, see:\nLive API overview page\nLive API reference guide\nInteractive conversations\nBuilt-in tools\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 10609,
      "line_count": 388
    },
    {
      "filename": "vertex-ai_generative-ai_docs_live-api_proactive-audio.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/proactive-audio",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/proactive-audio\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nUsing Proactive Audio\nStay organized with collections\nSave and categorize content based on your preferences.\nProactive Audio helps Gemini have more authentic\nconversations by letting you control when it responds and in what contexts with\nfewer interruptions. For example, you can ask Gemini to only respond\nwhen prompted or when certain specific topics are discussed. To see\nProactive Audio in action, check out\na demonstration of the features\n.\nThis guide covers how Proactive Audio works, how to\nintegrate it into your application, and what tokens you are billed for. This\nguide doesn't cover the price list for Proactive Audio. For\nfull pricing details, see\nVertex AI pricing\n.\nThis guide assumes you are working either in Vertex AI Studio or are\nusing the Google Gen AI SDK for Python.\nSupported models\nYou can use Proactive Audio with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\ngemini-live-2.5-flash-preview-native-audio\nPublic preview; Discontinuation date: October 17, 2025\nUse Proactive Audio\nProactive Audio is not enabled by default in\ngemini-live-2.5-flash-preview-native-audio-09-2025\n.\nTo use Proactive Audio, configure the\nproactivity\nfield in\nthe setup message and set\nproactive_audio\nto\ntrue\n:\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nproactivity\n=\nProactivityConfig\n(\nproactive_audio\n=\nTrue\n),\n)\nHave a conversation using Proactive Audio\nYou can initiate a conversation with Gemini using\nProactive Audio and define when Gemini can\nrespond, limiting its responses to relevant topics.\nFor example, the following is a sample of what a conversation with\nGemini about cooking might look like:\nPrompt: \"You are an AI assistant in Italian cooking; only chime in when the topic is about Italian cooking.\"\nSpeaker A: \"I really love cooking!\" (No response from Gemini.)\nSpeaker B: \"Oh yes, me too! My favorite is French cuisine.\" (No response from\nGemini.)\nSpeaker A: \"I really like Italian food; do you know how to make a pizza?\"\n(Italian cooking topic will trigger response from Gemini.)\nLive API: \"I'd be happy to help! Here's a recipe for a pizza.\"\nFeatures\nWhen using Proactive Audio, Gemini will respond with\nminimal latency after the user is done speaking. This reduces interruptions and\nhelps Gemini not lose context if an interruption happens.\nProactive Audio also helps Gemini avoid interruptions\nfrom background noise or external chatter, and prevents Gemini from\nresponding if external chatter is introduced during a conversation.\nIf the user needs to interrupt during a response from Gemini,\nProactive Audio makes it easier for Gemini to\nappropriately back-channel (meaning appropriate interruptions are handled),\nrather than if a user uses filler words such as\numm\nor\nuhh\n.\nGemini can co-listen to an audio file that's not the speaker's\nvoice and subsequently answer questions about that audio file later in the\nconversation.\nBilling\nWhile Gemini is listening to a conversation, input audio tokens will be\ncharged.\nFor output audio tokens, you're only charged when Gemini responds. If\nGemini does not respond or stays silent, there will be no charge to\nyour output audio tokens.\nFor more information, see\nVertex AI pricing\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 3841,
      "line_count": 114
    },
    {
      "filename": "vertex-ai_generative-ai_docs_live-api_streamed-conversations.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/streamed-conversations",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/streamed-conversations\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nInteractive conversations with the Live API\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Live API enables low-latency, two-way voice and video interactions\nwith Gemini.\nThis guide covers how to set up a two-way interactive\nconversation, adjust audio settings, manage sessions, and more.\nSupported models\nYou can use the Live API with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash\nPrivate GA\n*\ngemini-live-2.5-flash-preview-native-audio\nPublic preview\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\n*\nReach out to your Google account team representative to request\naccess.\nStart a conversation\nConsole\nOpen\nVertex AI Studio > Stream realtime\n.\nClick\nmic\nStart session\nto initiate the conversation.\nTo end the session, click\nstop_circle\nStop session\n.\nPython\nTo enable real-time, interactive conversations, run the following\nexample on a local computer with microphone and speaker access (not a\nColab notebook). This example sets up a conversation with the API,\nallowing you to send audio prompts and receive audio responses:\n\"\"\"\n# Installation\n# on linux\nsudo apt-get install portaudio19-dev\n# on mac\nbrew install portaudio\npython3 -m venv env\nsource env/bin/activate\npip install google-genai\n\"\"\"\nimport\nasyncio\nimport\npyaudio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nCHUNK\n=\n4200\nFORMAT\n=\npyaudio\n.\npaInt16\nCHANNELS\n=\n1\nRECORD_SECONDS\n=\n5\nMODEL\n=\n'gemini-live-2.5-flash'\nINPUT_RATE\n=\n16000\nOUTPUT_RATE\n=\n24000\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"AUDIO\"\n],\n\"input_audio_transcription\"\n:\n{},\n# Configure input transcription\n\"output_audio_transcription\"\n:\n{},\n# Configure output transcription\n}\nasync\ndef\nmain\n():\nprint\n(\nMODEL\n)\np\n=\npyaudio\n.\nPyAudio\n()\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nMODEL\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\n#exit()\nasync\ndef\nsend\n():\nstream\n=\np\n.\nopen\n(\nformat\n=\nFORMAT\n,\nchannels\n=\nCHANNELS\n,\nrate\n=\nINPUT_RATE\n,\ninput\n=\nTrue\n,\nframes_per_buffer\n=\nCHUNK\n)\nwhile\nTrue\n:\nframe\n=\nstream\n.\nread\n(\nCHUNK\n)\nawait\nsession\n.\nsend\n(\ninput\n=\n{\n\"data\"\n:\nframe\n,\n\"mime_type\"\n:\n\"audio/pcm\"\n})\nawait\nasyncio\n.\nsleep\n(\n10\n**-\n12\n)\nasync\ndef\nreceive\n():\noutput_stream\n=\np\n.\nopen\n(\nformat\n=\nFORMAT\n,\nchannels\n=\nCHANNELS\n,\nrate\n=\nOUTPUT_RATE\n,\noutput\n=\nTrue\n,\nframes_per_buffer\n=\nCHUNK\n)\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\nserver_content\n.\ninput_transcription\n:\nprint\n(\nmessage\n.\nserver_content\n.\nmodel_dump\n(\nmode\n=\n\"json\"\n,\nexclude_none\n=\nTrue\n))\nif\nmessage\n.\nserver_content\n.\noutput_transcription\n:\nprint\n(\nmessage\n.\nserver_content\n.\nmodel_dump\n(\nmode\n=\n\"json\"\n,\nexclude_none\n=\nTrue\n))\nif\nmessage\n.\nserver_content\n.\nmodel_turn\n:\nfor\npart\nin\nmessage\n.\nserver_content\n.\nmodel_turn\n.\nparts\n:\nif\npart\n.\ninline_data\n.\ndata\n:\naudio_data\n=\npart\n.\ninline_data\n.\ndata\noutput_stream\n.\nwrite\n(\naudio_data\n)\nawait\nasyncio\n.\nsleep\n(\n10\n**-\n12\n)\nsend_task\n=\nasyncio\n.\ncreate_task\n(\nsend\n())\nreceive_task\n=\nasyncio\n.\ncreate_task\n(\nreceive\n())\nawait\nasyncio\n.\ngather\n(\nsend_task\n,\nreceive_task\n)\nasyncio\n.\nrun\n(\nmain\n())\nPython\nSet up a conversation with the API that lets you send text prompts and receive audio responses:\n# Set model generation_config\nCONFIG\n=\n{\n\"response_modalities\"\n:\n[\n\"AUDIO\"\n]}\nheaders\n=\n{\n\"Content-Type\"\n:\n\"application/json\"\n,\n\"Authorization\"\n:\nf\n\"Bearer\n{\nbearer_token\n[\n0\n]\n}\n\"\n,\n}\nasync\ndef\nmain\n()\n->\nNone\n:\n# Connect to the server\nasync\nwith\nconnect\n(\nSERVICE_URL\n,\nadditional_headers\n=\nheaders\n)\nas\nws\n:\n# Setup the session\nasync\ndef\nsetup\n()\n->\nNone\n:\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\n{\n\"setup\"\n:\n{\n\"model\"\n:\n\"gemini-live-2.5-flash\"\n,\n\"generation_config\"\n:\nCONFIG\n,\n}\n}\n)\n)\n# Receive setup response\nraw_response\n=\nawait\nws\n.\nrecv\n(\ndecode\n=\nFalse\n)\nsetup_response\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n(\n\"ascii\"\n))\nprint\n(\nf\n\"Connected:\n{\nsetup_response\n}\n\"\n)\nreturn\n# Send text message\nasync\ndef\nsend\n()\n->\nbool\n:\ntext_input\n=\ninput\n(\n\"Input > \"\n)\nif\ntext_input\n.\nlower\n()\nin\n(\n\"q\"\n,\n\"quit\"\n,\n\"exit\"\n):\nreturn\nFalse\nmsg\n=\n{\n\"client_content\"\n:\n{\n\"turns\"\n:\n[{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n[{\n\"text\"\n:\ntext_input\n}]}],\n\"turn_complete\"\n:\nTrue\n,\n}\n}\nawait\nws\n.\nsend\n(\njson\n.\ndumps\n(\nmsg\n))\nreturn\nTrue\n# Receive server response\nasync\ndef\nreceive\n()\n->\nNone\n:\nresponses\n=\n[]\n# Receive chucks of server response\nasync\nfor\nraw_response\nin\nws\n:\nresponse\n=\njson\n.\nloads\n(\nraw_response\n.\ndecode\n())\nserver_content\n=\nresponse\n.\npop\n(\n\"serverContent\"\n,\nNone\n)\nif\nserver_content\nis\nNone\n:\nbreak\nmodel_turn\n=\nserver_content\n.\npop\n(\n\"modelTurn\"\n,\nNone\n)\nif\nmodel_turn\nis\nnot\nNone\n:\nparts\n=\nmodel_turn\n.\npop\n(\n\"parts\"\n,\nNone\n)\nif\nparts\nis\nnot\nNone\n:\nfor\npart\nin\nparts\n:\npcm_data\n=\nbase64\n.\nb64decode\n(\npart\n[\n\"inlineData\"\n][\n\"data\"\n])\nresponses\n.\nappend\n(\nnp\n.\nfrombuffer\n(\npcm_data\n,\ndtype\n=\nnp\n.\nint16\n))\n# End of turn\nturn_complete\n=\nserver_content\n.\npop\n(\n\"turnComplete\"\n,\nNone\n)\nif\nturn_complete\n:\nbreak\n# Play the returned audio message\ndisplay\n(\nMarkdown\n(\n\"**Response >**\"\n))\ndisplay\n(\nAudio\n(\nnp\n.\nconcatenate\n(\nresponses\n),\nrate\n=\n24000\n,\nautoplay\n=\nTrue\n))\nreturn\nawait\nsetup\n()\nwhile\nTrue\n:\nif\nnot\nawait\nsend\n():\nbreak\nawait\nreceive\n()\nStart the conversation, input your prompts, or type\nq\n,\nquit\nor\nexit\nto exit.\nawait\nmain\n()\nChange language and voice settings\nThe Live API uses Chirp 3 to support synthesized speech responses in a\nvariety of HD voices and languages. For a full list and demos of each voice, see\nChirp 3: HD voices\n.\nTo set the response voice and language:\nConsole\nOpen\nVertex AI Studio > Stream realtime\n.\nIn the\nOutputs\nexpander, select a voice from the\nVoice\ndrop-down.\nIn the same expander, select a language from the\nLanguage\ndrop-down.\nClick\nmic\nStart session\nto start the session.\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nspeech_config\n=\nSpeechConfig\n(\nvoice_config\n=\nVoiceConfig\n(\nprebuilt_voice_config\n=\nPrebuiltVoiceConfig\n(\nvoice_name\n=\nvoice_name\n,\n)\n),\nlanguage_code\n=\n\"en-US\"\n,\n),\n)\nChange voice activity detection settings\nVoice activity detection\n(VAD) allows the model to recognize when a person is\nspeaking. This is essential for creating natural conversations, as it allows a\nuser to interrupt the model at any time.\nThe model automatically performs voice activity detection (VAD) on a continuous\naudio input stream. You can configure the VAD settings using the\nrealtimeInputConfig.automaticActivityDetection\nfield of the\nsetup\nmessage\n. When VAD detects\nan interruption, the ongoing generation is canceled and discarded. Only the\ninformation already sent to the client is retained in the session history. The\nserver then sends a message to report the interruption.\nIf the audio stream pauses for more than a second (for example, if the user\nturns off the microphone), send an\naudioStreamEnd\nevent to flush any cached audio. The client can resume sending audio data at any\ntime.\nAlternatively, disable automatic VAD by setting\nrealtimeInputConfig.automaticActivityDetection.disabled\nto\ntrue\nin the setup\nmessage. With this configuration, the client detects user speech and sends\nactivityStart\nand\nactivityEnd\nmessages at the appropriate times. An\naudioStreamEnd\nisn't sent. Interruptions\nare marked by\nactivityEnd\n.\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"TEXT\"\n],\nrealtime_input_config\n=\nRealtimeInputConfig\n(\nautomatic_activity_detection\n=\nAutomaticActivityDetection\n(\ndisabled\n=\nFalse\n,\n# default\nstart_of_speech_sensitivity\n=\nStartSensitivity\n.\nSTART_SENSITIVITY_LOW\n,\n# Either START_SENSITIVITY_LOW or START_SENSITIVITY_HIGH\nend_of_speech_sensitivity\n=\nEndSensitivity\n.\nEND_SENSITIVITY_LOW\n,\n# Either END_SENSITIVITY_LOW or END_SENSITIVITY_HIGH\nprefix_padding_ms\n=\n20\n,\nsilence_duration_ms\n=\n100\n,\n)\n),\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nMODEL_ID\n,\nconfig\n=\nconfig\n,\n)\nas\nsession\n:\naudio_bytes\n=\nPath\n(\n\"sample.pcm\"\n)\n.\nread_bytes\n()\nawait\nsession\n.\nsend_realtime_input\n(\nmedia\n=\nBlob\n(\ndata\n=\naudio_bytes\n,\nmime_type\n=\n\"audio/pcm;rate=16000\"\n)\n)\n# if stream gets paused, send:\n# await session.send_realtime_input(audio_stream_end=True)\nresponse\n=\n[]\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\nserver_content\n.\ninterrupted\nis\nTrue\n:\n# The model generation was interrupted\nresponse\n.\nappend\n(\n\"The session was interrupted\"\n)\nif\nmessage\n.\ntext\n:\nresponse\n.\nappend\n(\nmessage\n.\ntext\n)\ndisplay\n(\nMarkdown\n(\nf\n\"**Response >**\n{\n''\n.\njoin\n(\nresponse\n)\n}\n\"\n))\nExtend a session\nThe default maximum length of a conversation session is 10 minutes. A\ngoAway\nnotification\n(\nBidiGenerateContentServerMessage.goAway\n)\nis sent to the client 60 seconds before the session ends.\nYou can extend the session length in 10-minute increments using the\nGen AI SDK. There's no limit to the number of times you can extend\na session. For an example, see\nEnable and disable session\nresumption\n.\nContext window\nThe Live API context window is used to store real-time streamed data\n(25 tokens per second (TPS) for audio and 258 TPS for video) and other content,\nincluding text inputs and model outputs.\nIf the context window exceeds the maximum length (set using the\nMax content\nsize\nslider in Vertex AI Studio, or\ntrigger_tokens\nin the API), the\noldest turns are truncated using\ncontext window compression\nto prevent abrupt\nsession termination. Context window compression triggers once the context window\nhits its maximum length (set either in Vertex AI Studio using the\nTarget context size\nslider, or using\ntarget_tokens\nin the API) and deletes\nthe oldest parts of the conversation until the total token count is back down to\nthis target size.\nFor example, if your maximum context length is set to\n32000\ntokens and your\ntarget size is set to\n16000\ntokens:\nTurn 1:\nThe conversation starts. In this example, the request uses\n12,000 tokens.\nTotal context size: 12,000 tokens\nTurn 2:\nYou make another request. This request uses another 12,000\ntokens.\nTotal context size: 24,000 tokens\nTurn 3:\nYou make another request. This request uses 14,000 tokens.\nTotal context size: 38,000 tokens\nSince the total context size is now higher than the 32,000 token maximum,\ncontext window compression now triggers. The system goes back to the beginning\nof the conversation and starts deleting old turns until the total token size is\nless than the 16,000 token target:\nIt deletes\nTurn 1\n(12,000 tokens). The total is now 26,000 tokens, which\nis still higher than the 16,000 token target.\nIt deletes\nTurn 2\n(12,000 tokens). The total is now 14,000 tokens.\nThe final result is that only\nTurn 3\nremains in active memory, and the\nconversation continues from that point.\nThe minimum and maximum lengths for the context length and target size are:\nSetting (API flag)\nMinimum value\nMaximum value\nMaximum context length (\ntrigger_tokens\n)\n5,000\n128,000\nTarget context size (\ntarget_tokens\n)\n0\n128,000\nTo set the context window:\nConsole\nOpen\nVertex AI Studio > Stream realtime\n.\nClick to open the\nAdvanced\nmenu.\nIn the\nSession Context\nsection, use the\nMax context size\nslider to set the context size to a value between 5,000 and 128,000.\n(Optional) In the same section, use the\nTarget context size\nslider to set the target size to a value between 0 and 128,000.\nPython\nSet the\ncontext_window_compression.trigger_tokens\nand\ncontext_window_compression.sliding_window.target_tokens\nfields in the setup message:\nconfig\n=\ntypes\n.\nLiveConnectConfig\n(\ntemperature\n=\n0.7\n,\nresponse_modalities\n=\n[\n'TEXT'\n],\nsystem_instruction\n=\ntypes\n.\nContent\n(\nparts\n=\n[\ntypes\n.\nPart\n(\ntext\n=\n'test instruction'\n)],\nrole\n=\n'user'\n),\ncontext_window_compression\n=\ntypes\n.\nContextWindowCompressionConfig\n(\ntrigger_tokens\n=\n1000\n,\nsliding_window\n=\ntypes\n.\nSlidingWindow\n(\ntarget_tokens\n=\n10\n),\n),\n)\nConcurrent sessions\nYou can have up to 1,000 concurrent sessions per project.\nUpdate system instructions during a session\nThe Live API lets you update the system instructions during an active\nsession. Use this to adapt the model's responses, such as changing the response\nlanguage or modifying the tone.\nTo update the system instructions mid-session, you can send text content with\nthe\nsystem\nrole. The updated system instruction will remain in effect for the\nremaining session.\nPython\nsession\n.\nsend_client_content\n(\nturns\n=\ntypes\n.\nContent\n(\nrole\n=\n\"system\"\n,\nparts\n=\n[\ntypes\n.\nPart\n(\ntext\n=\n\"new system instruction\"\n)]\n),\nturn_complete\n=\nFalse\n)\nEnable and disable session resumption\nSession resumption lets you reconnect to a previous session within 24 hours.\nThis is achieved by storing cached data, including text, video, audio prompts,\nand model outputs. Project-level privacy is enforced for this cached data.\nBy default, session resumption is disabled.\nTo enable the session resumption feature, set the\nsessionResumption\nfield of\nthe\nBidiGenerateContentSetup\nmessage. If enabled, the server will periodically take a snapshot of the current\ncached session contexts, and store it in the internal storage.\nWhen a snapshot is successfully taken, a\nresumptionUpdate\nis returned with\nthe handle ID that you can record and use later to resume the session from the\nsnapshot.\nHere's an example of enabling session resumption and retrieving the handle ID:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\nasync\ndef\nmain\n():\nprint\n(\nf\n\"Connecting to the service with handle\n{\nprevious_session_handle\n}\n...\"\n)\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\ntypes\n.\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nsession_resumption\n=\ntypes\n.\nSessionResumptionConfig\n(\n# The handle of the session to resume is passed here,\n# or else None to start a new session.\nhandle\n=\nprevious_session_handle\n),\n),\n)\nas\nsession\n:\nwhile\nTrue\n:\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\ntypes\n.\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\ntypes\n.\nPart\n(\ntext\n=\n\"Hello world!\"\n)]\n)\n)\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\n# Periodically, the server will send update messages that may\n# contain a handle for the current state of the session.\nif\nmessage\n.\nsession_resumption_update\n:\nupdate\n=\nmessage\n.\nsession_resumption_update\nif\nupdate\n.\nresumable\nand\nupdate\n.\nnew_handle\n:\n# The handle should be retained and linked to the session.\nreturn\nupdate\n.\nnew_handle\n# For the purposes of this example, placeholder input is continually fed\n# to the model. In non-sample code, the model inputs would come from\n# the user.\nif\nmessage\n.\nserver_content\nand\nmessage\n.\nserver_content\n.\nturn_complete\n:\nbreak\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nTo achieve seamless session resumption, enable\ntransparent mode\n:\nPython\ntypes\n.\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nsession_resumption\n=\ntypes\n.\nSessionResumptionConfig\n(\ntransparent\n=\nTrue\n,\n),\n)\nAfter transparent mode is enabled, the index of the client message that\ncorresponds with the context snapshot is explicitly returned. This helps\nidentify which client message you need to send again, when you resume the\nsession from the resumption handle.\nMore information\nFor more information on using the Live API, see:\nLive API overview\nLive API reference\nguide\nBuilt-in tools\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 15843,
      "line_count": 1444
    },
    {
      "filename": "vertex-ai_generative-ai_docs_live-api_tools.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/tools",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/live-api/tools\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nBuilt-in tools for the Live API\nStay organized with collections\nSave and categorize content based on your preferences.\nSeveral tools are compatible with various versions of\nLive API-supported models, including:\nFunction calling\nGrounding with Google Search\nTo enable a particular tool for usage in returned responses, include the name of\nthe tool in the\ntools\nlist when you initialize the model. The following\nsections provide examples of how to use each of the built-in tools in your code.\nSupported models\nYou can use the Live API with the following models:\nModel version\nAvailability level\ngemini-live-2.5-flash\nPrivate GA\n*\ngemini-live-2.5-flash-preview-native-audio\nPublic preview\ngemini-live-2.5-flash-preview-native-audio-09-2025\nPublic preview\n*\nReach out to your Google account team representative to request\naccess.\nFunction calling\nUse\nfunction calling\nto create a description of a\nfunction, then pass that description to the model in a request. The response\nfrom the model includes the name of a function that matches the description and\nthe arguments to call it with.\nAll functions must be declared at the start of the session by sending tool\ndefinitions as part of the\nLiveConnectConfig\nmessage.\nTo enable function calling, include\nfunction_declarations\nin the\ntools\nlist:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\n# Simple function definitions\nturn_on_the_lights\n=\n{\n\"name\"\n:\n\"turn_on_the_lights\"\n}\nturn_off_the_lights\n=\n{\n\"name\"\n:\n\"turn_off_the_lights\"\n}\ntools\n=\n[{\n\"function_declarations\"\n:\n[\nturn_on_the_lights\n,\nturn_off_the_lights\n]}]\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"TEXT\"\n],\n\"tools\"\n:\ntools\n}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nprompt\n=\n\"Turn on the lights please\"\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\n{\n\"parts\"\n:\n[{\n\"text\"\n:\nprompt\n}]})\nasync\nfor\nchunk\nin\nsession\n.\nreceive\n():\nif\nchunk\n.\nserver_content\n:\nif\nchunk\n.\ntext\nis\nnot\nNone\n:\nprint\n(\nchunk\n.\ntext\n)\nelif\nchunk\n.\ntool_call\n:\nfunction_responses\n=\n[]\nfor\nfc\nin\ntool_call\n.\nfunction_calls\n:\nfunction_response\n=\ntypes\n.\nFunctionResponse\n(\nname\n=\nfc\n.\nname\n,\nresponse\n=\n{\n\"result\"\n:\n\"ok\"\n}\n# simple, hard-coded function response\n)\nfunction_responses\n.\nappend\n(\nfunction_response\n)\nawait\nsession\n.\nsend_tool_response\n(\nfunction_responses\n=\nfunction_responses\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\nPython\nFor examples using function calling in system instructions, see our\nbest practices example\n.\nGrounding with Google Search\nYou can use\nGrounding with Google Search\nwith\nthe Live API by including\ngoogle_search\nin the\ntools\nlist:\nPython\nimport\nasyncio\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai\nimport\ntypes\nclient\n=\ngenai\n.\nClient\n(\nvertexai\n=\nTrue\n,\nproject\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n,\n)\nmodel\n=\n\"gemini-live-2.5-flash\"\ntools\n=\n[{\n'google_search'\n:\n{}}]\nconfig\n=\n{\n\"response_modalities\"\n:\n[\n\"TEXT\"\n],\n\"tools\"\n:\ntools\n}\nasync\ndef\nmain\n():\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel\n,\nconfig\n=\nconfig\n)\nas\nsession\n:\nprompt\n=\n\"When did the last Brazil vs. Argentina soccer match happen?\"\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\n{\n\"parts\"\n:\n[{\n\"text\"\n:\nprompt\n}]})\nasync\nfor\nchunk\nin\nsession\n.\nreceive\n():\nif\nchunk\n.\nserver_content\n:\nif\nchunk\n.\ntext\nis\nnot\nNone\n:\nprint\n(\nchunk\n.\ntext\n)\n# The model might generate and execute Python code to use Search\nmodel_turn\n=\nchunk\n.\nserver_content\n.\nmodel_turn\nif\nmodel_turn\n:\nfor\npart\nin\nmodel_turn\n.\nparts\n:\nif\npart\n.\nexecutable_code\nis\nnot\nNone\n:\nprint\n(\npart\n.\nexecutable_code\n.\ncode\n)\nif\npart\n.\ncode_execution_result\nis\nnot\nNone\n:\nprint\n(\npart\n.\ncode_execution_result\n.\noutput\n)\nif\n__name__\n==\n\"__main__\"\n:\nasyncio\n.\nrun\n(\nmain\n())\n(Public preview) Native audio\nGemini 2.5 Flash with Live API\nintroduces native audio capabilities, enhancing the standard Live API\nfeatures. Native audio provides richer and more natural voice interactions\nthrough\n30 HD voices\nin\n24\nlanguages\n. It also\nincludes two new features exclusive to native audio:\nProactive Audio\nand\nAffective Dialog\n.\nUse Affective Dialog\nAffective Dialog\nallows models using Live API\nnative audio to better understand and respond appropriately to users' emotional\nexpressions, leading to more nuanced conversations.\nTo enable Affective Dialog, set\nenable_affective_dialog\nto\ntrue\nin the setup message:\nPython\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\n\"AUDIO\"\n],\nenable_affective_dialog\n=\nTrue\n,\n)\nMore information\nFor more information on using the Live API, see:\nLive API overview\nLive API reference\nguide\nInteractive conversations\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 5339,
      "line_count": 502
    },
    {
      "filename": "vertex-ai_generative-ai_docs_model-garden_explore-models.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nOverview of Model Garden\nStay organized with collections\nSave and categorize content based on your preferences.\nModel Garden is an AI/ML model library that helps you discover, test,\ncustomize, and deploy models and assets from Google and Google partners.\nAdvantages of Model Garden\nWhen you're working with AI models, Model Garden provides the following\nadvantages:\nAvailable models are all grouped in a single location\nModel Garden provides a consistent deployment pattern for different\ntypes of models\nModel Garden provides built-in integration with other parts of\nVertex AI such as model tuning, evaluation, and serving\nServing generative AI models can be difficult—Vertex AI handles\nmodel deployment and serving for you\nExplore models\nTo view the list of available Vertex AI and open source foundation,\ntunable, and task-specific models, go to the Model Garden page in the\nGoogle Cloud console.\nGo to Model Garden\nThe model categories available in Model Garden are:\nCategory\nDescription\nFoundation models\nPretrained multitask large models that can be tuned or customized\nfor specific tasks using Vertex AI Studio, Vertex AI API, and the\nVertex AI SDK for Python.\nFine-tunable models\nModels that you can fine-tune using a custom notebook or\npipeline.\nTask-specific solutions\nMost of these prebuilt models are\nready to use. Many can be customized using your own data.\nTo filter models in the filter pane, specify the following:\nTasks\n: Click the task that you want the model to perform.\nModel collections\n: Click to choose models that are managed by Google,\npartners, or you.\nProviders\n: Click the provider of the model.\nFeatures\n: Click the features that you want in the model.\nTo learn more about each model, click its model card.\nFor a list of models available in Model Garden, see\nModels available in Model Garden\n.\nModel security scanning\nGoogle does thorough testing and benchmarking on the serving and tuning\ncontainers that we provide. Active vulnerability scanning is also applied to\ncontainer artifacts.\nThird-party models from featured partners undergo model checkpoint scans to\nensure authenticity. Third-party models from HuggingFace Hub are scanned\ndirectly by HuggingFace and their\nthird-party scanner\nfor malware, pickle files, Keras Lambda layers, and secrets. Models deemed\nunsafe from these scans are flagged by HuggingFace and blocked from deployment\nin Model Garden. Models deemed suspicious or those that have the\nability to potentially execute remote code are indicated in\nModel Garden but can still be deployed. We recommend you perform a\nthorough review of any suspicious model before deploying it\nwithin Model Garden.\nPricing\nFor the open source models in Model Garden, you are charged for use of\nfollowing on Vertex AI:\nModel tuning\n: You are charged for the compute resources used at the same\nrate as custom training. See\ncustom training pricing\n.\nModel deployment\n: You are charged for the compute resources used to\ndeploy the model to an endpoint. See\npredictions pricing\n.\nColab Enterprise\n: See\nColab Enterprise pricing\n.\nControl access to specific models\nYou can set a\nModel Garden organization\npolicy\nat the organization, folder, or\nproject level to control access to specific models in Model Garden. For\nexample, you can allow access to specific models that you've vetted and deny\naccess to all others.\nLearn more about Model Garden\nFor more information about the deployment options and customizations that you\ncan do with models in Model Garden, view the resources in the\nfollowing sections, which include links to tutorials, references, notebooks, and\nYouTube videos.\nDeploy and serve\nLearn more about customizing deployments and advance serving features.\nDeploy and serve open source model using Python SDK, CLI, REST API, or console\nDeveloper blog: Introducing the new Vertex AI Model Garden CLI and SDK\nDeploy open models by using the SDK tutorial notebook\nGet started with Vertex AI Model Garden SDK notebook\nDeploying and fine-tuning Gemma 3 in Model Garden YouTube video\nDeploying Gemma and making predictions\nServe open models with a Hex-LLM container on Cloud\nTPUs\nDeploying Llama models by using Hex-LLM tutorial notebook\nUse prefix caching and speculative decoding with\nHex-LLM or vLLM tutorial notebook\nUse vLLM to serve text-only and multimodel language models on Cloud GPUs\nText-only models tutorial notebook\nMultimodal models tutorial notebook\nUse xDiT GPU serving container for image and video generation\nServing Gemma 2 with multiple LoRA adapters with HuggingFace DLC for PyTorch inference tutorial on Medium\nUse custom handles to serve PaliGemma for image captioning with HuggingFace DLC for PyTorch inference tutorial on LinkedIn\nDeploy and serve a model that uses Spot VMs or a Compute Engine reservation tutorial notebook\nDeploy and serve a HuggingFace model\nContainer compliance\nModel Garden offers the following FedRAMP high compliant\ncontainers for model serving.\nContainer name\nSupported tasks\nContainer image version\nNotebook example\nPyTorch Inference v0.4\naudio2text\ntext2image\nzero-shot-image-classification\nzero-shot-object-detection\ncsm_text2speech\ndia_text2speech\nimage-to-text\nvisual-question-answering\ninstant-id\njanus_text2image\njanus_text_generation\nmask-generation\nnllb_translation\npaligemma_v2\npix2pix\nus-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/pytorch-inference.cu125.0-4.ubuntu2204.py310:model-garden.pytorch-inference-0-4-gpu-release_20251024.01_p0\nHiDream-I1\nSGLang\nText2text generation\nus-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/sglang-serve.cu124.0-4.ubuntu2204.py310:model-garden.sglang-0-4-release_20251031.01_p0\nQwen3 (Deployment)\nHuggingFace Inference Toolkit\ntext2image generation\nvanilla text-generation\ntext-classification\ntranslation\nzero-shot-object-detection\nmask-generation\nsentence embeddings\nfeature extraction\nfill mask\nFull task list:\nhttps://huggingface.co/docs/inference-endpoints/en/supported_tasks\nus-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/hf-inference-toolkit.cu125.0-1.ubuntu2204.py311:model-garden.hf-inference-toolkit-0-1-release_20251108.00_p0\nHugging Face PyTorch Inference Deployment\nHuggingFace Text Embeddings Inference (TEI)\ntext2embeddings\nus-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/hf-tei.cu125.0-1.ubuntu2204.py310:model-garden.hf-tei-0-1-release_20251030.00_p0\nHugging Face Text Embeddings Inference Deployment\nTuning\nLearn more about tuning models to tailor responses for specific use cases.\nWorkbench fine-tuning tutorial notebook\nFine-tuning and evaluation tutorial notebook\nDeploying and fine-tuning Gemma 3 in Model Garden YouTube video\nEvaluation\nLearn more about assessing model responses with Vertex AI\nEvaluate Gemma 2 with the generative AI evaluation service YouTube video\nAdditional resources\nModel and user journey-specific Model Garden notebooks\nVertex AI open model serving, fine-tuning and evaluation notebooks\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-11 UTC.",
      "content_length": 7533,
      "line_count": 188
    },
    {
      "filename": "vertex-ai_generative-ai_docs_model-reference_code-execution-api.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/code-execution-api",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/code-execution-api\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nAPI reference\nSend feedback\nExecute code with the Gemini API\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Gemini API code execution feature enables the model to generate and run\nPython code and learn iteratively from the results until it arrives at a final\noutput. You can use this code execution capability to build applications that\nbenefit from code-based reasoning and that produce text output. For example,\nyou could use code execution in an application that solves equations or\nprocesses text.\nThe Gemini API provides code execution as a tool, similar to\nfunction calling\n. After\nyou add code execution as a tool, the model decides when to use it.\nSupported models\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nLimitations\nThe feature doesn't support file I/O.\nCode execution can run for a maximum of 30 seconds before timing out.\nExample syntax\ncurl\nPROJECT_ID\n=\nmyproject\nREGION\n=\nus-central1\nMODEL_ID\n=\ngemini-2.0-flash-001\nhttps://\n${\nREGION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nREGION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n...\n}],\n\"tools\": [{\n\"code_execution\":  {}\n}]\n}'\nParameter list\nSee\nexamples\nfor implementation details.\nPython\nTo enable code execution, specify a code execution\ntool\nin your request.\nCodeExecution\nTool that executes code generated by the model, and automatically returns the result to the model. See also\nExecutableCode\nand\nCodeExecutionResult\nwhich are input and output to this tool.\nPart\nexecutable_code\nOptional:\nExecutableCode\nCode generated by the model that is meant to be executed.\nSee Code Execution [API].\ncode_execution_result\nOptional:\nCodeExecutionResult\nResult of executing the [ExecutableCode].\nSee Code Execution [API].\nExecutableCode\nlanguage\nRequired:\nstring (enum)\nSupported programming languages for the generated\ncode\n.\nSupported:\nPYTHON\ncode\nRequired:\nstring\nThe code to be executed.\nSee Code Execution [API].\nCodeExecutionResult\noutcome\nRequired:\nstring (enum)\nOutcome of the code execution.\nPossible outcomes:\nCode execution completed successfully. (\nOUTCOME_OK\n)\nCode execution finished but with a failure.\nstderr\nshould contain the reason. (\nOUTCOME_FAILED\n)\nCode execution ran for too long, and was cancelled. There may or may not be a partial output present. (\nOUTCOME_DEADLINE_EXCEEDED\n)\noutput\nRequired:\nstring\nContains\nstdout\nwhen code execution is successful,\nstderr\nor other description otherwise.\nSee Code Execution [API].\nExamples\nHere are illustrations of how you can submit a query and function declarations to the model.\nBasic use case\ncurl\nPROJECT_ID\n=\nmyproject\nREGION\n=\nus-central1\nMODEL_ID\n=\ngemini-2.0-flash-001\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nREGION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nREGION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\"\n}]\n}],\n\"tools\": [{'\ncodeExecution\n': {}}],\n}'\nPython\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nTool\n,\nToolCodeExecution\n,\nGenerateContentConfig\nclient\n=\ngenai\n.\nClient\n()\nmodel_id\n=\n\"gemini-2.0-flash-001\"\ncode_execution_tool\n=\nTool\n(\ncode_execution\n=\nToolCodeExecution\n()\n)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\nmodel_id\n,\ncontents\n=\n\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\ncode_execution_tool\n],\ntemperature\n=\n0\n,\n),\n)\nfor\npart\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n.\nparts\n:\nif\npart\n.\nexecutable_code\n:\nprint\n(\npart\n.\nexecutable_code\n)\nif\npart\n.\ncode_execution_result\n:\nprint\n(\npart\n.\ncode_execution_result\n)\n# Example response:\n# code='...' language='PYTHON'\n# outcome='OUTCOME_OK' output='The 20th Fibonacci number is: 6765\\n'\n# code='...' language='PYTHON'\n# outcome='OUTCOME_OK' output='Lower Palindrome: 6666\\nHigher Palindrome: 6776\\nNearest Palindrome to 6765: 6776\\n'\nEnable code execution on the model\nTo enable basic code execution, see\nCode execution\n.\nWhat's next\nLearn more about the\nGemini\nAPI\n.\nLearn more about\nFunction\ncalling\n.\nLearn more about\nGenerating content with Gemini\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 5049,
      "line_count": 326
    },
    {
      "filename": "vertex-ai_generative-ai_docs_model-reference_function-calling.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nAPI reference\nSend feedback\nFunction calling reference\nStay organized with collections\nSave and categorize content based on your preferences.\nFunction calling improves the LLM's ability to provide relevant and contextual\nanswers.\nYou can provide custom functions to a generative AI model with the Function\nCalling API. The model doesn't directly invoke these functions, but instead\ngenerates structured data output that specifies the function name and suggested\narguments.\nThis output enables the calling of external APIs or information\nsystems such as databases, customer relationship management systems, and\ndocument repositories. The resulting API output can be used by the LLM to\nimprove response quality.\nFor more conceptual documentation on function calling, see\nFunction calling\n.\nSupported models\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nGemini 2.0 Flash-Lite\nLimitations\n:\nThe maximum number of function declarations that can be provided with the request is 128.\nExample syntax\nSyntax to send a function call API request.\ncurl\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n...\n}],\n\"tools\": [{\n\"function_declarations\": [\n{\n...\n}\n]\n}]\n}'\nParameter list\nSee\nexamples\nfor implementation details.\nFunctionDeclaration\nDefines a function that the model can generate JSON inputs for based on\nOpenAPI 3.0\nspecifications.\nParameters\nname\nstring\nThe name of the function to call. Must start with a letter or an underscore. Must be a-z, A-Z, 0-9, or contains underscores, dots, or dashes, with a maximum length of 64.\ndescription\nOptional:\nstring\nThe description and purpose of the function. The model uses this to decide how and whether to call the function. For the best results, we recommend that you include a description.\nparameters\nOptional:\nSchema\nDescribes the parameters of the function in the OpenAPI JSON Schema Object format:\nOpenAPI 3.0 specification\n.\nresponse\nOptional:\nSchema\nDescribes the output from the function in the OpenAPI JSON Schema Object format:\nOpenAPI 3.0 specification\n.\nFor more information, see\nFunction calling\nSchema\nDefines the format of the input and output data in a function call based on the\nOpenAPI 3.0 Schema\nspecification.\nParameters\ntype\nstring\nEnum. The type of the data. Must be one of:\nSTRING\nINTEGER\nBOOLEAN\nNUMBER\nARRAY\nOBJECT\ndescription\nOptional:\nstring\nDescription of the data.\nenum\nOptional:\nstring[]\nPossible values of the element of primitive type with enum format.\nitems\nOptional:\nSchema[]\nSchema of the elements of\nType.ARRAY\nproperties\nOptional:\nSchema\nSchema of the properties of\nType.OBJECT\nrequired\nOptional:\nstring[]\nRequired properties of\nType.OBJECT\n.\nnullable\nOptional:\nbool\nIndicates if the value may be\nnull\n.\nFunctionCallingConfig\nThe\nFunctionCallingConfig\ncontrols the behavior of the model and\ndetermines what type of function to call.\nParameters\nmode\nOptional:\nenum/string[]\nAUTO\n: Default model behavior. The model can make predictions in either a function call form or a natural language response form. The model decides which form to use based on the context.\nNONE\n: The model doesn't make any predictions in the form of function calls.\nANY\n: The model is constrained to always predict a function call. If\nallowed_function_names\nis not provided, the model picks from all of the available function declarations. If\nallowed_function_names\nis provided, the model picks from the set of allowed functions.\nallowed_function_names\nOptional:\nstring[]\nFunction names to call. Only set when the\nmode\nis\nANY\n. Function names should match\n[FunctionDeclaration.name]\n. With mode set to\nANY\n, the model will predict a function call from the set of function names provided.\nfunctionCall\nA predicted\nfunctionCall\nreturned from the model that contains a string\nrepresenting the\nfunctionDeclaration.name\nand a structured JSON object\ncontaining the parameters and their values.\nParameters\nname\nstring\nThe name of the function to call.\nargs\nStruct\nThe function parameters and values in JSON object format.\nSee\nFunction calling\nfor parameter details.\nfunctionResponse\nThe resulting output from a\nFunctionCall\nthat contains a string representing the\nFunctionDeclaration.name\n. Also contains a structured JSON object with the\noutput from the function (and uses it as context for the model). This should contain the\nresult of a\nFunctionCall\nmade based on model prediction.\nParameters\nname\nstring\nThe name of the function to call.\nresponse\nStruct\nThe function response in JSON object format.\nExamples\nSend a function declaration\nThe following example is a basic example of sending a query and a function declaration\nto the model.\nREST\nBefore using any of the request data,\nmake the following replacements:\nPROJECT_ID\n: Your\nproject ID\n.\nMODEL_ID\n: The ID of the model that's being processed.\nROLE\n: The\nidentity of the entity\nthat creates the message.\nTEXT\n: The prompt to send to the model.\nNAME\n: The name of the function to call.\nDESCRIPTION\n: Description and purpose of the function.\nFor other fields, see the\nParameter list\ntable.\nHTTP method and URL:\nPOST https://aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/global/publishers/google/models/\nMODEL_ID\n:generateContent\nRequest JSON body:\n{\n\"contents\": [{\n\"role\": \"\nROLE\n\",\n\"parts\": [{\n\"text\": \"\nTEXT\n\"\n}]\n}],\n\"tools\": [{\n\"function_declarations\": [\n{\n\"name\": \"\nNAME\n\",\n\"description\": \"\nDESCRIPTION\n\",\n\"parameters\": {\n\"type\": \"\nTYPE\n\",\n\"properties\": {\n\"location\": {\n\"type\": \"\nTYPE\n\",\n\"description\": \"\nDESCRIPTION\n\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}]\n}\nTo send your request, choose one of these options:\ncurl\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/global/publishers/google/models/\nMODEL_ID\n:generateContent\"\nPowerShell\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/global/publishers/google/models/\nMODEL_ID\n:generateContent\" | Select-Object -Expand Content\nExample curl command\nPROJECT_ID\n=\nmyproject\nLOCATION\n=\nus-central1\nMODEL_ID\n=\ngemini-2.5-flash\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"What is the weather in Boston?\"\n}]\n}],\n\"tools\": [{\n\"functionDeclarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}]\n}'\nGen AI SDK for Python\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nGenerateContentConfig\n,\nHttpOptions\ndef\nget_current_weather\n(\nlocation\n:\nstr\n)\n-\n>\nstr\n:\n\"\"\"Example method. Returns the current weather.\nArgs:\nlocation: The city and state, e.g. San Francisco, CA\n\"\"\"\nweather_map\n:\ndict\n[\nstr\n,\nstr\n]\n=\n{\n\"Boston, MA\"\n:\n\"snowing\"\n,\n\"San Francisco, CA\"\n:\n\"foggy\"\n,\n\"Seattle, WA\"\n:\n\"raining\"\n,\n\"Austin, TX\"\n:\n\"hot\"\n,\n\"Chicago, IL\"\n:\n\"windy\"\n,\n}\nreturn\nweather_map\n.\nget\n(\nlocation\n,\n\"unknown\"\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nmodel_id\n=\n\"gemini-2.5-flash\"\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\nmodel_id\n,\ncontents\n=\n\"What is the weather like in Boston?\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\nget_current_weather\n],\ntemperature\n=\n0\n,\n),\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# The weather in Boston is sunny.\nNode.js\nconst\n{\nVertexAI\n,\nFunctionDeclarationSchemaType\n,\n}\n=\nrequire\n(\n'\n@google-cloud/vertexai\n'\n);\nconst\nfunctionDeclarations\n=\n[\n{\nfunction_declarations\n:\n[\n{\nname\n:\n'get_current_weather'\n,\ndescription\n:\n'get weather in a given location'\n,\nparameters\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nOBJECT\n,\nproperties\n:\n{\nlocation\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n},\nunit\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n,\nenum\n:\n[\n'celsius'\n,\n'fahrenheit'\n],\n},\n},\nrequired\n:\n[\n'location'\n],\n},\n},\n],\n},\n];\n/**\n* TODO(developer): Update these variables before running the sample.\n*/\nasync\nfunction\nfunctionCallingBasic\n(\nprojectId\n=\n'PROJECT_ID'\n,\nlocation\n=\n'us-central1'\n,\nmodel\n=\n'gemini-2.0-flash-001'\n)\n{\n// Initialize Vertex with your Cloud project and location\nconst\nvertexAI\n=\nnew\nVertexAI\n({\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n});\n// Instantiate the model\nconst\ngenerativeModel\n=\nvertexAI\n.\npreview\n.\ngetGenerativeModel\n({\nmodel\n:\nmodel\n,\n});\nconst\nrequest\n=\n{\ncontents\n:\n[\n{\nrole\n:\n'user'\n,\nparts\n:\n[{\ntext\n:\n'What is the weather in Boston?'\n}]},\n],\ntools\n:\nfunctionDeclarations\n,\n};\nconst\nresult\n=\nawait\ngenerativeModel\n.\ngenerateContent\n(\nrequest\n);\nconsole\n.\nlog\n(\nJSON\n.\nstringify\n(\nresult\n.\nresponse\n.\ncandidates\n[\n0\n].\ncontent\n));\n}\nJava\nimport\ncom.google.cloud.vertexai.\nVertexAI\n;\nimport\ncom.google.cloud.vertexai.api.\nContent\n;\nimport\ncom.google.cloud.vertexai.api.\nFunctionDeclaration\n;\nimport\ncom.google.cloud.vertexai.api.\nGenerateContentResponse\n;\nimport\ncom.google.cloud.vertexai.api.\nSchema\n;\nimport\ncom.google.cloud.vertexai.api.\nTool\n;\nimport\ncom.google.cloud.vertexai.api.\nType\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nChatSession\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nContentMaker\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nGenerativeModel\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nPartMaker\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nResponseHandler\n;\nimport\njava.io.IOException\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\npublic\nclass\nFunctionCalling\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\nthrows\nIOException\n{\n// TODO(developer): Replace these variables before running the sample.\nString\nprojectId\n=\n\"your-google-cloud-project-id\"\n;\nString\nlocation\n=\n\"us-central1\"\n;\nString\nmodelName\n=\n\"gemini-2.0-flash-001\"\n;\nString\npromptText\n=\n\"What's the weather like in Paris?\"\n;\nwhatsTheWeatherLike\n(\nprojectId\n,\nlocation\n,\nmodelName\n,\npromptText\n);\n}\n// A request involving the interaction with an external tool\npublic\nstatic\nString\nwhatsTheWeatherLike\n(\nString\nprojectId\n,\nString\nlocation\n,\nString\nmodelName\n,\nString\npromptText\n)\nthrows\nIOException\n{\n// Initialize client that will be used to send requests.\n// This client only needs to be created once, and can be reused for multiple requests.\ntry\n(\nVertexAI\nvertexAI\n=\nnew\nVertexAI\n(\nprojectId\n,\nlocation\n))\n{\nFunctionDeclaration\nfunctionDeclaration\n=\nFunctionDeclaration\n.\nnewBuilder\n()\n.\nsetName\n(\n\"getCurrentWeather\"\n)\n.\nsetDescription\n(\n\"Get the current weather in a given location\"\n)\n.\nsetParameters\n(\nSchema\n.\nnewBuilder\n()\n.\nsetType\n(\nType\n.\nOBJECT\n)\n.\nputProperties\n(\n\"location\"\n,\nSchema\n.\nnewBuilder\n()\n.\nsetType\n(\nType\n.\nSTRING\n)\n.\nsetDescription\n(\n\"location\"\n)\n.\nbuild\n()\n)\n.\naddRequired\n(\n\"location\"\n)\n.\nbuild\n()\n)\n.\nbuild\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Function declaration:\"\n);\nSystem\n.\nout\n.\nprintln\n(\nfunctionDeclaration\n);\n// Add the function to a \"tool\"\nTool\ntool\n=\nTool\n.\nnewBuilder\n()\n.\naddFunctionDeclarations\n(\nfunctionDeclaration\n)\n.\nbuild\n();\n// Start a chat session from a model, with the use of the declared function.\nGenerativeModel\nmodel\n=\nnew\nGenerativeModel\n(\nmodelName\n,\nvertexAI\n)\n.\nwithTools\n(\nArrays\n.\nasList\n(\ntool\n));\nChatSession\nchat\n=\nmodel\n.\nstartChat\n();\nSystem\n.\nout\n.\nprintln\n(\nString\n.\nformat\n(\n\"Ask the question: %s\"\n,\npromptText\n));\nGenerateContentResponse\nresponse\n=\nchat\n.\nsendMessage\n(\npromptText\n);\n// The model will most likely return a function call to the declared\n// function `getCurrentWeather` with \"Paris\" as the value for the\n// argument `location`.\nSystem\n.\nout\n.\nprintln\n(\n\"\\nPrint response: \"\n);\nSystem\n.\nout\n.\nprintln\n(\nResponseHandler\n.\ngetContent\n(\nresponse\n));\n// Provide an answer to the model so that it knows what the result\n// of a \"function call\" is.\nContent\ncontent\n=\nContentMaker\n.\nfromMultiModalData\n(\nPartMaker\n.\nfromFunctionResponse\n(\n\"getCurrentWeather\"\n,\nCollections\n.\nsingletonMap\n(\n\"currentWeather\"\n,\n\"sunny\"\n)));\nSystem\n.\nout\n.\nprintln\n(\n\"Provide the function response: \"\n);\nSystem\n.\nout\n.\nprintln\n(\ncontent\n);\nresponse\n=\nchat\n.\nsendMessage\n(\ncontent\n);\n// See what the model replies now\nSystem\n.\nout\n.\nprintln\n(\n\"Print response: \"\n);\nString\nfinalAnswer\n=\nResponseHandler\n.\ngetText\n(\nresponse\n);\nSystem\n.\nout\n.\nprintln\n(\nfinalAnswer\n);\nreturn\nfinalAnswer\n;\n}\n}\n}\nGo\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithFuncCall\nshows\nhow\nto\nsubmit\na\nprompt\nand\na\nfunction\ndeclaration\nto\nthe\nmodel\n,\n//\nallowing\nit\nto\nsuggest\na\ncall\nto\nthe\nfunction\nto\nfetch\nexternal\ndata\n.\nReturning\nthis\ndata\n//\nenables\nthe\nmodel\nto\ngenerate\na\ntext\nresponse\nthat\nincorporates\nthe\ndata\n.\nfunc\ngenerateWithFuncCall\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nweatherFunc\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nDescription\n:\n\"Returns the current weather in a location.\"\n,\nName\n:\n\"getCurrentWeather\"\n,\nParameters\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"object\"\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"location\"\n:\n{\nType\n:\n\"string\"\n},\n},\nRequired\n:\n[]\nstring\n{\n\"location\"\n},\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nTools\n:\n[]\n*\ngenai\n.\nTool\n{\n{\nFunctionDeclarations\n:\n[]\n*\ngenai\n.\nFunctionDeclaration\n{\nweatherFunc\n}},\n},\nTemperature\n:\ngenai\n.\nPtr\n(\nfloat32\n(\n0.0\n)),\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is the weather like in Boston?\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nvar\nfuncCall\n*\ngenai\n.\nFunctionCall\nfor\n_\n,\np\n:=\nrange\nresp\n.\nCandidates\n[\n0\n]\n.\nContent\n.\nParts\n{\nif\np\n.\nFunctionCall\n!=\nnil\n{\nfuncCall\n=\np\n.\nFunctionCall\nfmt\n.\nFprint\n(\nw\n,\n\"The model suggests to call the function \"\n)\nfmt\n.\nFprintf\n(\nw\n,\n\"%q with args: %v\n\\n\n\"\n,\nfuncCall\n.\nName\n,\nfuncCall\n.\nArgs\n)\n//\nExample\nresponse\n:\n//\nThe\nmodel\nsuggests\nto\ncall\nthe\nfunction\n\"getCurrentWeather\"\nwith\nargs\n:\nmap\n[\nlocation\n:\nBoston\n]\n}\n}\nif\nfuncCall\n==\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"model did not suggest a function call\"\n)\n}\n//\nUse\nsynthetic\ndata\nto\nsimulate\na\nresponse\nfrom\nthe\nexternal\nAPI\n.\n//\nIn\na\nreal\napplication\n,\nthis\nwould\ncome\nfrom\nan\nactual\nweather\nAPI\n.\nfuncResp\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\n\"getCurrentWeather\"\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"location\"\n:\n\"Boston\"\n,\n\"temperature\"\n:\n\"38\"\n,\n\"temperature_unit\"\n:\n\"F\"\n,\n\"description\"\n:\n\"Cold and cloudy\"\n,\n\"humidity\"\n:\n\"65\"\n,\n\"wind\"\n:\n`\n{\n\"speed\"\n:\n\"10\"\n,\n\"direction\"\n:\n\"NW\"\n}\n`\n,\n},\n}\n//\nReturn\nconversation\nturns\nand\nAPI\nresponse\nto\ncomplete\nthe\nmodel\n's response.\ncontents\n=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is the weather like in Boston?\"\n},\n},\nRole\n:\n\"user\"\n},\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nFunctionCall\n:\nfuncCall\n},\n}},\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nFunctionResponse\n:\nfuncResp\n},\n}},\n}\nresp\n,\nerr\n=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThe\nweather\nin\nBoston\nis\ncold\nand\ncloudy\nwith\na\ntemperature\nof\n38\ndegrees\nFahrenheit\n.\nThe\nhumidity\nis\n...\nreturn\nnil\n}\nREST (OpenAI)\nYou can call the Function Calling API by using the OpenAI library. For more information, see\nCall Vertex AI models by using the OpenAI library\n.\nBefore using any of the request data,\nmake the following replacements:\nPROJECT_ID\n: .\nMODEL_ID\n: The ID of the model that's being processed.\nHTTP method and URL:\nPOST https://aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/global/endpoints/openapi/chat/completions\nRequest JSON body:\n{\n\"model\": \"google/\nMODEL_ID\n\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"What is the weather in Boston?\"\n}\n],\n\"tools\": [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"OBJECT\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n]\n}\nTo send your request, choose one of these options:\ncurl\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/global/endpoints/openapi/chat/completions\"\nPowerShell\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/global/endpoints/openapi/chat/completions\" | Select-Object -Expand Content\nPython (OpenAI)\nYou can call the Function Calling API by using the OpenAI library. For more information, see\nCall Vertex AI models by using the OpenAI library\n.\nimport\nvertexai\nimport\nopenai\nfrom\ngoogle.auth\nimport\ndefault\n,\ntransport\n# TODO(developer): Update & uncomment below line\n# PROJECT_ID = \"your-project-id\"\nlocation\n=\n\"us-central1\"\nvertexai\n.\ninit\n(\nproject\n=\nPROJECT_ID\n,\nlocation\n=\nlocation\n)\n# Programmatically get an access token\ncredentials\n,\n_\n=\ndefault\n(\nscopes\n=\n[\n\"https://www.googleapis.com/auth/cloud-platform\"\n])\nauth_request\n=\ntransport\n.\nrequests\n.\nRequest\n()\ncredentials\n.\nrefresh\n(\nauth_request\n)\n# # OpenAI Client\nclient\n=\nopenai\n.\nOpenAI\n(\nbase_url\n=\nf\n\"https://\n{\nlocation\n}\n-aiplatform.googleapis.com/v1beta1/projects/\n{\nPROJECT_ID\n}\n/locations/\n{\nlocation\n}\n/endpoints/openapi\"\n,\napi_key\n=\ncredentials\n.\ntoken\n,\n)\ntools\n=\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"get_current_weather\"\n,\n\"description\"\n:\n\"Get the current weather in a given location\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n,\n},\n},\n\"required\"\n:\n[\n\"location\"\n],\n},\n},\n}\n]\nmessages\n=\n[]\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n,\n}\n)\nmessages\n.\nappend\n({\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in Boston?\"\n})\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"google/gemini-2.0-flash-001\"\n,\nmessages\n=\nmessages\n,\ntools\n=\ntools\n,\n)\nprint\n(\n\"Function:\"\n,\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\ntool_calls\n[\n0\n]\n.\nid\n)\nprint\n(\n\"Arguments:\"\n,\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\ntool_calls\n[\n0\n]\n.\nfunction\n.\narguments\n)\n# Example response:\n# Function: get_current_weather\n# Arguments: {\"location\":\"Boston\"}\nSend a function declaration with\nFunctionCallingConfig\nThe following example demonstrates how to pass a\nFunctionCallingConfig\nto the model.\nThe\nfunctionCallingConfig\nensures that the model output is always a\nspecific function call. To configure:\nSet the function calling\nmode\nto\nANY\n.\nSpecify the function names that you want to use in\nallowed_function_names\n.\nIf\nallowed_function_names\nis empty, any of the provided functions\ncan be returned.\nREST\nPROJECT_ID\n=\nmyproject\nLOCATION\n=\nus-central1\nMODEL_ID\n=\ngemini-2.5-flash\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1beta1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"Do you have the White Pixel 8 Pro 128GB in stock in the US?\"\n}]\n}],\n\"tools\": [{\n\"functionDeclarations\": [\n{\n\"name\": \"get_product_sku\",\n\"description\": \"Get the available inventory for a Google products, e.g: Pixel phones, Pixel Watches, Google Home etc\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"product_name\": {\"type\": \"string\", \"description\": \"Product name\"}\n}\n}\n},\n{\n\"name\": \"get_store_location\",\n\"description\": \"Get the location of the closest store\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\"type\": \"string\", \"description\": \"Location\"}\n},\n}\n}\n]\n}],\n\"toolConfig\": {\n\"functionCallingConfig\": {\n\"mode\":\"ANY\",\n\"allowedFunctionNames\": [\"get_product_sku\"]\n}\n},\n\"generationConfig\": {\n\"temperature\": 0.95,\n\"topP\": 1.0,\n\"maxOutputTokens\": 8192\n}\n}'\nGen AI SDK for Python\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nFunctionDeclaration\n,\nGenerateContentConfig\n,\nHttpOptions\n,\nTool\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nmodel_id\n=\n\"gemini-2.5-flash\"\nget_album_sales\n=\nFunctionDeclaration\n(\nname\n=\n\"get_album_sales\"\n,\ndescription\n=\n\"Gets the number of albums sold\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"OBJECT\"\n,\n\"properties\"\n:\n{\n\"albums\"\n:\n{\n\"type\"\n:\n\"ARRAY\"\n,\n\"description\"\n:\n\"List of albums\"\n,\n\"items\"\n:\n{\n\"description\"\n:\n\"Album and its sales\"\n,\n\"type\"\n:\n\"OBJECT\"\n,\n\"properties\"\n:\n{\n\"album_name\"\n:\n{\n\"type\"\n:\n\"STRING\"\n,\n\"description\"\n:\n\"Name of the music album\"\n,\n},\n\"copies_sold\"\n:\n{\n\"type\"\n:\n\"INTEGER\"\n,\n\"description\"\n:\n\"Number of copies sold\"\n,\n},\n},\n},\n},\n},\n},\n)\nsales_tool\n=\nTool\n(\nfunction_declarations\n=\n[\nget_album_sales\n],\n)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\nmodel_id\n,\ncontents\n=\n'At Stellar Sounds, a music label, 2024 was a rollercoaster. \"Echoes of the Night,\" a debut synth-pop album, '\n'surprisingly sold 350,000 copies, while veteran rock band \"Crimson Tide\n\\'\ns\" latest, \"Reckless Hearts,\" '\n'lagged at 120,000. Their up-and-coming indie artist, \"Luna Bloom\n\\'\ns\" EP, \"Whispers of Dawn,\" '\n'secured 75,000 sales. The biggest disappointment was the highly-anticipated rap album \"Street Symphony\" '\n\"only reaching 100,000 units. Overall, Stellar Sounds moved over 645,000 units this year, revealing unexpected \"\n\"trends in music consumption.\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\nsales_tool\n],\ntemperature\n=\n0\n,\n),\n)\nprint\n(\nresponse\n.\nfunction_calls\n)\n# Example response:\n# [FunctionCall(\n#     id=None,\n#     name=\"get_album_sales\",\n#     args={\n#         \"albums\": [\n#             {\"album_name\": \"Echoes of the Night\", \"copies_sold\": 350000},\n#             {\"copies_sold\": 120000, \"album_name\": \"Reckless Hearts\"},\n#             {\"copies_sold\": 75000, \"album_name\": \"Whispers of Dawn\"},\n#             {\"copies_sold\": 100000, \"album_name\": \"Street Symphony\"},\n#         ]\n#     },\n# )]\nNode.js\nconst\n{\nVertexAI\n,\nFunctionDeclarationSchemaType\n,\n}\n=\nrequire\n(\n'\n@google-cloud/vertexai\n'\n);\nconst\nfunctionDeclarations\n=\n[\n{\nfunction_declarations\n:\n[\n{\nname\n:\n'get_product_sku'\n,\ndescription\n:\n'Get the available inventory for a Google products, e.g: Pixel phones, Pixel Watches, Google Home etc'\n,\nparameters\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nOBJECT\n,\nproperties\n:\n{\nproductName\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n},\n},\n},\n},\n{\nname\n:\n'get_store_location'\n,\ndescription\n:\n'Get the location of the closest store'\n,\nparameters\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nOBJECT\n,\nproperties\n:\n{\nlocation\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n},\n},\n},\n},\n],\n},\n];\nconst\ntoolConfig\n=\n{\nfunction_calling_config\n:\n{\nmode\n:\n'\nANY\n'\n,\nallowed_function_names\n:\n[\n'get_product_sku'\n],\n},\n};\nconst\ngenerationConfig\n=\n{\ntemperature\n:\n0.95\n,\ntopP\n:\n1.0\n,\nmaxOutputTokens\n:\n8192\n,\n};\n/**\n* TODO(developer): Update these variables before running the sample.\n*/\nasync\nfunction\nfunctionCallingAdvanced\n(\nprojectId\n=\n'PROJECT_ID'\n,\nlocation\n=\n'us-central1'\n,\nmodel\n=\n'gemini-2.0-flash-001'\n)\n{\n// Initialize Vertex with your Cloud project and location\nconst\nvertexAI\n=\nnew\nVertexAI\n({\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n});\n// Instantiate the model\nconst\ngenerativeModel\n=\nvertexAI\n.\npreview\n.\ngetGenerativeModel\n({\nmodel\n:\nmodel\n,\n});\nconst\nrequest\n=\n{\ncontents\n:\n[\n{\nrole\n:\n'user'\n,\nparts\n:\n[\n{\ntext\n:\n'Do you have the White Pixel 8 Pro 128GB in stock in the US?'\n},\n],\n},\n],\ntools\n:\nfunctionDeclarations\n,\ntool_config\n:\ntoolConfig\n,\ngeneration_config\n:\ngenerationConfig\n,\n};\nconst\nresult\n=\nawait\ngenerativeModel\n.\ngenerateContent\n(\nrequest\n);\nconsole\n.\nlog\n(\nJSON\n.\nstringify\n(\nresult\n.\nresponse\n.\ncandidates\n[\n0\n].\ncontent\n));\n}\nGo\nimport\n(\n\"context\"\n\"encoding/json\"\n\"errors\"\n\"fmt\"\n\"io\"\n\"cloud.google.com/go/vertexai/genai\"\n)\n// functionCallsChat opens a chat session and sends 4 messages to the model:\n// - convert a first text question into a structured function call request\n// - convert the first structured function call response into natural language\n// - convert a second text question into a structured function call request\n// - convert the second structured function call response into natural language\nfunc\nfunctionCallsChat\n(\nw\nio\n.\nWriter\n,\nprojectID\n,\nlocation\n,\nmodelName\nstring\n)\nerror\n{\n// location := \"us-central1\"\n// modelName := \"gemini-2.0-flash-001\"\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\nprojectID\n,\nlocation\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"unable to create client: %w\"\n,\nerr\n)\n}\ndefer\nclient\n.\nClose\n()\nmodel\n:=\nclient\n.\nGenerativeModel\n(\nmodelName\n)\n// Build an OpenAPI schema, in memory\nparamsProduct\n:=\n&\ngenai\n.\nSchema\n{\nType\n:\ngenai\n.\nTypeObject\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"productName\"\n:\n{\nType\n:\ngenai\n.\nTypeString\n,\nDescription\n:\n\"Product name\"\n,\n},\n},\n}\nfundeclProductInfo\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nName\n:\n\"getProductSku\"\n,\nDescription\n:\n\"Get the SKU for a product\"\n,\nParameters\n:\nparamsProduct\n,\n}\nparamsStore\n:=\n&\ngenai\n.\nSchema\n{\nType\n:\ngenai\n.\nTypeObject\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"location\"\n:\n{\nType\n:\ngenai\n.\nTypeString\n,\nDescription\n:\n\"Location\"\n,\n},\n},\n}\nfundeclStoreLocation\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nName\n:\n\"getStoreLocation\"\n,\nDescription\n:\n\"Get the location of the closest store\"\n,\nParameters\n:\nparamsStore\n,\n}\nmodel\n.\nTools\n=\n[]\n*\ngenai\n.\nTool\n{\n{\nFunctionDeclarations\n:\n[]\n*\ngenai\n.\nFunctionDeclaration\n{\nfundeclProductInfo\n,\nfundeclStoreLocation\n,\n}},\n}\nmodel\n.\nSetTemperature\n(\n0.0\n)\nchat\n:=\nmodel\n.\nStartChat\n()\n// Send a prompt for the first conversation turn that should invoke the getProductSku function\nprompt\n:=\n\"Do you have the Pixel 8 Pro in stock?\"\nfmt\n.\nFprintf\n(\nw\n,\n\"Question: %s\\n\"\n,\nprompt\n)\nresp\n,\nerr\n:=\nchat\n.\nSendMessage\n(\nctx\n,\ngenai\n.\nText\n(\nprompt\n))\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n||\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"empty response from model\"\n)\n}\n// The model has returned a function call to the declared function `getProductSku`\n// with a value for the argument `productName`.\njsondata\n,\nerr\n:=\njson\n.\nMarshalIndent\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n],\n\"\\t\"\n,\n\"  \"\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"json.MarshalIndent: %w\"\n,\nerr\n)\n}\nfmt\n.\nFprintf\n(\nw\n,\n\"function call generated by the model:\\n\\t%s\\n\"\n,\nstring\n(\njsondata\n))\n// Create a function call response, to simulate the result of a call to a\n// real service\nfunresp\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\n\"getProductSku\"\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"sku\"\n:\n\"GA04834-US\"\n,\n\"in_stock\"\n:\n\"yes\"\n,\n},\n}\njsondata\n,\nerr\n=\njson\n.\nMarshalIndent\n(\nfunresp\n,\n\"\\t\"\n,\n\"  \"\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"json.MarshalIndent: %w\"\n,\nerr\n)\n}\nfmt\n.\nFprintf\n(\nw\n,\n\"function call response sent to the model:\\n\\t%s\\n\\n\"\n,\nstring\n(\njsondata\n))\n// And provide the function call response to the model\nresp\n,\nerr\n=\nchat\n.\nSendMessage\n(\nctx\n,\nfunresp\n)\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n||\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"empty response from model\"\n)\n}\n// The model has taken the function call response as input, and has\n// reformulated the response to the user.\njsondata\n,\nerr\n=\njson\n.\nMarshalIndent\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n],\n\"\\t\"\n,\n\"  \"\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"json.MarshalIndent: %w\"\n,\nerr\n)\n}\nfmt\n.\nFprintf\n(\nw\n,\n\"Answer generated by the model:\\n\\t%s\\n\\n\"\n,\nstring\n(\njsondata\n))\n// Send a prompt for the second conversation turn that should invoke the getStoreLocation function\nprompt2\n:=\n\"Is there a store in Mountain View, CA that I can visit to try it out?\"\nfmt\n.\nFprintf\n(\nw\n,\n\"Question: %s\\n\"\n,\nprompt\n)\nresp\n,\nerr\n=\nchat\n.\nSendMessage\n(\nctx\n,\ngenai\n.\nText\n(\nprompt2\n))\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n||\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"empty response from model\"\n)\n}\n// The model has returned a function call to the declared function `getStoreLocation`\n// with a value for the argument `store`.\njsondata\n,\nerr\n=\njson\n.\nMarshalIndent\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n],\n\"\\t\"\n,\n\"  \"\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"json.MarshalIndent: %w\"\n,\nerr\n)\n}\nfmt\n.\nFprintf\n(\nw\n,\n\"function call generated by the model:\\n\\t%s\\n\"\n,\nstring\n(\njsondata\n))\n// Create a function call response, to simulate the result of a call to a\n// real service\nfunresp\n=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\n\"getStoreLocation\"\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"store\"\n:\n\"2000 N Shoreline Blvd, Mountain View, CA 94043, US\"\n,\n},\n}\njsondata\n,\nerr\n=\njson\n.\nMarshalIndent\n(\nfunresp\n,\n\"\\t\"\n,\n\"  \"\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"json.MarshalIndent: %w\"\n,\nerr\n)\n}\nfmt\n.\nFprintf\n(\nw\n,\n\"function call response sent to the model:\\n\\t%s\\n\\n\"\n,\nstring\n(\njsondata\n))\n// And provide the function call response to the model\nresp\n,\nerr\n=\nchat\n.\nSendMessage\n(\nctx\n,\nfunresp\n)\nif\nerr\n!=\nnil\n{\nreturn\nerr\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n||\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"empty response from model\"\n)\n}\n// The model has taken the function call response as input, and has\n// reformulated the response to the user.\njsondata\n,\nerr\n=\njson\n.\nMarshalIndent\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n],\n\"\\t\"\n,\n\"  \"\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"json.MarshalIndent: %w\"\n,\nerr\n)\n}\nfmt\n.\nFprintf\n(\nw\n,\n\"Answer generated by the model:\\n\\t%s\\n\\n\"\n,\nstring\n(\njsondata\n))\nreturn\nnil\n}\nREST (OpenAI)\nYou can call the Function Calling API by using the OpenAI library. For more information, see\nCall Vertex AI models by using the OpenAI library\n.\nBefore using any of the request data,\nmake the following replacements:\nPROJECT_ID\n: .\nMODEL_ID\n: The ID of the model that's being processed.\nHTTP method and URL:\nPOST https://aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/global/endpoints/openapi/chat/completions\nRequest JSON body:\n{\n\"model\": \"google/\nMODEL_ID\n\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"What is the weather in Boston?\"\n}\n],\n\"tools\": [\n{\n\"type\": \"function\",\n\"function\": {\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"OBJECT\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n}\n},\n\"required\": [\"location\"]\n}\n}\n}\n],\n\"tool_choice\": \"auto\"\n}\nTo send your request, choose one of these options:\ncurl\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/global/endpoints/openapi/chat/completions\"\nPowerShell\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://aiplatform.googleapis.com/v1beta1/projects/\nPROJECT_ID\n/locations/global/endpoints/openapi/chat/completions\" | Select-Object -Expand Content\nPython (OpenAI)\nYou can call the Function Calling API by using the OpenAI library. For more information, see\nCall Vertex AI models by using the OpenAI library\n.\nimport\nvertexai\nimport\nopenai\nfrom\ngoogle.auth\nimport\ndefault\n,\ntransport\n# TODO(developer): Update & uncomment below line\n# PROJECT_ID = \"your-project-id\"\nlocation\n=\n\"us-central1\"\nvertexai\n.\ninit\n(\nproject\n=\nPROJECT_ID\n,\nlocation\n=\nlocation\n)\n# Programmatically get an access token\ncredentials\n,\n_\n=\ndefault\n(\nscopes\n=\n[\n\"https://www.googleapis.com/auth/cloud-platform\"\n])\nauth_request\n=\ntransport\n.\nrequests\n.\nRequest\n()\ncredentials\n.\nrefresh\n(\nauth_request\n)\n# OpenAI Client\nclient\n=\nopenai\n.\nOpenAI\n(\nbase_url\n=\nf\n\"https://\n{\nlocation\n}\n-aiplatform.googleapis.com/v1beta1/projects/\n{\nPROJECT_ID\n}\n/locations/\n{\nlocation\n}\n/endpoints/openapi\"\n,\napi_key\n=\ncredentials\n.\ntoken\n,\n)\ntools\n=\n[\n{\n\"type\"\n:\n\"function\"\n,\n\"function\"\n:\n{\n\"name\"\n:\n\"get_current_weather\"\n,\n\"description\"\n:\n\"Get the current weather in a given location\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\"\n,\n},\n},\n\"required\"\n:\n[\n\"location\"\n],\n},\n},\n}\n]\nmessages\n=\n[]\nmessages\n.\nappend\n(\n{\n\"role\"\n:\n\"system\"\n,\n\"content\"\n:\n\"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n,\n}\n)\nmessages\n.\nappend\n({\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the weather in Boston, MA?\"\n})\nresponse\n=\nclient\n.\nchat\n.\ncompletions\n.\ncreate\n(\nmodel\n=\n\"google/gemini-2.0-flash-001\"\n,\nmessages\n=\nmessages\n,\ntools\n=\ntools\n,\ntool_choice\n=\n\"auto\"\n,\n)\nprint\n(\n\"Function:\"\n,\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\ntool_calls\n[\n0\n]\n.\nid\n)\nprint\n(\n\"Arguments:\"\n,\nresponse\n.\nchoices\n[\n0\n]\n.\nmessage\n.\ntool_calls\n[\n0\n]\n.\nfunction\n.\narguments\n)\n# Example response:\n# Function: get_current_weather\n# Arguments: {\"location\":\"Boston\"}\nWhat's next\nFor detailed documentation, see the following:\nFunction calling\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 35914,
      "line_count": 3726
    },
    {
      "filename": "vertex-ai_generative-ai_docs_model-reference_multimodal-live.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nAPI reference\nSend feedback\nLive API reference\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Live API enables low-latency bidirectional voice and video\ninteractions with Gemini. Using the Live API, you can\nprovide end users with the experience of natural, human-like voice\nconversations, and with the ability to interrupt the model's responses using\nvoice commands. The Live API\ncan process text, audio, and video input, and it can provide text and audio\noutput.\nFor more information about the Live API, see\nLive API\n.\nCapabilities\nLive API includes the following key capabilities:\nMultimodality\n: The model can see, hear, and speak.\nLow-latency realtime interaction\n: The model can provide fast responses.\nSession memory\n: The model retains memory of all interactions\nwithin a single session, recalling previously heard or seen information.\nSupport for function calling and Search as a Tool\n:\nYou can integrate the model with external services and data sources.\nLive API is designed for server-to-server communication.\nFor web and mobile apps, we recommend using the integration from our partners\nat\nDaily\n.\nSupported models\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGet started\nTo try the Live API, go to the\nVertex AI Studio\n,\nand then click\nStart Session\n.\nLive API is a stateful API that uses\nWebSockets\n.\nThis section shows an example of how to use Live API\nfor text-to-text generation, using Python 3.9+.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nContent\n,\nHttpOptions\n,\nLiveConnectConfig\n,\nModality\n,\nPart\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1beta1\"\n))\nmodel_id\n=\n\"gemini-2.0-flash-live-preview-04-09\"\nasync\nwith\nclient\n.\naio\n.\nlive\n.\nconnect\n(\nmodel\n=\nmodel_id\n,\nconfig\n=\nLiveConnectConfig\n(\nresponse_modalities\n=\n[\nModality\n.\nTEXT\n]),\n)\nas\nsession\n:\ntext_input\n=\n\"Hello? Gemini, are you there?\"\nprint\n(\n\"> \"\n,\ntext_input\n,\n\"\n\\n\n\"\n)\nawait\nsession\n.\nsend_client_content\n(\nturns\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n(\ntext\n=\ntext_input\n)])\n)\nresponse\n=\n[]\nasync\nfor\nmessage\nin\nsession\n.\nreceive\n():\nif\nmessage\n.\ntext\n:\nresponse\n.\nappend\n(\nmessage\n.\ntext\n)\nprint\n(\n\"\"\n.\njoin\n(\nresponse\n))\n# Example output:\n# >  Hello? Gemini, are you there?\n# Yes, I'm here. What would you like to talk about?\nIntegration guide\nThis section describes how integration works with Live API.\nSessions\nA WebSocket connection establishes a session between the client and the\nGemini server.\nAfter a client initiates a new connection the session can exchange messages\nwith the server to:\nSend text, audio, or video to the Gemini server.\nReceive audio, text, or function call requests from the\nGemini server.\nThe session configuration is sent in the first message after connection.\nA session configuration includes the model, generation parameters,\nsystem instructions, and tools.\nSee the following example configuration:\n{\n\"model\": string,\n\"generationConfig\": {\n\"candidateCount\": integer,\n\"maxOutputTokens\": integer,\n\"temperature\": number,\n\"topP\": number,\n\"topK\": integer,\n\"presencePenalty\": number,\n\"frequencyPenalty\": number,\n\"responseModalities\": [string],\n\"speechConfig\": object\n},\n\"systemInstruction\": string,\n\"tools\": [object]\n}\nFor more information, see\nBidiGenerateContentSetup\n.\nSend messages\nMessages are JSON-formatted objects exchanged over the WebSocket connection.\nTo send a message the client must send a JSON object over an open WebSocket\nconnection. The JSON object must have\nexactly one\nof the fields from the\nfollowing object set:\n{\n\"setup\": BidiGenerateContentSetup,\n\"clientContent\": BidiGenerateContentClientContent,\n\"realtimeInput\": BidiGenerateContentRealtimeInput,\n\"toolResponse\": BidiGenerateContentToolResponse\n}\nSupported client messages\nSee the supported client messages in the following table:\nMessage\nDescription\nBidiGenerateContentSetup\nSession configuration to be sent in the first message\nBidiGenerateContentClientContent\nIncremental content update of the current conversation delivered from the client\nBidiGenerateContentRealtimeInput\nReal time audio or video input\nBidiGenerateContentToolResponse\nResponse to a\nToolCallMessage\nreceived from the server\nReceive messages\nTo receive messages from Gemini, listen for the WebSocket\n'message' event, and then parse the result according to the definition of the\nsupported server messages.\nSee the following:\nws\n.\naddEventListener\n(\n\"message\"\n,\nasync\n(\nevt\n)\n=>\n{\nif\n(\nevt\n.\ndata\ninstanceof\nBlob\n)\n{\n//\nProcess\nthe\nreceived\ndata\n(\naudio\n,\nvideo\n,\netc\n.\n)\n}\nelse\n{\n//\nProcess\nJSON\nresponse\n}\n});\nServer messages will have\nexactly one\nof the fields from the following object\nset:\n{\n\"setupComplete\": BidiGenerateContentSetupComplete,\n\"serverContent\": BidiGenerateContentServerContent,\n\"toolCall\": BidiGenerateContentToolCall,\n\"toolCallCancellation\": BidiGenerateContentToolCallCancellation\n\"usageMetadata\": UsageMetadata\n\"goAway\": GoAway\n\"sessionResumptionUpdate\": SessionResumptionUpdate\n\"inputTranscription\": BidiGenerateContentTranscription\n\"outputTranscription\": BidiGenerateContentTranscription\n}\nSupported server messages\nSee the supported server messages in the following table:\nMessage\nDescription\nBidiGenerateContentSetupComplete\nA\nBidiGenerateContentSetup\nmessage from the client, sent when setup is complete\nBidiGenerateContentServerContent\nContent generated by the model in response to a client message\nBidiGenerateContentToolCall\nRequest for the client to run the function calls and return the responses with the matching IDs\nBidiGenerateContentToolCallCancellation\nSent when a function call is canceled due to the user interrupting model output\nUsageMetadata\nA report of the number of tokens used by the session so far\nGoAway\nA signal that the current connection will soon be terminated\nSessionResumptionUpdate\nA session checkpoint, which can be resumed\nBidiGenerateContentTranscription\nA transcription of either the user's or model's speech\nIncremental content updates\nUse incremental updates to send text input, establish session context, or\nrestore session context. For short contexts you can send turn-by-turn\ninteractions to represent the exact sequence of events. For longer contexts\nit's recommended to provide a single message summary to free up the\ncontext window for the follow up interactions.\nSee the following example context message:\n{\n\"clientContent\"\n:\n{\n\"turns\"\n:\n[\n{\n\"parts\"\n:[\n{\n\"text\"\n:\n\"\"\n}\n],\n\"role\"\n:\n\"user\"\n},\n{\n\"parts\"\n:[\n{\n\"text\"\n:\n\"\"\n}\n],\n\"role\"\n:\n\"model\"\n}\n],\n\"turnComplete\"\n:\ntrue\n}\n}\nNote that while content parts can be of a\nfunctionResponse\ntype,\nBidiGenerateContentClientContent\nshouldn't be used to provide a response\nto the function calls issued by the model.\nBidiGenerateContentToolResponse\nshould be used instead.\nBidiGenerateContentClientContent\nshould only be\nused to establish previous context or provide text input to the conversation.\nStreaming audio and video\nFunction calling\nAll functions must be declared at the start of the session by sending tool\ndefinitions as part of the\nBidiGenerateContentSetup\nmessage.\nYou define functions by using JSON, specifically with a\nselect subset\nof the\nOpenAPI schema format\n.\nA single function declaration can include the following parameters:\nname\n(string): The unique identifier for the function\nwithin the API call.\ndescription\n(string): A comprehensive explanation\nof the function's purpose and capabilities.\nparameters\n(object): Defines the input data required\nby the function.\ntype\n(string): Specifies the overall data type,\nsuch as object.\nproperties\n(object): Lists individual parameters,\neach with:\ntype\n(string): The data type of the parameter,\nsuch as string, integer, boolean.\ndescription\n(string): A clear explanation of\nthe parameter's purpose and expected format.\nrequired\n(array): An array of strings listing\nthe parameter names that are mandatory for the function to operate.\nFor code examples of a function declaration using curl commands, see\nFunction calling with the Gemini\nAPI\n.\nFor examples of how to create function declarations using the\nGemini API SDKs, see the\nFunction calling\ntutorial\n.\nFrom a single prompt, the model can generate multiple function calls and the\ncode necessary to chain their outputs. This code executes in a\nsandbox environment, generating subsequent\nBidiGenerateContentToolCall\nmessages. The execution pauses until the results of each function call\nare available, which ensures sequential processing.\nThe client should respond with\nBidiGenerateContentToolResponse\n.\nTo learn more, see\nIntroduction to function calling\n.\nAudio formats\nSee the list of\nsupported audio formats\n.\nSystem instructions\nYou can provide system instructions to better control the model's output\nand specify the tone and sentiment of audio responses.\nSystem instructions are added to the prompt before the interaction begins\nand remain in effect for the entire session.\nSystem instructions can only be set at the beginning of a session,\nimmediately following the initial connection. To provide further input\nto the model during the session, use incremental content updates.\nInterruptions\nUsers can interrupt the model's output at any time. When\nVoice activity detection (VAD) detects an interruption, the ongoing\ngeneration is canceled and discarded. Only the information already sent\nto the client is retained in the session history. The server then\nsends a\nBidiGenerateContentServerContent\nmessage to report the interruption.\nIn addition, the Gemini server discards any pending\nfunction calls and sends a\nBidiGenerateContentServerContent\nmessage with the\nIDs of the canceled calls.\nVoices\nTo specify a voice, set the\nvoiceName\nwithin the\nspeechConfig\nobject,\nas part of your\nsession configuration\n.\nSee the following JSON representation of a\nspeechConfig\nobject:\n{\n\"voiceConfig\"\n:\n{\n\"prebuiltVoiceConfig\"\n:\n{\n\"voiceName\"\n:\n\"\nVOICE_NAME\n\"\n}\n}\n}\nTo see the list of supported voices, see\nChange voice and language settings\n.\nLimitations\nConsider the following limitations of Live API and\nGemini 2.0 when you plan your project.\nClient authentication\nLive API only provides server to server authentication\nand isn't recommended for direct client use. Client input should be routed\nthrough an intermediate application server for secure authentication with\nthe Live API.\nMaximum session duration\nThe default maximum length of a conversation session is 10 minutes. For\nmore information, see\nSession length\n.\nVideo frame rate\nWhen you stream video to the model, it is processed at 1 frame per second (FPS). This makes the API unsuitable for use cases that require analyzing fast-changing video, such as play-by-play in high-speed sports.\nVoice activity detection (VAD)\nBy default, the model automatically performs voice activity detection (VAD) on\na continuous audio input stream. VAD can be configured with the\nRealtimeInputConfig.AutomaticActivityDetection\nfield of the\nsetup message\n.\nWhen the audio stream is paused for more than a second (for example,\nwhen the user switches off the microphone), an\nAudioStreamEnd\nevent is sent to flush any cached audio. The client can resume sending\naudio data at any time.\nAlternatively, the automatic VAD can be turned off by setting\nRealtimeInputConfig.AutomaticActivityDetection.disabled\nto\ntrue\nin the setup\nmessage. In this configuration the client is responsible for detecting user\nspeech and sending\nActivityStart\nand\nActivityEnd\nmessages at the appropriate times. An\nAudioStreamEnd\nisn't sent in\nthis configuration. Instead, any interruption of the stream is marked by\nan\nActivityEnd\nmessage.\nAdditional limitations\nManual endpointing isn't supported.\nAudio inputs and audio outputs negatively impact the model's ability to\nuse function calling.\nToken count\nToken count isn't supported.\nRate limits\nThe following rate limits apply:\n1,000 concurrent sessions per project\n4M tokens per minute\nMessages and events\nBidiGenerateContentClientContent\nIncremental update of the current conversation delivered from the client. All the content here is unconditionally appended to the conversation history and used as part of the prompt to the model to generate content.\nA message here will interrupt any current model generation.\nFields\nturns[]\nContent\nOptional. The content appended to the current conversation with the model.\nFor single-turn queries, this is a single instance. For multi-turn queries, this is a repeated field that contains conversation history and latest request.\nturn_complete\nbool\nOptional. If true, indicates that the server content generation should start with the currently accumulated prompt. Otherwise, the server will await additional messages before starting generation.\nBidiGenerateContentRealtimeInput\nUser input that is sent in real time.\nThis is different from\nClientContentUpdate\nin a few ways:\nCan be sent continuously without interruption to model generation.\nIf there is a need to mix data interleaved across the\nClientContentUpdate\nand the\nRealtimeUpdate\n, server attempts to  optimize for best response, but there are no guarantees.\nEnd of turn is not explicitly specified, but is rather derived from user  activity (for example, end of speech).\nEven before the end of turn, the data is processed incrementally  to optimize for a fast start of the response from the model.\nIs always assumed to be the user's input (cannot be used to populate  conversation history).\nFields\nmedia_chunks[]\nBlob\nOptional. Inlined bytes data for media input.\nactivity_start\nActivityStart\nOptional. Marks the start of user activity. This can only be sent if automatic (i.e. server-side) activity detection is disabled.\nactivity_end\nActivityEnd\nOptional. Marks the end of user activity. This can only be sent if automatic (i.e. server-side) activity detection is disabled.\nActivityEnd\nThis type has no fields.\nMarks the end of user activity.\nActivityStart\nThis type has no fields.\nOnly one of the fields in this message must be set at a time. Marks the start of user activity.\nBidiGenerateContentServerContent\nIncremental server update generated by the model in response to client messages.\nContent is generated as quickly as possible, and not in realtime. Clients may choose to buffer and play it out in realtime.\nFields\nturn_complete\nbool\nOutput only. If true, indicates that the model is done generating. Generation will only start in response to additional client messages. Can be set alongside\ncontent\n, indicating that the\ncontent\nis the last in the turn.\ninterrupted\nbool\nOutput only. If true, indicates that a client message has interrupted current model generation. If the client is playing out the content in realtime, this is a good signal to stop and empty the current queue. If the client is playing out the content in realtime, this is a good signal to stop and empty the current playback queue.\ngeneration_complete\nbool\nOutput only. If true, indicates that the model is done generating.\nWhen model is interrupted while generating there will be no 'generation_complete' message in interrupted turn, it will go through 'interrupted > turn_complete'.\nWhen model assumes realtime playback there will be delay between generation_complete and turn_complete that is caused by model waiting for playback to finish.\ngrounding_metadata\nGroundingMetadata\nOutput only. Metadata specifies sources used to ground generated content.\ninput_transcription\nTranscription\nOptional. Input transcription. The transcription is independent to the model turn which means it doesn't imply any ordering between transcription and model turn.\noutput_transcription\nTranscription\nOptional. Output transcription. The transcription is independent to the model turn which means it doesn't imply any ordering between transcription and model turn.\nmodel_turn\nContent\nOutput only. The content that the model has generated as part of the current conversation with the user.\nTranscription\nAudio transcription message.\nFields\ntext\nstring\nOptional. Transcription text.\nfinished\nbool\nOptional. The bool indicates the end of the transcription.\nBidiGenerateContentSetup\nMessage to be sent in the first and only first client message. Contains configuration that will apply for the duration of the streaming session.\nClients should wait for a\nBidiGenerateContentSetupComplete\nmessage before sending any additional messages.\nFields\nmodel\nstring\nRequired. The fully qualified name of the publisher model.\nPublisher model format:\nprojects/{project}/locations/{location}/publishers/\\*/models/\\*\ngeneration_config\nGenerationConfig\nOptional. Generation config.\nThe following fields aren't supported:\nresponse_logprobs\nresponse_mime_type\nlogprobs\nresponse_schema\nstop_sequence\nrouting_config\naudio_timestamp\nsystem_instruction\nContent\nOptional. The user provided system instructions for the model. Note: only text should be used in parts and content in each part will be in a separate paragraph.\ntools[]\nTool\nOptional. A list of\nTools\nthe model may use to generate the next response.\nA\nTool\nis a piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of knowledge and scope of the model.\nsession_resumption\nSessionResumptionConfig\nOptional. Configures session resumption mechanism. If included, the server will send periodical\nSessionResumptionUpdate\nmessages to the client.\ncontext_window_compression\nContextWindowCompressionConfig\nOptional. Configures context window compression mechanism.\nIf included, server will compress context window to fit into given length.\nrealtime_input_config\nRealtimeInputConfig\nOptional. Configures the handling of realtime input.\ninput_audio_transcription\nAudioTranscriptionConfig\nOptional. The transcription of the input aligns with the input audio language.\noutput_audio_transcription\nAudioTranscriptionConfig\nOptional. The transcription of the output aligns with the language code specified for the output audio.\nAudioTranscriptionConfig\nThis type has no fields.\nThe audio transcription configuration.\nBidiGenerateContentSetupComplete\nSent in response to a\nBidiGenerateContentSetup\nmessage from the client.\nFields\nsession_id\nstring\nThe session id of the session.\nBidiGenerateContentToolCall\nRequest for the client to execute the\nfunction_calls\nand return the responses with the matching\nid\ns.\nFields\nfunction_calls[]\nFunctionCall\nOutput only. The function call to be executed.\nBidiGenerateContentToolCallCancellation\nNotification for the client that a previously issued\nToolCallMessage\nwith the specified\nid\ns should have been not executed and should be cancelled. If there were side-effects to those tool calls, clients may attempt to undo the tool calls. This message occurs only in cases where the clients interrupt server turns.\nFields\nids[]\nstring\nOutput only. The ids of the tool calls to be cancelled.\nBidiGenerateContentToolResponse\nClient generated response to a\nToolCall\nreceived from the server. Individual\nFunctionResponse\nobjects are matched to the respective\nFunctionCall\nobjects by the\nid\nfield.\nNote that in the unary and server-streaming GenerateContent APIs function calling happens by exchanging the\nContent\nparts, while in the bidi GenerateContent APIs function calling happens over these dedicated set of messages.\nFields\nfunction_responses[]\nFunctionResponse\nOptional. The response to the function calls.\nRealtimeInputConfig\nConfigures the realtime input behavior in\nBidiGenerateContent\n.\nFields\nautomatic_activity_detection\nAutomaticActivityDetection\nOptional. If not set, automatic activity detection is enabled by default. If automatic voice detection is disabled, the client must send activity signals.\nactivity_handling\nActivityHandling\nOptional. Defines what effect activity has.\nturn_coverage\nTurnCoverage\nOptional. Defines which input is included in the user's turn.\nActivityHandling\nThe different ways of handling user activity.\nEnums\nACTIVITY_HANDLING_UNSPECIFIED\nIf unspecified, the default behavior is\nSTART_OF_ACTIVITY_INTERRUPTS\n.\nSTART_OF_ACTIVITY_INTERRUPTS\nIf true, start of activity will interrupt the model's response (also called \"barge in\"). The model's current response will be cut-off in the moment of the interruption. This is the default behavior.\nNO_INTERRUPTION\nThe model's response will not be interrupted.\nAutomaticActivityDetection\nConfigures automatic detection of activity.\nFields\nstart_of_speech_sensitivity\nStartSensitivity\nOptional. Determines how likely speech is to be detected.\nend_of_speech_sensitivity\nEndSensitivity\nOptional. Determines how likely detected speech is ended.\nprefix_padding_ms\nint32\nOptional. The required duration of detected speech before start-of-speech is committed. The lower this value the more sensitive the start-of-speech detection is and the shorter speech can be recognized. However, this also increases the probability of false positives.\nsilence_duration_ms\nint32\nOptional. The required duration of detected silence (or non-speech) before end-of-speech is committed. The larger this value, the longer speech gaps can be without interrupting the user's activity but this will increase the model's latency.\ndisabled\nbool\nOptional. If enabled, detected voice and text input count as activity. If disabled, the client must send activity signals.\nEndSensitivity\nEnd of speech sensitivity.\nEnums\nEND_SENSITIVITY_UNSPECIFIED\nThe default is END_SENSITIVITY_LOW.\nEND_SENSITIVITY_HIGH\nAutomatic detection ends speech more often.\nEND_SENSITIVITY_LOW\nAutomatic detection ends speech less often.\nStartSensitivity\nStart of speech sensitivity.\nEnums\nSTART_SENSITIVITY_UNSPECIFIED\nThe default is START_SENSITIVITY_LOW.\nSTART_SENSITIVITY_HIGH\nAutomatic detection will detect the start of speech more often.\nSTART_SENSITIVITY_LOW\nAutomatic detection will detect the start of speech less often.\nTurnCoverage\nOptions about which input is included in the user's turn.\nEnums\nTURN_COVERAGE_UNSPECIFIED\nIf unspecified, the default behavior is\nTURN_INCLUDES_ALL_INPUT\n.\nTURN_INCLUDES_ONLY_ACTIVITY\nThe users turn only includes activity since the last turn, excluding inactivity (e.g. silence on the audio stream).\nTURN_INCLUDES_ALL_INPUT\nThe users turn includes all realtime input since the last turn, including inactivity (e.g. silence on the audio stream). This is the default behavior.\nUsageMetadata\nMetadata on the usage of the cached content.\nFields\ntotal_token_count\nint32\nTotal number of tokens that the cached content consumes.\ntext_count\nint32\nNumber of text characters.\nimage_count\nint32\nNumber of images.\nvideo_duration_seconds\nint32\nDuration of video in seconds.\naudio_duration_seconds\nint32\nDuration of audio in seconds.\nGoAway\nServer will not be able to service client soon.\nFields\ntime_left\nDuration\nThe remaining time before the connection will be terminated as ABORTED. The minimal time returned here is specified differently together with the rate limits for a given model.\nSessionResumptionUpdate\nUpdate of the session resumption state.\nOnly sent if\nBidiGenerateContentSetup.session_resumption\nwas set.\nFields\nnew_handle\nstring\nNew handle that represents state that can be resumed. Empty if\nresumable\n=false.\nresumable\nbool\nTrue if session can be resumed at this point.\nIt might be not possible to resume session at some points. In that case we send update empty new_handle and resumable=false. Example of such case could be model executing function calls or just generating. Resuming session (using previous session token) in such state will result in some data loss.\nlast_consumed_client_message_index\nint64\nIndex of last message sent by client that is included in state represented by this SessionResumptionToken. Only sent when\nSessionResumptionConfig.transparent\nis set.\nPresence of this index allows users to transparently reconnect and avoid issue of losing some part of realtime audio input/video. If client wishes to temporarily disconnect (for example as result of receiving GoAway) they can do it without losing state by buffering messages sent since last\nSessionResmumptionTokenUpdate\n. This field will enable them to limit buffering (avoid keeping all requests in RAM).\nIt will not be used for 'resumption to restore state' some time later -- in those cases partial audio and video frames are likely not needed.\nWhat's next\nLearn more about\nfunction\ncalling\n.\nSee the\nFunction calling\nreference\nfor examples.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 25240,
      "line_count": 899
    },
    {
      "filename": "vertex-ai_generative-ai_docs_models_evaluation-overview.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGen AI evaluation service overview\nStay organized with collections\nSave and categorize content based on your preferences.\nThe Gen AI evaluation service provides enterprise-grade tools for objective, data-driven assessment of generative AI models. It supports and informs a number of development tasks like model migrations, prompt editing, and fine-tuning.\nGen AI evaluation service features\nThe defining feature of the Gen AI evaluation service is the ability to use\nadaptive rubrics\n, a set of tailored pass or fail tests for each individual prompt. Evaluation rubrics are similar to unit tests in software development and aim to improve model performance across a variety of tasks.\nThe Gen AI evaluation service supports the following common evaluation methods:\nAdaptive rubrics (Recommended)\n: Generates a unique set of pass or fail\nrubrics\nfor each individual prompt in your dataset.\nStatic rubrics\n: Apply a fixed set of scoring criteria across all prompts.\nComputation-based metrics\n: Use deterministic algorithms like\nROUGE\nor\nBLEU\nwhen a ground truth is available.\nCustom functions\n: Define your own evaluation logic in Python for specialized requirements.\nEvaluation dataset generation\nYou can create an evaluation dataset through the following methods:\nUpload a file\ncontaining complete prompt instances, or provide a prompt template alongside a corresponding file of variable values to populate the completed prompts.\nSample directly from production logs\nto evaluate the real-world usage of your model.\nUse synthetic data generation\nto generate a large number of consistent examples for any prompt template.\nSupported interfaces\nYou can define and run your evaluations using the following interfaces:\nGoogle Cloud console\n: A\nweb user interface\nthat provides a guided, end-to-end workflow. Manage your datasets, run evaluations, and dive deep into interactive reports and visualizations. See\nPerform evaluation using the console\n.\nPython SDK\n: Programmatically run evaluations and render side-by-side model comparisons directly in your Colab or Jupyter environment. See\nPerform evaluation using the GenAI Client in Vertex AI SDK\nUse cases\nThe Gen AI evaluation service lets you see how a model performs on your specific tasks and against your unique criteria providing valuable insights which cannot be derived from public leaderboards and general benchmarks. This supports critical development tasks, including:\nModel migrations\n: Compare model versions to understand behavioral differences and fine-tune your prompts and settings accordingly.\nFinding the best model\n: Run head-to-head comparisons of Google and third-party models on your data to establish a performance baseline and identify the best fit for your use case.\nPrompt improvement\n: Use evaluation results to guide your customization efforts. Re-running an evaluation creates a tight feedback loop, providing immediate, quantifiable feedback on your changes.\nModel fine-tuning\n: Evaluate the quality of a fine-tuned model by applying consistent evaluation criteria to every run.\nAgent evaluation\n: Evaluate the performance of an agent using\nagent-specific metrics, such as agent traces and response quality.\nEvaluations with adaptive rubrics\nAdaptive rubrics\nare the recommended method for most evaluation use cases and are typically the fastest way to get started with evaluations.\nInstead of using a general set of rating rubrics like most LLM-as-a-judge systems, the test-driven evaluation framework adaptively generates a unique set of pass or fail\nrubrics\nfor each individual prompt in your dataset. This approach ensures that every evaluation is relevant to the specific task being evaluated.\nThe evaluation process for each prompt uses a two-step system:\nRubric generation\n: The service first analyzes your prompt and generates a list of specific, verifiable tests—the rubrics—that a good response should meet.\nRubric validation\n: After your model generates a response, the service assesses the response against each rubric, delivering a clear\nPass\nor\nFail\nverdict and a rationale.\nThe final result is an aggregated pass rate and a detailed breakdown of which rubrics the model passed, giving you actionable insights to diagnose issues and measure improvements.\nBy moving from high-level, subjective scores to granular, objective test results, you can adopt an evaluation-driven development cycle and bring the software engineering best practices to the process of building generative AI applications.\nRubrics evaluation example\nTo understand how the Gen AI evaluation service generates and uses rubrics, consider this example:\nUser prompt\n:\nWrite a four-sentence summary of the provided article about renewable energy, maintaining an optimistic tone.\nFor this prompt, the\nrubric generation\nstep might produce the following rubrics:\nRubric 1\n: The response is a summary of the provided article.\nRubric 2\n: The response contains exactly four sentences.\nRubric 3\n: The response maintains an optimistic tone.\nYour model may produce the following response:\nThe article highlights significant growth in solar and wind power. These advancements are making clean energy more affordable. The future looks bright for renewables. However, the report also notes challenges with grid infrastructure.\nDuring\nrubric validation\n, the Gen AI evaluation service assesses the response against each rubric:\nRubric 1\n: The response is a summary of the provided article.\nVerdict\n:\nPass\nReason\n: The response accurately summarizes the main points.\nRubric 2\n: The response contains exactly four sentences.\nVerdict\n:\nPass\nReason\n: The response is composed of four distinct sentences\nRubric 3\n: The response maintains an optimistic tone.\nVerdict\n:\nFail\nReason\n: The final sentence introduces a negative point, which detracts from the optimistic tone.\nThe final pass rate for this response is 66.7%. To compare two models, you can evaluate their responses against this same set of generated tests and compare their overall pass rates.\nEvaluation workflow\nCompleting an evaluation typically requires going through the following steps:\nCreate an evaluation dataset\n: Assemble a dataset of prompt instances that reflect your specific use case. You can include reference answers (ground truth) if you plan to use computation-based metrics.\nDefine evaluation metrics\n: Choose the metrics you want to use to measure model performance. The SDK supports all metric types, while the console supports adaptive rubrics.\nGenerate model responses\n: Select one or more models to generate responses for your dataset. The SDK supports any model callable via\nLiteLLM\n, while the console supports Google Gemini models.\nRun the evaluation\n: Execute the evaluation job, which assesses each model's responses against your selected metrics.\nInterpret the results\n: Review the aggregated scores and individual responses to analyze model performance.\nGetting started with evaluations\nYou can get started with evaluations\nusing the console\n.\nAlternatively, the following code shows how to complete an evaluation with the GenAI Client in Vertex AI SDK:\nfrom\nvertexai\nimport\nClient\nfrom\nvertexai\nimport\ntypes\nimport\npandas\nas\npd\nclient\n=\nClient\n(\nproject\n=\nPROJECT_ID\n,\nlocation\n=\nLOCATION\n)\n# Create an evaluation dataset\nprompts_df\n=\npd\n.\nDataFrame\n({\n\"prompt\"\n:\n[\n\"Write a simple story about a dinosaur\"\n,\n\"Generate a poem about Vertex AI\"\n,\n],\n})\n# Get responses from one or multiple models\neval_dataset\n=\nclient\n.\nevals\n.\nrun_inference\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\nsrc\n=\nprompts_df\n)\n# Define the evaluation metrics and run the evaluation job\neval_result\n=\nclient\n.\nevals\n.\nevaluate\n(\ndataset\n=\neval_dataset\n,\nmetrics\n=\n[\ntypes\n.\nRubricMetric\n.\nGENERAL_QUALITY\n]\n)\n# View the evaluation results\neval_result\n.\nshow\n()\nThe Gen AI evaluation service offers two SDK interfaces:\nGenAI Client in Vertex AI SDK\n(Recommended) (Preview)\nfrom vertexai import client\nThe GenAI Client is the newer, recommended interface for evaluation, accessed through the unified Client class. It supports all evaluation methods and is designed for workflows that include model comparison, in-notebook visualization, and insights for model customization.\nEvaluation module in Vertex AI SDK\n(GA)\nfrom vertexai.evaluation import EvalTask\nThe evaluation module is the\nolder interface\n, maintained for backward compatibility with existing workflows but no longer under active development. It is accessed through the\nEvalTask\nclass. This method supports standard LLM-as-a-judge and computation-based metrics but does not support newer evaluation methods like adaptive rubrics.\nSupported regions\nThe following regions are supported for the Gen AI evaluation service:\nIowa (\nus-central1\n)\nNorthern Virginia (\nus-east4\n)\nOregon (\nus-west1\n)\nLas Vegas, Nevada (\nus-west4\n)\nBelgium (\neurope-west1\n)\nNetherlands (\neurope-west4\n)\nParis, France (\neurope-west9\n)\nAvailable notebooks\nNotebook links\nDescription\nGetting Started: Quick Gen AI Evaluation\nProvides an introduction to the Gen AI evaluation service.\nEvaluating third-party models with the Gen AI evaluation service\nDemonstrates how to use the Vertex Gen AI Evaluation SDK to evaluate various types of third-party models, including models accessed using API (like OpenAI, Anthropic), Model as a Service (MaaS) from Vertex Model Garden, and Bring Your Own Model (BYOM) endpoints.\nModel migration with the Gen AI evaluation service\nShows how to use the Vertex AI SDK for Gen AI evaluation service to compare two first-party models (such as Gemini 2.0 Flash to Gemini 2.5 Flash). It highlights using predefined adaptive rubric-based metrics and how evaluation results can guide prompt optimization. Key features like multi-candidate evaluation, in-notebook visualization, and asynchronous batch evaluation are also covered.\nWhat's next\nPerform evaluation using the console\nPerform evaluation using the SDK\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 10448,
      "line_count": 282
    },
    {
      "filename": "vertex-ai_generative-ai_docs_models_gemini_2-5-flash-live-api.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-live-api",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-live-api\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGemini 2.5 Flash Live API native audio\nStay organized with collections\nSave and categorize content based on your preferences.\nGemini 2.5 Flash with Live API native audio features our\ncutting-edge native audio functionality for\nLive API\n. In addition to the\nstandard Live API features, this preview model includes:\nEnhanced audio quality:\nExperience dramatically improved audio\nquality that feels like speaking with a person.\nEnhanced voice quality and adaptability:\nLive API\nnative audio provides richer, more natural voice interactions with\n30 HD voices\nin\n24 languages\n.\nIntroducing\nProactive Audio\n:\nWhen Proactive Audio is enabled, the model only responds when it's\nrelevant. The model generates text transcripts and audio responses\nproactively only for queries directed to the device, and does not\nrespond to non-device directed queries.\nIntroducing Affective Dialog:\nModels using\nLive API native audio can understand and respond\nappropriately to users' emotional expressions for more nuanced\nconversations.\nImproved barge-in:\nInterrupt Gemini more naturally and\nreliably, even in loud and noisy environments.\nRobust function calling:\nWe've improved the triggering rate,\nallowing Gemini to successfully execute the functions you\ndefine to support your use cases.\nAccurate transcription:\nThe accuracy of audio-to-text\ntranscription has been significantly enhanced.\nSeamless multilingual support:\nSpeak to Gemini in\nmultiple languages, and it will effortlessly switch between them without\nany pre-configuration. Language is no longer a barrier.\nFor more information on Live API, see:\nOur\nstandalone Live API documentation\n.\nOur\nLive API supported audio formats\n.\nOur\nLive API concurrent session limits\n.\nTry in\nVertex AI\nModel ID\ngemini-live-2.5-flash-preview-native-audio-09-2025\nSupported inputs & outputs\nInputs:\nText\n,\nImages\n,\nAudio\n,\nVideo\nOutputs:\nText\n,\nAudio\nToken limits\nMaximum input tokens: 128K\nMaximum output tokens: 64K\nContext window: 32K (default), upgradable to 128K\nCapabilities\nSupported\nGrounding with Google Search\nSystem instructions\nFunction calling\nLive API\npreview\nPreview feature\nNot supported\nCode execution\nTuning\nStructured output\nThinking\nVertex AI RAG Engine\nChat completions\nUsage types\nSupported\nUp to 1000 concurrent sessions\nProvisioned Throughput\nNot supported\nDynamic shared quota\nBatch prediction\nTechnical specifications\nImages\nphoto\nMaximum images per prompt:\n3,000\nMaximum image size:\n7 MB\nSupported MIME types:\nimage/png\n,\nimage/jpeg\n,\nimage/webp\nVideo\nvideocam\nStandard resolution:\n768 x 768\nSupported MIME types:\nvideo/x-flv\n,\nvideo/quicktime\n,\nvideo/mpeg\n,\nvideo/mpegs\n,\nvideo/mpg\n,\nvideo/mp4\n,\nvideo/webm\n,\nvideo/wmv\n,\nvideo/3gpp\nAudio\nmic\nMaximum conversation length:\nDefault 10 minutes that can\nbe extended.\nRequired audio input format:\nRaw 16-bit PCM audio at 16kHz, little-endian\nRequired audio output format:\nRaw 16-bit PCM audio at 24kHz, little-endian\nSupported MIME types:\naudio/x-aac\n,\naudio/flac\n,\naudio/mp3\n,\naudio/m4a\n,\naudio/mpeg\n,\naudio/mpga\n,\naudio/mp4\n,\naudio/ogg\n,\naudio/pcm\n,\naudio/wav\n,\naudio/webm\nParameter defaults\ntune\nStart of speech sensitivity: Low\nEnd of speech sensitivity: High\nPrefix padding: 0\nMax context size: 128K\nSupported regions\nModel availability\nUnited States\nus-central1\nSee\nData residency\nfor more information.\nKnowledge cutoff date\nJanuary 2025\nVersions\ngemini-live-2.5-flash-preview-native-audio-09-2025\nLaunch stage: Public preview\nRelease date: September 18, 2025\ngemini-live-2.5-flash-preview-native-audio\nLaunch stage: Public preview\nRelease date: June 17, 2025\nDiscontinuation date: October 18, 2025\nSecurity controls\nSee\nSecurity controls\nfor more information.\nSupported languages\nSee\nSupported languages\n.\nPricing\nSee\nPricing\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 4322,
      "line_count": 215
    },
    {
      "filename": "vertex-ai_generative-ai_docs_multimodal_control-generated-output.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nStructured output\nStay organized with collections\nSave and categorize content based on your preferences.\nYou can guarantee that a model's generated output always adheres to a specific\nschema so that you receive consistently formatted responses. For example, you\nmight have an established data schema that you use for other tasks. If you\nhave the model follow the same schema, you can directly extract data from the\nmodel's output without any post-processing.\nTo specify the structure of a model's output, define a\nresponse schema\n, which\nworks like a blueprint for model responses. When you submit a prompt and include\nthe\nresponse schema\n, the model's response always follows your defined\nschema.\nYou can control generated output when using the following models:\nGemini models:\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nGemini 2.0 Flash-Lite\nOpen models:\nDeepSeek R1-0528\nLlama 4 Maverick\nLlama 4 Scout\nLlama 3.3\nFor Open Models, follow this\nuser guide\n.\nExample use cases\nOne use case for applying a response schema is to ensure that a model's response\nproduces valid JSON and conforms to your schema. Generative model outputs can\nhave some degree of variability, so including a response schema ensures that you\nalways receive valid JSON. Consequently, your downstream tasks can reliably\nexpect valid JSON input from generated responses.\nAnother example is to constrain how a model can respond. For example, you can\nhave a model annotate text with user-defined labels, not with labels that the\nmodel produces. This constraint is useful when you expect a specific set of\nlabels such as\npositive\nor\nnegative\nand don't want to receive a mixture of\nother labels that the model might generate like\ngood\n,\npositive\n,\nnegative\n,\nor\nbad\n.\nConsiderations\nThe following considerations discuss potential limitations if you plan on using\na response schema:\nYou must use the API to define and use a response schema. There's no console\nsupport.\nThe size of your response schema counts towards the input token limit.\nOnly certain output formats are supported, such as\napplication/json\nor\ntext/x.enum\n. For more information, see the\nresponseMimeType\nparameter in\nthe\nGemini API reference\n.\nStructured output supports a subset of the\nVertex AI\nschema reference\n. For more information, see\nSupported schema\nfields\n.\nA complex schema can result in an\nInvalidArgument: 400\nerror. Complexity\nmight come from long property names, long array length limits, enums with\nmany values, objects with lots of optional properties, or a combination of\nthese factors.\nIf you get this error with a valid schema, make one or more of the following\nchanges to resolve the error:\nShorten property names or enum names.\nFlatten nested arrays.\nReduce the number of properties with constraints, such as numbers with\nminimum and maximum limits.\nReduce the number of properties with complex constraints, such as\nproperties with complex formats like\ndate-time\n.\nReduce the number of optional properties.\nReduce the number of valid values for enums.\nSupported schema fields\nStructured output supports the following fields from the\nVertex AI schema\n. If you use an unsupported\nfield, Vertex AI can still handle your request but ignores the field.\nanyOf\nenum\n: only\nstring\nenums are supported\nformat\nitems\nmaximum\nmaxItems\nminimum\nminItems\nnullable\nproperties\npropertyOrdering\n*\nrequired\n*\npropertyOrdering\nis specifically for structured output and not part of the Vertex AI schema. This field defines the order in which properties are generated. The listed properties must be unique and must be valid keys in the\nproperties\ndictionary.\nWhen you define a schema, the model doesn't strictly follow the order of properties that you define in the\nproperties\nfield. To enforce a specific order for property generation, use the\npropertyOrdering\nfield. Properties listed in\npropertyOrdering\nare generated first, in the specified order, followed by any other properties.\nIf you use the Python SDK, the default property ordering follows the order that is defined in your schema. For all other cases, properties are generated alphabetically with the required properties grouped first followed by optional properties.\nFor the\nformat\nfield, Vertex AI supports the following values:\ndate\n,\ndate-time\n,\nduration\n, and\ntime\n. The description and format of each value is\ndescribed in the\nOpenAPI Initiative Registry\nBefore you begin\nDefine a response schema to specify the structure of a model's output, the field\nnames, and the expected data type for each field. Use only the supported fields\nas listed in the\nConsiderations\nsection. All other fields are\nignored.\nInclude your response schema as part of the\nresponseSchema\nfield only. Don't\nduplicate the schema in your input prompt. If you do, the generated output might\nbe lower in quality.\nFor sample schemas, see the\nExample schemas and model responses\nsection.\nModel behavior and response schema\nWhen a model generates a response, it uses the field name and context from your\nprompt. As such, we recommend that you use a clear structure and unambiguous\nfield names so that your intent is clear.\nBy default, fields are optional, meaning the model can\npopulate\nthe fields or\nskip\nthem. You can set fields as required to force the model to provide a\nvalue. If there's insufficient context in the associated input prompt, the\nmodel generates responses mainly based on the data it was trained on.\nIf you aren't seeing the results you expect, add more context to your input\nprompts or revise your response schema. For example, review the model's\nresponse without structured output to see how the model responds. You can\nthen update your response schema that better fits the model's output.\nSend a prompt with a response schema\nBy default, all fields are optional, meaning a model might generate a response\nto a field. To force the model to always generate a response to a field, set the\nfield as required.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nHttpOptions\nresponse_schema\n=\n{\n\"type\"\n:\n\"ARRAY\"\n,\n\"items\"\n:\n{\n\"type\"\n:\n\"OBJECT\"\n,\n\"properties\"\n:\n{\n\"recipe_name\"\n:\n{\n\"type\"\n:\n\"STRING\"\n},\n\"ingredients\"\n:\n{\n\"type\"\n:\n\"ARRAY\"\n,\n\"items\"\n:\n{\n\"type\"\n:\n\"STRING\"\n}},\n},\n\"required\"\n:\n[\n\"recipe_name\"\n,\n\"ingredients\"\n],\n},\n}\nprompt\n=\n\"\"\"\nList a few popular cookie recipes.\n\"\"\"\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\nprompt\n,\nconfig\n=\n{\n\"response_mime_type\"\n:\n\"application/json\"\n,\n\"response_schema\"\n:\nresponse_schema\n,\n},\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example output:\n# [\n#     {\n#         \"ingredients\": [\n#             \"2 1/4 cups all-purpose flour\",\n#             \"1 teaspoon baking soda\",\n#             \"1 teaspoon salt\",\n#             \"1 cup (2 sticks) unsalted butter, softened\",\n#             \"3/4 cup granulated sugar\",\n#             \"3/4 cup packed brown sugar\",\n#             \"1 teaspoon vanilla extract\",\n#             \"2 large eggs\",\n#             \"2 cups chocolate chips\",\n#         ],\n#         \"recipe_name\": \"Chocolate Chip Cookies\",\n#     }\n# ]\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithRespSchema\nshows\nhow\nto\nuse\na\nresponse\nschema\nto\ngenerate\noutput\nin\na\nspecific\nformat\n.\nfunc\ngenerateWithRespSchema\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nResponseMIMEType\n:\n\"application/json\"\n,\n//\nSee\nthe\nOpenAPI\nspecification\nfor\nmore\ndetails\nand\nexamples\n:\n//\nhttps\n:\n//\nspec\n.\nopenapis\n.\norg\n/\noas\n/\nv3\n.0.3\n.\nhtml\n#schema-object\nResponseSchema\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"array\"\n,\nItems\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"object\"\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"recipe_name\"\n:\n{\nType\n:\n\"string\"\n},\n\"ingredients\"\n:\n{\nType\n:\n\"array\"\n,\nItems\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"string\"\n},\n},\n},\nRequired\n:\n[]\nstring\n{\n\"recipe_name\"\n,\n\"ingredients\"\n},\n},\n},\n}\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"List a few popular cookie recipes.\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\n[\n//\n{\n//\n\"ingredients\"\n:\n[\n//\n\"2 1/4 cups all-purpose flour\"\n,\n//\n\"1 teaspoon baking soda\"\n,\n//\n...\n//\n],\n//\n\"recipe_name\"\n:\n\"Chocolate Chip Cookies\"\n//\n},\n//\n{\n//\n...\n//\n},\n//\n...\n//\n]\nreturn\nnil\n}\nNode.js\nInstall\nnpm install @google/genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nconst\n{\nGoogleGenAI\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'global'\n;\nasync\nfunction\ngenerateResponseSchema\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nprompt\n=\n'List a few popular cookie recipes.'\n;\nconst\nresponseSchema\n=\n{\ntype\n:\n'ARRAY'\n,\nitems\n:\n{\ntype\n:\n'OBJECT'\n,\nproperties\n:\n{\nrecipeName\n:\n{\ntype\n:\n'STRING'\n},\ningredients\n:\n{\ntype\n:\n'ARRAY'\n,\nitems\n:\n{\ntype\n:\n'STRING'\n},\n},\n},\nrequired\n:\n[\n'recipeName'\n,\n'ingredients'\n],\n},\n};\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n'gemini-2.5-flash'\n,\ncontents\n:\nprompt\n,\nconfig\n:\n{\nresponseMimeType\n:\n'application/json'\n,\nresponseSchema\n:\nresponseSchema\n,\n},\n});\nconsole\n.\nlog\n(\nresponse\n.\ntext\n);\n//\nExample\noutput\n:\n//\n[\n//\n{\n//\n\"ingredients\"\n:\n[\n//\n\"2 1/4 cups all-purpose flour\"\n,\n//\n\"1 teaspoon baking soda\"\n,\n//\n\"1 teaspoon salt\"\n,\n//\n\"1 cup (2 sticks) unsalted butter, softened\"\n,\n//\n\"3/4 cup granulated sugar\"\n,\n//\n\"3/4 cup packed brown sugar\"\n,\n//\n\"1 teaspoon vanilla extract\"\n,\n//\n\"2 large eggs\"\n,\n//\n\"2 cups chocolate chips\"\n,\n//\n],\n//\n\"recipe_name\"\n:\n\"Chocolate Chip Cookies\"\n,\n//\n}\n//\n]\nreturn\nresponse\n.\ntext\n;\n}\nREST\nBefore using any of the request data,\nmake the following replacements:\nGENERATE_RESPONSE_METHOD\n: The type of response that you want the model to generate.\nChoose a method that generates how you want the model's response to be returned:\nstreamGenerateContent\n: The response is streamed as it's being generated to reduce the perception of latency to a human audience.\ngenerateContent\n: The response is returned after it's fully generated.\nLOCATION\n: The region to process the request.\nPROJECT_ID\n: Your\nproject ID\n.\nMODEL_ID\n: The model ID of the\nmultimodal model that you want to use.\nROLE\n:\nThe role in a conversation associated with the content. Specifying a role is required even in\nsingleturn use cases.\nAcceptable values include the following:\nUSER\n: Specifies content that's sent by you.\nTEXT\n:\nThe text instructions to include in the prompt.\nRESPONSE_MIME_TYPE\n: The format type of the\ngenerated candidate text. For a list of supported values, see the\nresponseMimeType\nparameter in the\nGemini API\n.\nRESPONSE_SCHEMA\n: Schema for the\nmodel to follow when generating responses. For more information, see the\nSchema\nreference.\nHTTP method and URL:\nPOST https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:\nGENERATE_RESPONSE_METHOD\nRequest JSON body:\n{\n\"contents\": {\n\"role\": \"\nROLE\n\",\n\"parts\": {\n\"text\": \"\nTEXT\n\"\n}\n},\n\"generation_config\": {\n\"responseMimeType\": \"\nRESPONSE_MIME_TYPE\n\",\n\"responseSchema\":\nRESPONSE_SCHEMA\n,\n}\n}\nTo send your request, choose one of these options:\ncurl\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:\nGENERATE_RESPONSE_METHOD\n\"\nPowerShell\nSave the request body in a file named\nrequest.json\n,\nand execute the following command:\n$cred = gcloud auth print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n-Method POST `\n-Headers $headers `\n-ContentType: \"application/json; charset=utf-8\" `\n-InFile request.json `\n-Uri \"https://\nLOCATION\n-aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:\nGENERATE_RESPONSE_METHOD\n\" | Select-Object -Expand Content\nYou should receive a JSON response similar to the following.\nResponse\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"text\": \"[{\\\"recipe_name\\\": \\\"Chocolate Chip Cookies\\\"}, {\\\"recipe_name\\\": \\\"Peanut Butter Cookies\\\"}, {\\\"recipe_name\\\": \\\"Oatmeal Raisin Cookies\\\"}, {\\\"recipe_name\\\": \\\"Sugar Cookies\\\"}, {\\\"recipe_name\\\": \\\"Snickerdoodles\\\"}]\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n\"safetyRatings\": [\n{\n\"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n\"probability\": \"NEGLIGIBLE\",\n\"probabilityScore\": 0.08021325,\n\"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n\"severityScore\": 0.0921962\n},\n{\n\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n\"probability\": \"NEGLIGIBLE\",\n\"probabilityScore\": 0.14730969,\n\"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n\"severityScore\": 0.08866235\n},\n{\n\"category\": \"HARM_CATEGORY_HARASSMENT\",\n\"probability\": \"NEGLIGIBLE\",\n\"probabilityScore\": 0.13432105,\n\"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n\"severityScore\": 0.07172113\n},\n{\n\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n\"probability\": \"NEGLIGIBLE\",\n\"probabilityScore\": 0.12787028,\n\"severity\": \"HARM_SEVERITY_NEGLIGIBLE\",\n\"severityScore\": 0.10017223\n}\n]\n}\n],\n\"usageMetadata\": {\n\"promptTokenCount\": 7,\n\"candidatesTokenCount\": 55,\n\"totalTokenCount\": 62\n}\n}\nExample curl command\nLOCATION\n=\n\"us-central1\"\nMODEL_ID\n=\n\"gemini-2.5-flash\"\nPROJECT_ID\n=\n\"test-project\"\nGENERATE_RESPONSE_METHOD\n=\n\"generateContent\"\ncat\n<<\nEOF\n>\nrequest.json\n{\n\"contents\"\n:\n{\n\"role\"\n:\n\"user\"\n,\n\"parts\"\n:\n{\n\"text\"\n:\n\"List a few popular cookie recipes.\"\n}\n}\n,\n\"generation_config\"\n:\n{\n\"maxOutputTokens\"\n:\n2048\n,\n\"responseMimeType\"\n:\n\"application/json\"\n,\n\"responseSchema\"\n:\n{\n\"type\"\n:\n\"array\"\n,\n\"items\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"recipe_name\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n}\n,\n}\n,\n\"required\"\n:\n[\n\"recipe_name\"\n]\n,\n}\n,\n}\n}\n}\nEOF\ncurl\n\\\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:\n${\nGENERATE_RESPONSE_METHOD\n}\n\\\n-d\n'@request.json'\nExample schemas for JSON output\nThe following sections demonstrate a variety of sample prompts and response\nschemas. A sample model response is also included after each code sample.\nForecast the weather for each day of the week in an array\nClassify a product with a well-defined enum\nForecast the weather for each day of the week\nThe following example outputs a\nforecast\nobject for each day\nof the week that includes an array of properties such as the expected\ntemperature and humidity level for the day. Some properties are set to nullable\nso the model can return a null value when it doesn't have enough context to\ngenerate a meaningful response. This strategy helps reduce hallucinations.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nGenerateContentConfig\n,\nHttpOptions\nresponse_schema\n=\n{\n\"type\"\n:\n\"OBJECT\"\n,\n\"properties\"\n:\n{\n\"forecast\"\n:\n{\n\"type\"\n:\n\"ARRAY\"\n,\n\"items\"\n:\n{\n\"type\"\n:\n\"OBJECT\"\n,\n\"properties\"\n:\n{\n\"Day\"\n:\n{\n\"type\"\n:\n\"STRING\"\n,\n\"nullable\"\n:\nTrue\n},\n\"Forecast\"\n:\n{\n\"type\"\n:\n\"STRING\"\n,\n\"nullable\"\n:\nTrue\n},\n\"Temperature\"\n:\n{\n\"type\"\n:\n\"INTEGER\"\n,\n\"nullable\"\n:\nTrue\n},\n\"Humidity\"\n:\n{\n\"type\"\n:\n\"STRING\"\n,\n\"nullable\"\n:\nTrue\n},\n\"Wind Speed\"\n:\n{\n\"type\"\n:\n\"INTEGER\"\n,\n\"nullable\"\n:\nTrue\n},\n},\n\"required\"\n:\n[\n\"Day\"\n,\n\"Temperature\"\n,\n\"Forecast\"\n,\n\"Wind Speed\"\n],\n},\n}\n},\n}\nprompt\n=\n\"\"\"\nThe week ahead brings a mix of weather conditions.\nSunday is expected to be sunny with a temperature of 77°F and a humidity level of 50%. Winds will be light at around 10 km/h.\nMonday will see partly cloudy skies with a slightly cooler temperature of 72°F and the winds will pick up slightly to around 15 km/h.\nTuesday brings rain showers, with temperatures dropping to 64°F and humidity rising to 70%.\nWednesday may see thunderstorms, with a temperature of 68°F.\nThursday will be cloudy with a temperature of 66°F and moderate humidity at 60%.\nFriday returns to partly cloudy conditions, with a temperature of 73°F and the Winds will be light at 12 km/h.\nFinally, Saturday rounds off the week with sunny skies, a temperature of 80°F, and a humidity level of 40%. Winds will be gentle at 8 km/h.\n\"\"\"\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\nprompt\n,\nconfig\n=\nGenerateContentConfig\n(\nresponse_mime_type\n=\n\"application/json\"\n,\nresponse_schema\n=\nresponse_schema\n,\n),\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example output:\n# {\"forecast\": [{\"Day\": \"Sunday\", \"Forecast\": \"sunny\", \"Temperature\": 77, \"Wind Speed\": 10, \"Humidity\": \"50%\"},\n#   {\"Day\": \"Monday\", \"Forecast\": \"partly cloudy\", \"Temperature\": 72, \"Wind Speed\": 15},\n#   {\"Day\": \"Tuesday\", \"Forecast\": \"rain showers\", \"Temperature\": 64, \"Wind Speed\": null, \"Humidity\": \"70%\"},\n#   {\"Day\": \"Wednesday\", \"Forecast\": \"thunderstorms\", \"Temperature\": 68, \"Wind Speed\": null},\n#   {\"Day\": \"Thursday\", \"Forecast\": \"cloudy\", \"Temperature\": 66, \"Wind Speed\": null, \"Humidity\": \"60%\"},\n#   {\"Day\": \"Friday\", \"Forecast\": \"partly cloudy\", \"Temperature\": 73, \"Wind Speed\": 12},\n#   {\"Day\": \"Saturday\", \"Forecast\": \"sunny\", \"Temperature\": 80, \"Wind Speed\": 8, \"Humidity\": \"40%\"}]}\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithNullables\nshows\nhow\nto\nuse\nthe\nresponse\nschema\nwith\nnullable\nvalues\n.\nfunc\ngenerateWithNullables\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\nprompt\n:=\n`\nThe\nweek\nahead\nbrings\na\nmix\nof\nweather\nconditions\n.\nSunday\nis\nexpected\nto\nbe\nsunny\nwith\na\ntemperature\nof\n77\n°\nF\nand\na\nhumidity\nlevel\nof\n50\n%.\nWinds\nwill\nbe\nlight\nat\naround\n10\nkm\n/\nh\n.\nMonday\nwill\nsee\npartly\ncloudy\nskies\nwith\na\nslightly\ncooler\ntemperature\nof\n72\n°\nF\nand\nthe\nwinds\nwill\npick\nup\nslightly\nto\naround\n15\nkm\n/\nh\n.\nTuesday\nbrings\nrain\nshowers\n,\nwith\ntemperatures\ndropping\nto\n64\n°\nF\nand\nhumidity\nrising\nto\n70\n%.\nWednesday\nmay\nsee\nthunderstorms\n,\nwith\na\ntemperature\nof\n68\n°\nF\n.\nThursday\nwill\nbe\ncloudy\nwith\na\ntemperature\nof\n66\n°\nF\nand\nmoderate\nhumidity\nat\n60\n%.\nFriday\nreturns\nto\npartly\ncloudy\nconditions\n,\nwith\na\ntemperature\nof\n73\n°\nF\nand\nthe\nWinds\nwill\nbe\nlight\nat\n12\nkm\n/\nh\n.\nFinally\n,\nSaturday\nrounds\noff\nthe\nweek\nwith\nsunny\nskies\n,\na\ntemperature\nof\n80\n°\nF\n,\nand\na\nhumidity\nlevel\nof\n40\n%.\nWinds\nwill\nbe\ngentle\nat\n8\nkm\n/\nh\n.\n`\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\nprompt\n},\n},\nRole\n:\n\"user\"\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nResponseMIMEType\n:\n\"application/json\"\n,\n//\nSee\nthe\nOpenAPI\nspecification\nfor\nmore\ndetails\nand\nexamples\n:\n//\nhttps\n:\n//\nspec\n.\nopenapis\n.\norg\n/\noas\n/\nv3\n.0.3\n.\nhtml\n#schema-object\nResponseSchema\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"object\"\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"forecast\"\n:\n{\nType\n:\n\"array\"\n,\nItems\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"object\"\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"Day\"\n:\n{\nType\n:\n\"string\"\n,\nNullable\n:\ngenai\n.\nPtr\n(\ntrue\n)},\n\"Forecast\"\n:\n{\nType\n:\n\"string\"\n,\nNullable\n:\ngenai\n.\nPtr\n(\ntrue\n)},\n\"Temperature\"\n:\n{\nType\n:\n\"integer\"\n,\nNullable\n:\ngenai\n.\nPtr\n(\ntrue\n)},\n\"Humidity\"\n:\n{\nType\n:\n\"string\"\n,\nNullable\n:\ngenai\n.\nPtr\n(\ntrue\n)},\n\"Wind Speed\"\n:\n{\nType\n:\n\"integer\"\n,\nNullable\n:\ngenai\n.\nPtr\n(\ntrue\n)},\n},\nRequired\n:\n[]\nstring\n{\n\"Day\"\n,\n\"Temperature\"\n,\n\"Forecast\"\n,\n\"Wind Speed\"\n},\n},\n},\n},\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\n{\n//\n\"forecast\"\n:\n[\n//\n{\n\"Day\"\n:\n\"Sunday\"\n,\n\"Forecast\"\n:\n\"Sunny\"\n,\n\"Temperature\"\n:\n77\n,\n\"Wind Speed\"\n:\n10\n,\n\"Humidity\"\n:\n\"50%\"\n},\n//\n{\n\"Day\"\n:\n\"Monday\"\n,\n\"Forecast\"\n:\n\"Partly Cloudy\"\n,\n\"Temperature\"\n:\n72\n,\n\"Wind Speed\"\n:\n15\n},\n//\n{\n\"Day\"\n:\n\"Tuesday\"\n,\n\"Forecast\"\n:\n\"Rain Showers\"\n,\n\"Temperature\"\n:\n64\n,\n\"Wind Speed\"\n:\nnull\n,\n\"Humidity\"\n:\n\"70%\"\n},\n//\n{\n\"Day\"\n:\n\"Wednesday\"\n,\n\"Forecast\"\n:\n\"Thunderstorms\"\n,\n\"Temperature\"\n:\n68\n,\n\"Wind Speed\"\n:\nnull\n},\n//\n{\n\"Day\"\n:\n\"Thursday\"\n,\n\"Forecast\"\n:\n\"Cloudy\"\n,\n\"Temperature\"\n:\n66\n,\n\"Wind Speed\"\n:\nnull\n,\n\"Humidity\"\n:\n\"60%\"\n},\n//\n{\n\"Day\"\n:\n\"Friday\"\n,\n\"Forecast\"\n:\n\"Partly Cloudy\"\n,\n\"Temperature\"\n:\n73\n,\n\"Wind Speed\"\n:\n12\n},\n//\n{\n\"Day\"\n:\n\"Saturday\"\n,\n\"Forecast\"\n:\n\"Sunny\"\n,\n\"Temperature\"\n:\n80\n,\n\"Wind Speed\"\n:\n8\n,\n\"Humidity\"\n:\n\"40%\"\n}\n//\n]\n//\n}\nreturn\nnil\n}\nNode.js\nInstall\nnpm install @google/genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nconst\n{\nGoogleGenAI\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'global'\n;\nasync\nfunction\ngenerateNullableSchema\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nprompt\n=\n`\nThe\nweek\nahead\nbrings\na\nmix\nof\nweather\nconditions\n.\nSunday\nis\nexpected\nto\nbe\nsunny\nwith\na\ntemperature\nof\n77\n°\nF\nand\na\nhumidity\nlevel\nof\n50\n%.\nWinds\nwill\nbe\nlight\nat\naround\n10\nkm\n/\nh\n.\nMonday\nwill\nsee\npartly\ncloudy\nskies\nwith\na\nslightly\ncooler\ntemperature\nof\n72\n°\nF\nand\nthe\nwinds\nwill\npick\nup\nslightly\nto\naround\n15\nkm\n/\nh\n.\nTuesday\nbrings\nrain\nshowers\n,\nwith\ntemperatures\ndropping\nto\n64\n°\nF\nand\nhumidity\nrising\nto\n70\n%.\nWednesday\nmay\nsee\nthunderstorms\n,\nwith\na\ntemperature\nof\n68\n°\nF\n.\nThursday\nwill\nbe\ncloudy\nwith\na\ntemperature\nof\n66\n°\nF\nand\nmoderate\nhumidity\nat\n60\n%.\nFriday\nreturns\nto\npartly\ncloudy\nconditions\n,\nwith\na\ntemperature\nof\n73\n°\nF\nand\nthe\nWinds\nwill\nbe\nlight\nat\n12\nkm\n/\nh\n.\nFinally\n,\nSaturday\nrounds\noff\nthe\nweek\nwith\nsunny\nskies\n,\na\ntemperature\nof\n80\n°\nF\n,\nand\na\nhumidity\nlevel\nof\n40\n%.\nWinds\nwill\nbe\ngentle\nat\n8\nkm\n/\nh\n.\n`\n;\nconst\nresponseSchema\n=\n{\ntype\n:\n'object'\n,\nproperties\n:\n{\nforecast\n:\n{\ntype\n:\n'array'\n,\nitems\n:\n{\ntype\n:\n'object'\n,\nproperties\n:\n{\nDay\n:\n{\ntype\n:\n'string'\n,\nnullable\n:\ntrue\n},\nForecast\n:\n{\ntype\n:\n'string'\n,\nnullable\n:\ntrue\n},\nTemperature\n:\n{\ntype\n:\n'integer'\n,\nnullable\n:\ntrue\n},\nHumidity\n:\n{\ntype\n:\n'string'\n,\nnullable\n:\ntrue\n},\nWindSpeed\n:\n{\ntype\n:\n'integer'\n,\nnullable\n:\ntrue\n},\n},\nrequired\n:\n[\n'Day'\n,\n'Temperature'\n,\n'Forecast'\n,\n'WindSpeed'\n],\n},\n},\n},\n};\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n'gemini-2.5-flash'\n,\ncontents\n:\nprompt\n,\nconfig\n:\n{\nresponseMimeType\n:\n'application/json'\n,\nresponseSchema\n:\nresponseSchema\n,\n},\n});\nconsole\n.\nlog\n(\nresponse\n.\ntext\n);\n//\nExample\noutput\n:\n//\n{\n\"forecast\"\n:\n[{\n\"Day\"\n:\n\"Sunday\"\n,\n\"Forecast\"\n:\n\"sunny\"\n,\n\"Temperature\"\n:\n77\n,\n\"Wind Speed\"\n:\n10\n,\n\"Humidity\"\n:\n\"50%\"\n},\n//\n{\n\"Day\"\n:\n\"Monday\"\n,\n\"Forecast\"\n:\n\"partly cloudy\"\n,\n\"Temperature\"\n:\n72\n,\n\"Wind Speed\"\n:\n15\n},\n//\n{\n\"Day\"\n:\n\"Tuesday\"\n,\n\"Forecast\"\n:\n\"rain showers\"\n,\n\"Temperature\"\n:\n64\n,\n\"Wind Speed\"\n:\nnull\n,\n\"Humidity\"\n:\n\"70%\"\n},\n//\n{\n\"Day\"\n:\n\"Wednesday\"\n,\n\"Forecast\"\n:\n\"thunderstorms\"\n,\n\"Temperature\"\n:\n68\n,\n\"Wind Speed\"\n:\nnull\n},\n//\n{\n\"Day\"\n:\n\"Thursday\"\n,\n\"Forecast\"\n:\n\"cloudy\"\n,\n\"Temperature\"\n:\n66\n,\n\"Wind Speed\"\n:\nnull\n,\n\"Humidity\"\n:\n\"60%\"\n},\n//\n{\n\"Day\"\n:\n\"Friday\"\n,\n\"Forecast\"\n:\n\"partly cloudy\"\n,\n\"Temperature\"\n:\n73\n,\n\"Wind Speed\"\n:\n12\n},\n//\n{\n\"Day\"\n:\n\"Saturday\"\n,\n\"Forecast\"\n:\n\"sunny\"\n,\n\"Temperature\"\n:\n80\n,\n\"Wind Speed\"\n:\n8\n,\n\"Humidity\"\n:\n\"40%\"\n}]}\nreturn\nresponse\n.\ntext\n;\n}\nClassify a product\nThe following example includes enums where the model must classify an\nobject's type and condition from a list of given values.\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nGenerateContentConfig\n,\nHttpOptions\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"What type of instrument is an oboe?\"\n,\nconfig\n=\nGenerateContentConfig\n(\nresponse_mime_type\n=\n\"text/x.enum\"\n,\nresponse_schema\n=\n{\n\"type\"\n:\n\"STRING\"\n,\n\"enum\"\n:\n[\n\"Percussion\"\n,\n\"String\"\n,\n\"Woodwind\"\n,\n\"Brass\"\n,\n\"Keyboard\"\n],\n},\n),\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example output:\n# Woodwind\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithEnumSchema\nshows\nhow\nto\nuse\nenum\nschema\nto\ngenerate\noutput\n.\nfunc\ngenerateWithEnumSchema\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What type of instrument is an oboe?\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nResponseMIMEType\n:\n\"text/x.enum\"\n,\nResponseSchema\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"STRING\"\n,\nEnum\n:\n[]\nstring\n{\n\"Percussion\"\n,\n\"String\"\n,\n\"Woodwind\"\n,\n\"Brass\"\n,\n\"Keyboard\"\n},\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nWoodwind\nreturn\nnil\n}\nNode.js\nInstall\nnpm install @google/genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nconst\n{\nGoogleGenAI\n,\nType\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'global'\n;\nasync\nfunction\ngenerateContent\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nresponseSchema\n=\n{\ntype\n:\nType\n.\nSTRING\n,\nenum\n:\n[\n'Percussion'\n,\n'String'\n,\n'Woodwind'\n,\n'Brass'\n,\n'Keyboard'\n],\n};\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n'gemini-2.5-flash'\n,\ncontents\n:\n'What type of instrument is an oboe?'\n,\nconfig\n:\n{\nresponseMimeType\n:\n'text/x.enum'\n,\nresponseSchema\n:\nresponseSchema\n,\n},\n});\nconsole\n.\nlog\n(\nresponse\n.\ntext\n);\n//\nExample\noutput\n:\n//\nWoodwind\nreturn\nresponse\n.\ntext\n;\n}\nJava\nLearn how to install or update the\nJava\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.GenerateContentConfig\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.HttpOptions\n;\nimport\ncom.google.genai.types.Schema\n;\nimport\ncom.google.genai.types.Type\n;\nimport\njava.util.List\n;\npublic\nclass\nControlledGenerationWithEnumSchema\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\ncontents\n=\n\"What type of instrument is an oboe?\"\n;\nString\nmodelId\n=\n\"gemini-2.5-flash\"\n;\ngenerateContent\n(\nmodelId\n,\ncontents\n);\n}\n//\nGenerates\ncontent\nwith\nan\nenum\nresponse\nschema\npublic\nstatic\nString\ngenerateContent\n(\nString\nmodelId\n,\nString\ncontents\n)\n{\n//\nInitialize\nclient\nthat\nwill\nbe\nused\nto\nsend\nrequests\n.\nThis\nclient\nonly\nneeds\nto\nbe\ncreated\n//\nonce\n,\nand\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nhttpOptions\n(\nHttpOptions\n.\nbuilder\n()\n.\napiVersion\n(\n\"v1\"\n)\n.\nbuild\n())\n.\nbuild\n())\n{\n//\nDefine\nthe\nresponse\nschema\nwith\nan\nenum\n.\nSchema\nresponseSchema\n=\nSchema\n.\nbuilder\n()\n.\ntype\n(\nType\n.\nKnown\n.\nSTRING\n)\n.\nenum_\n(\nList\n.\nof\n(\n\"Percussion\"\n,\n\"String\"\n,\n\"Woodwind\"\n,\n\"Brass\"\n,\n\"Keyboard\"\n))\n.\nbuild\n();\nGenerateContentConfig\nconfig\n=\nGenerateContentConfig\n.\nbuilder\n()\n.\nresponseMimeType\n(\n\"text/x.enum\"\n)\n.\nresponseSchema\n(\nresponseSchema\n)\n.\nbuild\n();\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\ncontents\n,\nconfig\n);\nSystem\n.\nout\n.\nprint\n(\nresponse\n.\ntext\n());\n//\nExample\nresponse\n:\n//\nWoodwind\nreturn\nresponse\n.\ntext\n();\n}\n}\n}\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 32955,
      "line_count": 3448
    },
    {
      "filename": "vertex-ai_generative-ai_docs_multimodal_function-calling.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nIntroduction to function calling\nStay organized with collections\nSave and categorize content based on your preferences.\nFunction calling\n, also known as\ntool use\n, provides the LLM with definitions of external tools (for example, a\nget_current_weather\nfunction). When processing a prompt, the model intelligently determines if a tool is needed and, if so, outputs structured data specifying the tool to call and its parameters (for example,\nget_current_weather(location='Boston')\n). Your application then executes this tool, feeds the result back to the model, allowing it to complete its response with dynamic, real-world information or the outcome of an action. This effectively bridges the LLM with your systems and extends its capabilities.\nFunction calling enables two primary use cases:\nFetching data\n: Retrieve up-to-date information for model responses, such as current weather, currency conversion, or specific data from knowledge bases and APIs (RAG).\nTaking action\n: Perform external operations like submitting forms, updating application state, or orchestrating agentic workflows (e.g., conversation handoffs).\nFor more use cases and examples that are powered by function calling, see\nUse cases\n.\nFeatures and limitations\nThe following models support function calling:\nGemini models:\nGemini 2.5 Flash\n(Preview)\nGemini 2.5 Flash-Lite\n(Preview)\nGemini 2.5 Flash-Lite\nGemini 2.5 Flash with Live API native audio\n(Preview)\nGemini 2.0 Flash with Live API\n(Preview)\nGemini 2.5 Pro\nGemini 2.5 Flash\nGemini 2.0 Flash\nGemini 2.0 Flash-Lite\nOpen models:\nDeepSeek R1-0528\nLlama 4 Maverick\nLlama 4 Scout\nLlama 3.3\nYou can specify up to 512\nFunctionDeclarations\nDefine your functions in the\nOpenAPI schema\nformat.\nFor best practices related to the function declarations, including tips for names and descriptions, see\nBest practices\n.\nFor Open Models, follow this\nuser guide\n.\nHow to create a function calling application\nTo use function calling, perform the following tasks:\nSubmit function declarations and prompt to the model\n.\nProvide the API output to the model\n.\nStep 1: Submit the prompt and function declarations to the model\nDeclare a\nTool\nin a schema format that's compatible with the\nOpenAPI schema\n. For more information, see\nSchema examples\n.\nThe following examples submit a prompt and function declaration to the Gemini models.\nREST\nPROJECT_ID\n=\nmyproject\nLOCATION\n=\nus-central1\nMODEL_ID\n=\ngemini-2.0-flash-001\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [{\n\"role\": \"user\",\n\"parts\": [{\n\"text\": \"What is the weather in Boston?\"\n}]\n}],\n\"tools\": [{\n\"functionDeclarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a given location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city name of the location for which to get the weather.\",\n\"default\": {\n\"string_value\": \"Boston, MA\"\n}\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}]\n}'\nPython\nYou can specify the schema either manually using a Python dictionary or automatically with the\nfrom_func\nhelper function. The following example demonstrates how to declare a function manually.\nimport\nvertexai\nfrom\nvertexai.generative_models\nimport\n(\nContent\n,\nFunctionDeclaration\n,\nGenerationConfig\n,\nGenerativeModel\n,\nPart\n,\nTool\n,\nToolConfig\n)\n# Initialize Vertex AI\n# TODO(developer): Update the project\nvertexai\n.\ninit\n(\nproject\n=\n\"PROJECT_ID\"\n,\nlocation\n=\n\"us-central1\"\n)\n# Initialize Gemini model\nmodel\n=\nGenerativeModel\n(\nmodel_name\n=\n\"gemini-2.0-flash\"\n)\n# Manual function declaration\nget_current_weather_func\n=\nFunctionDeclaration\n(\nname\n=\n\"get_current_weather\"\n,\ndescription\n=\n\"Get the current weather in a given location\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city name of the location for which to get the weather.\"\n,\n\"default\"\n:\n{\n\"string_value\"\n:\n\"Boston, MA\"\n}\n}\n},\n},\n)\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston?\"\n),\n],\n)\n],\ngeneration_config\n=\nGenerationConfig\n(\ntemperature\n=\n0\n),\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n]\n)\nAlternatively, you can declare the function automatically with the\nfrom_func\nhelper function as shown in the following example:\ndef\nget_current_weather\n(\nlocation\n:\nstr\n=\n\"Boston, MA\"\n):\n\"\"\"\nGet the current weather in a given location\nArgs:\nlocation: The city name of the location for which to get the weather.\n\"\"\"\n# This example uses a mock implementation.\n# You can define a local function or import the requests library to call an API\nreturn\n{\n\"location\"\n:\n\"Boston, MA\"\n,\n\"temperature\"\n:\n38\n,\n\"description\"\n:\n\"Partly Cloudy\"\n,\n\"icon\"\n:\n\"partly-cloudy\"\n,\n\"humidity\"\n:\n65\n,\n\"wind\"\n:\n{\n\"speed\"\n:\n10\n,\n\"direction\"\n:\n\"NW\"\n}\n}\nget_current_weather_func\n=\nFunctionDeclaration\n.\nfrom_func\n(\nget_current_weather\n)\nNode.js\nThis example demonstrates a text scenario with one function and one\nprompt.\nNode.js\nBefore trying this sample, follow the\nNode.js\nsetup instructions in the\nVertex AI quickstart using\nclient libraries\n.\nFor more information, see the\nVertex AI\nNode.js\nAPI\nreference documentation\n.\nTo authenticate to Vertex AI, set up Application Default Credentials.\nFor more information, see\nSet up authentication for a local development environment\n.\nconst\n{\nVertexAI\n,\nFunctionDeclarationSchemaType\n,\n}\n=\nrequire\n(\n'\n@google-cloud/vertexai\n'\n);\nconst\nfunctionDeclarations\n=\n[\n{\nfunction_declarations\n:\n[\n{\nname\n:\n'get_current_weather'\n,\ndescription\n:\n'get weather in a given location'\n,\nparameters\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nOBJECT\n,\nproperties\n:\n{\nlocation\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n},\nunit\n:\n{\ntype\n:\nFunctionDeclarationSchemaType\n.\nSTRING\n,\nenum\n:\n[\n'celsius'\n,\n'fahrenheit'\n],\n},\n},\nrequired\n:\n[\n'location'\n],\n},\n},\n],\n},\n];\nconst\nfunctionResponseParts\n=\n[\n{\nfunctionResponse\n:\n{\nname\n:\n'get_current_weather'\n,\nresponse\n:\n{\nname\n:\n'get_current_weather'\n,\ncontent\n:\n{\nweather\n:\n'super nice'\n}},\n},\n},\n];\n/**\n* TODO(developer): Update these variables before running the sample.\n*/\nasync\nfunction\nfunctionCallingStreamContent\n(\nprojectId\n=\n'PROJECT_ID'\n,\nlocation\n=\n'us-central1'\n,\nmodel\n=\n'gemini-2.0-flash-001'\n)\n{\n// Initialize Vertex with your Cloud project and location\nconst\nvertexAI\n=\nnew\nVertexAI\n({\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n});\n// Instantiate the model\nconst\ngenerativeModel\n=\nvertexAI\n.\ngetGenerativeModel\n({\nmodel\n:\nmodel\n,\n});\nconst\nrequest\n=\n{\ncontents\n:\n[\n{\nrole\n:\n'user'\n,\nparts\n:\n[{\ntext\n:\n'What is the weather in Boston?'\n}]},\n{\nrole\n:\n'ASSISTANT'\n,\nparts\n:\n[\n{\nfunctionCall\n:\n{\nname\n:\n'get_current_weather'\n,\nargs\n:\n{\nlocation\n:\n'Boston'\n},\n},\n},\n],\n},\n{\nrole\n:\n'USER'\n,\nparts\n:\nfunctionResponseParts\n},\n],\ntools\n:\nfunctionDeclarations\n,\n};\nconst\nstreamingResp\n=\nawait\ngenerativeModel\n.\ngenerateContentStream\n(\nrequest\n);\nfor\nawait\n(\nconst\nitem\nof\nstream\ningResp\n.\nstream\n)\n{\nconsole\n.\nlog\n(\nitem\n.\ncandidates\n[\n0\n].\ncontent\n.\nparts\n[\n0\n].\ntext\n);\n}\n}\nGo\nThis example demonstrates a text scenario with one function and one prompt.\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithFuncCall\nshows\nhow\nto\nsubmit\na\nprompt\nand\na\nfunction\ndeclaration\nto\nthe\nmodel\n,\n//\nallowing\nit\nto\nsuggest\na\ncall\nto\nthe\nfunction\nto\nfetch\nexternal\ndata\n.\nReturning\nthis\ndata\n//\nenables\nthe\nmodel\nto\ngenerate\na\ntext\nresponse\nthat\nincorporates\nthe\ndata\n.\nfunc\ngenerateWithFuncCall\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nweatherFunc\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nDescription\n:\n\"Returns the current weather in a location.\"\n,\nName\n:\n\"getCurrentWeather\"\n,\nParameters\n:\n&\ngenai\n.\nSchema\n{\nType\n:\n\"object\"\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"location\"\n:\n{\nType\n:\n\"string\"\n},\n},\nRequired\n:\n[]\nstring\n{\n\"location\"\n},\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nTools\n:\n[]\n*\ngenai\n.\nTool\n{\n{\nFunctionDeclarations\n:\n[]\n*\ngenai\n.\nFunctionDeclaration\n{\nweatherFunc\n}},\n},\nTemperature\n:\ngenai\n.\nPtr\n(\nfloat32\n(\n0.0\n)),\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is the weather like in Boston?\"\n},\n},\nRole\n:\n\"user\"\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nvar\nfuncCall\n*\ngenai\n.\nFunctionCall\nfor\n_\n,\np\n:=\nrange\nresp\n.\nCandidates\n[\n0\n]\n.\nContent\n.\nParts\n{\nif\np\n.\nFunctionCall\n!=\nnil\n{\nfuncCall\n=\np\n.\nFunctionCall\nfmt\n.\nFprint\n(\nw\n,\n\"The model suggests to call the function \"\n)\nfmt\n.\nFprintf\n(\nw\n,\n\"%q with args: %v\n\\n\n\"\n,\nfuncCall\n.\nName\n,\nfuncCall\n.\nArgs\n)\n//\nExample\nresponse\n:\n//\nThe\nmodel\nsuggests\nto\ncall\nthe\nfunction\n\"getCurrentWeather\"\nwith\nargs\n:\nmap\n[\nlocation\n:\nBoston\n]\n}\n}\nif\nfuncCall\n==\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"model did not suggest a function call\"\n)\n}\n//\nUse\nsynthetic\ndata\nto\nsimulate\na\nresponse\nfrom\nthe\nexternal\nAPI\n.\n//\nIn\na\nreal\napplication\n,\nthis\nwould\ncome\nfrom\nan\nactual\nweather\nAPI\n.\nfuncResp\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\n\"getCurrentWeather\"\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"location\"\n:\n\"Boston\"\n,\n\"temperature\"\n:\n\"38\"\n,\n\"temperature_unit\"\n:\n\"F\"\n,\n\"description\"\n:\n\"Cold and cloudy\"\n,\n\"humidity\"\n:\n\"65\"\n,\n\"wind\"\n:\n`\n{\n\"speed\"\n:\n\"10\"\n,\n\"direction\"\n:\n\"NW\"\n}\n`\n,\n},\n}\n//\nReturn\nconversation\nturns\nand\nAPI\nresponse\nto\ncomplete\nthe\nmodel\n's response.\ncontents\n=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is the weather like in Boston?\"\n},\n},\nRole\n:\n\"user\"\n},\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nFunctionCall\n:\nfuncCall\n},\n}},\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nFunctionResponse\n:\nfuncResp\n},\n}},\n}\nresp\n,\nerr\n=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThe\nweather\nin\nBoston\nis\ncold\nand\ncloudy\nwith\na\ntemperature\nof\n38\ndegrees\nFahrenheit\n.\nThe\nhumidity\nis\n...\nreturn\nnil\n}\nC#\nThis example demonstrates a text scenario with one function and one prompt.\nC#\nBefore trying this sample, follow the\nC#\nsetup instructions in the\nVertex AI quickstart using\nclient libraries\n.\nFor more information, see the\nVertex AI\nC#\nAPI\nreference documentation\n.\nTo authenticate to Vertex AI, set up Application Default Credentials.\nFor more information, see\nSet up authentication for a local development environment\n.\nusing\nGoogle.Cloud.AIPlatform.V1\n;\nusing\nSystem\n;\nusing\nSystem.Threading.Tasks\n;\nusing\nType\n=\nGoogle\n.\nCloud\n.\nAIPlatform\n.\nV1\n.\nType\n;\nusing\nValue\n=\nGoogle\n.\nProtobuf\n.\nWellKnownTypes\n.\nValue\n;\npublic\nclass\nFunctionCalling\n{\npublic\nasync\nTask<string>\nGenerateFunctionCall\n(\nstring\nprojectId\n=\n\"your-project-id\"\n,\nstring\nlocation\n=\n\"us-central1\"\n,\nstring\npublisher\n=\n\"google\"\n,\nstring\nmodel\n=\n\"gemini-2.0-flash-001\"\n)\n{\nvar\npredictionServiceClient\n=\nnew\nPredictionServiceClientBuilder\n{\nEndpoint\n=\n$\"{location}-aiplatform.googleapis.com\"\n}.\nBuild\n();\n// Define the user's prompt in a Content object that we can reuse in\n// model calls\nvar\nuserPromptContent\n=\nnew\nContent\n{\nRole\n=\n\"USER\"\n,\nParts\n=\n{\nnew\nPart\n{\nText\n=\n\"What is the weather like in Boston?\"\n}\n}\n};\n// Specify a function declaration and parameters for an API request\nvar\nfunctionName\n=\n\"get_current_weather\"\n;\nvar\ngetCurrentWeatherFunc\n=\nnew\nFunctionDeclaration\n{\nName\n=\nfunctionName\n,\nDescription\n=\n\"Get the current weather in a given location\"\n,\nParameters\n=\nnew\nOpenApiSchema\n{\nType\n=\nType\n.\nObject\n,\nProperties\n=\n{\n[\"location\"]\n=\nnew\n()\n{\nType\n=\nType\n.\nString\n,\nDescription\n=\n\"Get the current weather in a given location\"\n},\n[\"unit\"]\n=\nnew\n()\n{\nType\n=\nType\n.\nString\n,\nDescription\n=\n\"The unit of measurement for the temperature\"\n,\nEnum\n=\n{\n\"celsius\"\n,\n\"fahrenheit\"\n}\n}\n},\nRequired\n=\n{\n\"location\"\n}\n}\n};\n// Send the prompt and instruct the model to generate content using the tool that you just created\nvar\ngenerateContentRequest\n=\nnew\nGenerateContentRequest\n{\nModel\n=\n$\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\"\n,\nGenerationConfig\n=\nnew\nGenerationConfig\n{\nTemperature\n=\n0f\n},\nContents\n=\n{\nuserPromptContent\n},\nTools\n=\n{\nnew\nTool\n{\nFunctionDeclarations\n=\n{\ngetCurrentWeatherFunc\n}\n}\n}\n};\nGenerateContentResponse\nresponse\n=\nawait\npredictionServiceClient\n.\nGenerateContentAsync\n(\ngenerateContentRequest\n);\nvar\nfunctionCall\n=\nresponse\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n].\nFunctionCall\n;\nConsole\n.\nWriteLine\n(\nfunctionCall\n);\nstring\napiResponse\n=\n\"\"\n;\n// Check the function name that the model responded with, and make an API call to an external system\nif\n(\nfunctionCall\n.\nName\n==\nfunctionName\n)\n{\n// Extract the arguments to use in your API call\nstring\nlocationCity\n=\nfunctionCall\n.\nArgs\n.\nFields\n[\n\"location\"\n].\nStringValue\n;\n// Here you can use your preferred method to make an API request to\n// fetch the current weather\n// In this example, we'll use synthetic data to simulate a response\n// payload from an external API\napiResponse\n=\n@\"{ \"\"location\"\": \"\"Boston, MA\"\",\n\"\"temperature\"\": 38, \"\"description\"\": \"\"Partly Cloudy\"\"}\"\n;\n}\n// Return the API response to Gemini so it can generate a model response or request another function call\ngenerateContentRequest\n=\nnew\nGenerateContentRequest\n{\nModel\n=\n$\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\"\n,\nContents\n=\n{\nuserPromptContent\n,\n// User prompt\nresponse\n.\nCandidates\n[\n0\n].\nContent\n,\n// Function call response,\nnew\nContent\n{\nParts\n=\n{\nnew\nPart\n{\nFunctionResponse\n=\nnew\n()\n{\nName\n=\nfunctionName\n,\nResponse\n=\nnew\n()\n{\nFields\n=\n{\n{\n\"content\"\n,\nnew\nValue\n{\nStringValue\n=\napiResponse\n}\n}\n}\n}\n}\n}\n}\n}\n},\nTools\n=\n{\nnew\nTool\n{\nFunctionDeclarations\n=\n{\ngetCurrentWeatherFunc\n}\n}\n}\n};\nresponse\n=\nawait\npredictionServiceClient\n.\nGenerateContentAsync\n(\ngenerateContentRequest\n);\nstring\nresponseText\n=\nresponse\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n].\nText\n;\nConsole\n.\nWriteLine\n(\nresponseText\n);\nreturn\nresponseText\n;\n}\n}\nJava\nJava\nBefore trying this sample, follow the\nJava\nsetup instructions in the\nVertex AI quickstart using\nclient libraries\n.\nFor more information, see the\nVertex AI\nJava\nAPI\nreference documentation\n.\nTo authenticate to Vertex AI, set up Application Default Credentials.\nFor more information, see\nSet up authentication for a local development environment\n.\nimport\ncom.google.cloud.vertexai.\nVertexAI\n;\nimport\ncom.google.cloud.vertexai.api.\nContent\n;\nimport\ncom.google.cloud.vertexai.api.\nFunctionDeclaration\n;\nimport\ncom.google.cloud.vertexai.api.\nGenerateContentResponse\n;\nimport\ncom.google.cloud.vertexai.api.\nSchema\n;\nimport\ncom.google.cloud.vertexai.api.\nTool\n;\nimport\ncom.google.cloud.vertexai.api.\nType\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nChatSession\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nContentMaker\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nGenerativeModel\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nPartMaker\n;\nimport\ncom.google.cloud.vertexai.generativeai.\nResponseHandler\n;\nimport\njava.io.IOException\n;\nimport\njava.util.Arrays\n;\nimport\njava.util.Collections\n;\npublic\nclass\nFunctionCalling\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\nthrows\nIOException\n{\n// TODO(developer): Replace these variables before running the sample.\nString\nprojectId\n=\n\"your-google-cloud-project-id\"\n;\nString\nlocation\n=\n\"us-central1\"\n;\nString\nmodelName\n=\n\"gemini-2.0-flash-001\"\n;\nString\npromptText\n=\n\"What's the weather like in Paris?\"\n;\nwhatsTheWeatherLike\n(\nprojectId\n,\nlocation\n,\nmodelName\n,\npromptText\n);\n}\n// A request involving the interaction with an external tool\npublic\nstatic\nString\nwhatsTheWeatherLike\n(\nString\nprojectId\n,\nString\nlocation\n,\nString\nmodelName\n,\nString\npromptText\n)\nthrows\nIOException\n{\n// Initialize client that will be used to send requests.\n// This client only needs to be created once, and can be reused for multiple requests.\ntry\n(\nVertexAI\nvertexAI\n=\nnew\nVertexAI\n(\nprojectId\n,\nlocation\n))\n{\nFunctionDeclaration\nfunctionDeclaration\n=\nFunctionDeclaration\n.\nnewBuilder\n()\n.\nsetName\n(\n\"getCurrentWeather\"\n)\n.\nsetDescription\n(\n\"Get the current weather in a given location\"\n)\n.\nsetParameters\n(\nSchema\n.\nnewBuilder\n()\n.\nsetType\n(\nType\n.\nOBJECT\n)\n.\nputProperties\n(\n\"location\"\n,\nSchema\n.\nnewBuilder\n()\n.\nsetType\n(\nType\n.\nSTRING\n)\n.\nsetDescription\n(\n\"location\"\n)\n.\nbuild\n()\n)\n.\naddRequired\n(\n\"location\"\n)\n.\nbuild\n()\n)\n.\nbuild\n();\nSystem\n.\nout\n.\nprintln\n(\n\"Function declaration:\"\n);\nSystem\n.\nout\n.\nprintln\n(\nfunctionDeclaration\n);\n// Add the function to a \"tool\"\nTool\ntool\n=\nTool\n.\nnewBuilder\n()\n.\naddFunctionDeclarations\n(\nfunctionDeclaration\n)\n.\nbuild\n();\n// Start a chat session from a model, with the use of the declared function.\nGenerativeModel\nmodel\n=\nnew\nGenerativeModel\n(\nmodelName\n,\nvertexAI\n)\n.\nwithTools\n(\nArrays\n.\nasList\n(\ntool\n));\nChatSession\nchat\n=\nmodel\n.\nstartChat\n();\nSystem\n.\nout\n.\nprintln\n(\nString\n.\nformat\n(\n\"Ask the question: %s\"\n,\npromptText\n));\nGenerateContentResponse\nresponse\n=\nchat\n.\nsendMessage\n(\npromptText\n);\n// The model will most likely return a function call to the declared\n// function `getCurrentWeather` with \"Paris\" as the value for the\n// argument `location`.\nSystem\n.\nout\n.\nprintln\n(\n\"\\nPrint response: \"\n);\nSystem\n.\nout\n.\nprintln\n(\nResponseHandler\n.\ngetContent\n(\nresponse\n));\n// Provide an answer to the model so that it knows what the result\n// of a \"function call\" is.\nContent\ncontent\n=\nContentMaker\n.\nfromMultiModalData\n(\nPartMaker\n.\nfromFunctionResponse\n(\n\"getCurrentWeather\"\n,\nCollections\n.\nsingletonMap\n(\n\"currentWeather\"\n,\n\"sunny\"\n)));\nSystem\n.\nout\n.\nprintln\n(\n\"Provide the function response: \"\n);\nSystem\n.\nout\n.\nprintln\n(\ncontent\n);\nresponse\n=\nchat\n.\nsendMessage\n(\ncontent\n);\n// See what the model replies now\nSystem\n.\nout\n.\nprintln\n(\n\"Print response: \"\n);\nString\nfinalAnswer\n=\nResponseHandler\n.\ngetText\n(\nresponse\n);\nSystem\n.\nout\n.\nprintln\n(\nfinalAnswer\n);\nreturn\nfinalAnswer\n;\n}\n}\n}\nIf the model determines that it needs the output of a particular function, the\nresponse that the application receives from the model contains the function name\nand the parameter values that the function should be called with.\nThe following is an example of a model response to the user prompt \"What is the weather like in Boston?\". The model proposes calling\nthe\nget_current_weather\nfunction with the parameter\nBoston, MA\n.\ncandidates {\ncontent {\nrole: \"model\"\nparts {\nfunction_call {\nname: \"get_current_weather\"\nargs {\nfields {\nkey: \"location\"\nvalue {\nstring_value: \"Boston, MA\"\n}\n}\n}\n}\n}\n}\n...\n}\nStep 2: Provide the API output to the model\nInvoke the external API and pass the API output back to the model.\nThe following example uses synthetic data to simulate a response payload from an\nexternal API and submits the output back to the model.\nREST\nPROJECT_ID\n=\nmyproject\nMODEL_ID\n=\ngemini-2.0-flash\nLOCATION\n=\n\"us-central1\"\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\nhttps://\n${\nLOCATION\n}\n-aiplatform.googleapis.com/v1/projects/\n${\nPROJECT_ID\n}\n/locations/\n${\nLOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:generateContent\n\\\n-d\n'{\n\"contents\": [\n{\n\"role\": \"user\",\n\"parts\": {\n\"text\": \"What is the weather in Boston?\"\n}\n},\n{\n\"role\": \"model\",\n\"parts\": [\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"Boston, MA\"\n}\n}\n}\n]\n},\n{\n\"role\": \"user\",\n\"parts\": [\n{\n\"functionResponse\": {\n\"name\": \"get_current_weather\",\n\"response\": {\n\"temperature\": 20,\n\"unit\": \"C\"\n}\n}\n}\n]\n}\n],\n\"tools\": [\n{\n\"function_declarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a specific location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city name of the location for which to get the weather.\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}\n]\n}'\nPython\nfunction_response_contents\n=\n[]\nfunction_response_parts\n=\n[]\n# Iterates through the function calls in the response in case there are parallel function call requests\nfor\nfunction_call\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\nfunction_calls\n:\nprint\n(\nf\n\"Function call:\n{\nfunction_call\n.\nname\n}\n\"\n)\n# In this example, we'll use synthetic data to simulate a response payload from an external API\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"Boston, MA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"Boston, MA\"\n,\n\"temperature\"\n:\n38\n,\n\"description\"\n:\n\"Partly Cloudy\"\n}\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"San Francisco, CA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"San Francisco, CA\"\n,\n\"temperature\"\n:\n58\n,\n\"description\"\n:\n\"Sunny\"\n}\nfunction_response_parts\n.\nappend\n(\nPart\n.\nfrom_function_response\n(\nname\n=\nfunction_call\n.\nname\n,\nresponse\n=\n{\n\"contents\"\n:\napi_response\n}\n)\n)\n# Add the function call response to the contents\nfunction_response_contents\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\nfunction_response_parts\n)\n# Submit the User's prompt, model's response, and API output back to the model\nresponse\n=\nmodel\n.\ngenerate_content\n(\n[\nContent\n(\n# User prompt\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston?\"\n),\n],\n),\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n,\n# Function call response\nfunction_response_contents\n# API output\n],\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n],\n)\n# Get the model summary response\nprint\n(\nresponse\n.\ntext\n)\nFor best practices related to API invocation, see\nBest practices - API invocation\n.\nIf the model had proposed several parallel function calls, the application must\nprovide all of the responses back to the model. To learn more, see\nParallel function calling example\n.\nThe model may determine that the\noutput of another function is necessary for responding to the prompt. In this case,\nthe response that the application receives from the model contains another\nfunction name and another set of parameter values.\nIf the model determines that the API response is sufficient for responding to\nthe user's prompt, it creates a natural language response and returns it to the\napplication. In this case, the application must pass the response back to the\nuser. The following is an example of a natural language response:\nIt is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies.\nFunction calling with thoughts\nWhen calling functions with\nthinking\nenabled, you'll\nneed to get the\nthought_signature\nfrom\nthe model response object and return it when you send the result of the function\nexecution back to the model. For example:\nPython\n# Call the model with function declarations\n# ...Generation config, Configure the client, and Define user prompt (No changes)\n# Send request with declarations (using a thinking model)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\nconfig\n=\nconfig\n,\ncontents\n=\ncontents\n)\n# See thought signatures\nfor\npart\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n.\nparts\n:\nif\nnot\npart\n.\ntext\n:\ncontinue\nif\npart\n.\nthought\nand\npart\n.\nthought_signature\n:\nprint\n(\n\"Thought signature:\"\n)\nprint\n(\npart\n.\nthought_signature\n)\nViewing thought signatures isn't required, but you will need to adjust\nStep\n2\nto return them along with the result of the function execution\nso it can incorporate the thoughts into its final response:\nPython\n# Create user friendly response with function result and call the model again\n# ...Create a function response part (No change)\n# Append thought signatures, function call and result of the function execution to contents\nfunction_call_content\n=\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n# Append the model's function call message, which includes thought signatures\ncontents\n.\nappend\n(\nfunction_call_content\n)\ncontents\n.\nappend\n(\ntypes\n.\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nfunction_response_part\n]))\n# Append the function response\nfinal_response\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\nconfig\n=\nconfig\n,\ncontents\n=\ncontents\n,\n)\nprint\n(\nfinal_response\n.\ntext\n)\nWhen returning thought signatures, follow these guidelines:\nThe model returns signatures within other parts in the response,\nfor example function calling or text, text, or thought summaries parts.\nReturn the entire response with all parts back to the model in\nsubsequent turns.\nDon't merge part with one signature with another part which also contains a\nsignature. Signatures can't be concatenated together.\nDon't merge one part with a signature with another part without a signature.\nThis breaks the correct positioning of the thought represented by the\nsignature.\nLearn more about limitations and usage of thought signatures, and about thinking\nmodels in general, on the\nThinking\npage.\nParallel function calling\nFor prompts such as \"Get weather details in Boston and San Francisco?\",\nthe model may propose several parallel function calls. For a list of models that\nsupport parallel function calling, see\nSupported models\n.\nREST\nThis example demonstrates a scenario with one\nget_current_weather\nfunction.\nThe user prompt is \"Get weather details in Boston and San Francisco?\". The\nmodel proposes two parallel\nget_current_weather\nfunction calls: one with the\nparameter\nBoston\nand the other with the parameter\nSan Francisco\n.\nTo learn more about the request parameters, see\nGemini API\n.\n{\n\"candidates\": [\n{\n\"content\": {\n\"role\": \"model\",\n\"parts\": [\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"Boston\"\n}\n}\n},\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"San Francisco\"\n}\n}\n}\n]\n},\n...\n}\n],\n...\n}\nThe following command demonstrates how you can provide the function output to\nthe model. Replace\nmy-project\nwith the name of your Google Cloud project.\nModel request\nPROJECT_ID=\nmy-project\nMODEL_ID=gemini-2.0-flash\nLOCATION=\"us-central1\"\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json\" \\\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent \\\n-d '{\n\"contents\": [\n{\n\"role\": \"user\",\n\"parts\": {\n\"text\": \"What is difference in temperature in Boston and San Francisco?\"\n}\n},\n{\n\"role\": \"model\",\n\"parts\": [\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"Boston\"\n}\n}\n},\n{\n\"functionCall\": {\n\"name\": \"get_current_weather\",\n\"args\": {\n\"location\": \"San Francisco\"\n}\n}\n}\n]\n},\n{\n\"role\": \"user\",\n\"parts\": [\n{\n\"functionResponse\": {\n\"name\": \"get_current_weather\",\n\"response\": {\n\"temperature\": 30.5,\n\"unit\": \"C\"\n}\n}\n},\n{\n\"functionResponse\": {\n\"name\": \"get_current_weather\",\n\"response\": {\n\"temperature\": 20,\n\"unit\": \"C\"\n}\n}\n}\n]\n}\n],\n\"tools\": [\n{\n\"function_declarations\": [\n{\n\"name\": \"get_current_weather\",\n\"description\": \"Get the current weather in a specific location\",\n\"parameters\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city name of the location for which to get the weather.\"\n}\n},\n\"required\": [\n\"location\"\n]\n}\n}\n]\n}\n]\n}'\nThe natural language response created by the model is similar to the following:\nModel response\n[\n{\n\"candidates\": [\n{\n\"content\": {\n\"parts\": [\n{\n\"text\": \"The temperature in Boston is 30.5C and the temperature in San Francisco is 20C. The difference is 10.5C. \\n\"\n}\n]\n},\n\"finishReason\": \"STOP\",\n...\n}\n]\n...\n}\n]\nPython\nThis example demonstrates a scenario with one\nget_current_weather\nfunction.\nThe user prompt is \"What is the weather like in Boston and San Francisco?\".\nReplace\nmy-project\nwith the name of your Google Cloud project.\nimport\nvertexai\nfrom\nvertexai.generative_models\nimport\n(\nContent\n,\nFunctionDeclaration\n,\nGenerationConfig\n,\nGenerativeModel\n,\nPart\n,\nTool\n,\nToolConfig\n)\n# Initialize Vertex AI\n# TODO(developer): Update the project\nvertexai\n.\ninit\n(\nproject\n=\n\"my-project\"\n,\nlocation\n=\n\"us-central1\"\n)\n# Initialize Gemini model\nmodel\n=\nGenerativeModel\n(\nmodel_name\n=\n\"gemini-2.0-flash\"\n)\n# Manual function declaration\nget_current_weather_func\n=\nFunctionDeclaration\n(\nname\n=\n\"get_current_weather\"\n,\ndescription\n=\n\"Get the current weather in a given location\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"location\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The city name of the location for which to get the weather.\"\n,\n\"default\"\n:\n{\n\"string_value\"\n:\n\"Boston, MA\"\n}\n}\n},\n},\n)\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston and San Francisco?\"\n),\n],\n)\n],\ngeneration_config\n=\nGenerationConfig\n(\ntemperature\n=\n0\n),\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n]\n)\nThe following command demonstrates how you can provide the function output to\nthe model.\nfunction_response_contents\n=\n[]\nfunction_response_parts\n=\n[]\n# You can have parallel function call requests for the same function type.\n# For example, 'location_to_lat_long(\"London\")' and 'location_to_lat_long(\"Paris\")'\n# In that case, collect API responses in parts and send them back to the model\nfor\nfunction_call\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\nfunction_calls\n:\nprint\n(\nf\n\"Function call:\n{\nfunction_call\n.\nname\n}\n\"\n)\n# In this example, we'll use synthetic data to simulate a response payload from an external API\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"Boston, MA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"Boston, MA\"\n,\n\"temperature\"\n:\n38\n,\n\"description\"\n:\n\"Partly Cloudy\"\n}\nif\n(\nfunction_call\n.\nargs\n[\n'location'\n]\n==\n\"San Francisco, CA\"\n):\napi_response\n=\n{\n\"location\"\n:\n\"San Francisco, CA\"\n,\n\"temperature\"\n:\n58\n,\n\"description\"\n:\n\"Sunny\"\n}\nfunction_response_parts\n.\nappend\n(\nPart\n.\nfrom_function_response\n(\nname\n=\nfunction_call\n.\nname\n,\nresponse\n=\n{\n\"contents\"\n:\napi_response\n}\n)\n)\n# Add the function call response to the contents\nfunction_response_contents\n=\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\nfunction_response_parts\n)\nfunction_response_contents\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston and San Francisco?\"\n),\n],\n),\n# User prompt\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n,\n# Function call response\nfunction_response_contents\n,\n# Function response\n],\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_current_weather_func\n],\n)\n]\n)\n# Get the model summary response\nprint\n(\nresponse\n.\ntext\n)\nGo\nimport\n(\n\"context\"\n\"encoding/json\"\n\"errors\"\n\"fmt\"\n\"io\"\n\"cloud.google.com/go/vertexai/genai\"\n)\n// parallelFunctionCalling shows how to execute multiple function calls in parallel\n// and return their results to the model for generating a complete response.\nfunc\nparallelFunctionCalling\n(\nw\nio\n.\nWriter\n,\nprojectID\n,\nlocation\n,\nmodelName\nstring\n)\nerror\n{\n// location = \"us-central1\"\n// modelName = \"gemini-2.0-flash-001\"\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\nprojectID\n,\nlocation\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create GenAI client: %w\"\n,\nerr\n)\n}\ndefer\nclient\n.\nClose\n()\nmodel\n:=\nclient\n.\nGenerativeModel\n(\nmodelName\n)\n// Set temperature to 0.0 for maximum determinism in function calling.\nmodel\n.\nSetTemperature\n(\n0.0\n)\nfuncName\n:=\n\"getCurrentWeather\"\nfuncDecl\n:=\n&\ngenai\n.\nFunctionDeclaration\n{\nName\n:\nfuncName\n,\nDescription\n:\n\"Get the current weather in a given location\"\n,\nParameters\n:\n&\ngenai\n.\nSchema\n{\nType\n:\ngenai\n.\nTypeObject\n,\nProperties\n:\nmap\n[\nstring\n]\n*\ngenai\n.\nSchema\n{\n\"location\"\n:\n{\nType\n:\ngenai\n.\nTypeString\n,\nDescription\n:\n\"The location for which to get the weather. \"\n+\n\"It can be a city name, a city name and state, or a zip code. \"\n+\n\"Examples: 'San Francisco', 'San Francisco, CA', '95616', etc.\"\n,\n},\n},\nRequired\n:\n[]\nstring\n{\n\"location\"\n},\n},\n}\n// Add the weather function to our model toolbox.\nmodel\n.\nTools\n=\n[]\n*\ngenai\n.\nTool\n{\n{\nFunctionDeclarations\n:\n[]\n*\ngenai\n.\nFunctionDeclaration\n{\nfuncDecl\n},\n},\n}\nprompt\n:=\ngenai\n.\nText\n(\n\"Get weather details in New Delhi and San Francisco?\"\n)\nresp\n,\nerr\n:=\nmodel\n.\nGenerateContent\n(\nctx\n,\nprompt\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"got empty response from model\"\n)\n}\nelse\nif\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nFunctionCalls\n())\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"got no function call suggestions from model\"\n)\n}\n// In a production environment, consider adding validations for function names and arguments.\nfor\n_\n,\nfnCall\n:=\nrange\nresp\n.\nCandidates\n[\n0\n].\nFunctionCalls\n()\n{\nfmt\n.\nFprintf\n(\nw\n,\n\"The model suggests to call the function %q with args: %v\\n\"\n,\nfnCall\n.\nName\n,\nfnCall\n.\nArgs\n)\n// Example response:\n// The model suggests to call the function \"getCurrentWeather\" with args: map[location:New Delhi]\n// The model suggests to call the function \"getCurrentWeather\" with args: map[location:San Francisco]\n}\n// Use synthetic data to simulate responses from the external API.\n// In a real application, this would come from an actual weather API.\nmockAPIResp1\n,\nerr\n:=\njson\n.\nMarshal\n(\nmap\n[\nstring\n]\nstring\n{\n\"location\"\n:\n\"New Delhi\"\n,\n\"temperature\"\n:\n\"42\"\n,\n\"temperature_unit\"\n:\n\"C\"\n,\n\"description\"\n:\n\"Hot and humid\"\n,\n\"humidity\"\n:\n\"65\"\n,\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to marshal function response to JSON: %w\"\n,\nerr\n)\n}\nmockAPIResp2\n,\nerr\n:=\njson\n.\nMarshal\n(\nmap\n[\nstring\n]\nstring\n{\n\"location\"\n:\n\"San Francisco\"\n,\n\"temperature\"\n:\n\"36\"\n,\n\"temperature_unit\"\n:\n\"F\"\n,\n\"description\"\n:\n\"Cold and cloudy\"\n,\n\"humidity\"\n:\n\"N/A\"\n,\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to marshal function response to JSON: %w\"\n,\nerr\n)\n}\n// Note, that the function calls don't have to be chained. We can obtain both responses in parallel\n// and return them to Gemini at once.\nfuncResp1\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\nfuncName\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"content\"\n:\nmockAPIResp1\n,\n},\n}\nfuncResp2\n:=\n&\ngenai\n.\nFunctionResponse\n{\nName\n:\nfuncName\n,\nResponse\n:\nmap\n[\nstring\n]\nany\n{\n\"content\"\n:\nmockAPIResp2\n,\n},\n}\n// Return both API responses to the model allowing it to complete its response.\nresp\n,\nerr\n=\nmodel\n.\nGenerateContent\n(\nctx\n,\nprompt\n,\nfuncResp1\n,\nfuncResp2\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nif\nlen\n(\nresp\n.\nCandidates\n)\n==\n0\n||\nlen\n(\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n)\n==\n0\n{\nreturn\nerrors\n.\nNew\n(\n\"got empty response from model\"\n)\n}\nfmt\n.\nFprintln\n(\nw\n,\nresp\n.\nCandidates\n[\n0\n].\nContent\n.\nParts\n[\n0\n])\n// Example response:\n// The weather in New Delhi is hot and humid with a humidity of 65 and a temperature of 42°C. The weather in San Francisco ...\nreturn\nnil\n}\nFunction calling modes\nYou can control how the model uses the provided tools (function declarations) by setting the mode within the\nfunction_calling_config\n.\nMode\nDescription\nAUTO\nThe default model behavior. The model decides whether to predict function calls or respond with natural language based on the context. This is the most flexible mode and recommended for most scenarios.\nVALIDATED\n(Preview)\nThe model is constrained to predict either function calls or natural language, and ensures function schema adherence. If\nallowed_function_names\nis not provided, the model picks from all of the available function declarations. If\nallowed_function_names\nis provided, the model picks from the set of allowed functions.\nANY\nThe model is constrained to always predict one or more function calls and ensures function schema adherence. If\nallowed_function_names\nis not provided, the model picks from all of the available function declarations. If\nallowed_function_names\nis provided, the model picks from the set of allowed functions. Use this mode when you require a function call response to every prompt (if applicable).\nNONE\nThe model is\nprohibited\nfrom making function calls. This is equivalent to sending a request without any function declarations. Use this mode to temporarily disable function calling without removing your tool definitions.\nForced function calling\nInstead of allowing the model to choose between a natural language response and a function call, you can force it to only predict function calls. This is known as\nforced function calling\n. You can also choose to provide the model with a full set of function declarations, but restrict its responses to a subset of these functions.\nThe following example is forced to predict only\nget_weather\nfunction calls.\nPython\nresponse\n=\nmodel\n.\ngenerate_content\n(\ncontents\n=\n[\nContent\n(\nrole\n=\n\"user\"\n,\nparts\n=\n[\nPart\n.\nfrom_text\n(\n\"What is the weather like in Boston?\"\n),\n],\n)\n],\ngeneration_config\n=\nGenerationConfig\n(\ntemperature\n=\n0\n),\ntools\n=\n[\nTool\n(\nfunction_declarations\n=\n[\nget_weather_func\n,\nsome_other_function\n],\n)\n],\ntool_config\n=\nToolConfig\n(\nfunction_calling_config\n=\nToolConfig\n.\nFunctionCallingConfig\n(\n# ANY mode forces the model to predict only function calls\nmode\n=\nToolConfig\n.\nFunctionCallingConfig\n.\nMode\n.\nANY\n,\n# Allowed function calls to predict when the mode is ANY. If empty, any of\n# the provided function calls will be predicted.\nallowed_function_names\n=\n[\n\"get_weather\"\n],\n)\n)\n)\nFunction schema examples\nFunction declarations are compatible with the\nOpenAPI schema\n. We support the following attributes:\ntype\n,\nnullable\n,\nrequired\n,\nformat\n,\ndescription\n,\nproperties\n,\nitems\n,\nenum\n,\nanyOf\n,\n$ref\n, and\n$defs\n. Remaining attributes are not supported.\nFunction with object and array parameters\nThe following example uses a Python dictionary to declare a function that takes both object and array parameters:\nextract_sale_records_func\n=\nFunctionDeclaration\n(\nname\n=\n\"extract_sale_records\"\n,\ndescription\n=\n\"Extract sale records from a document.\"\n,\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"records\"\n:\n{\n\"type\"\n:\n\"array\"\n,\n\"description\"\n:\n\"A list of sale records\"\n,\n\"items\"\n:\n{\n\"description\"\n:\n\"Data for a sale record\"\n,\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"id\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"description\"\n:\n\"The unique id of the sale.\"\n},\n\"date\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"Date of the sale, in the format of MMDDYY, e.g., 031023\"\n},\n\"total_amount\"\n:\n{\n\"type\"\n:\n\"number\"\n,\n\"description\"\n:\n\"The total amount of the sale.\"\n},\n\"customer_name\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The name of the customer, including first name and last name.\"\n},\n\"customer_contact\"\n:\n{\n\"type\"\n:\n\"string\"\n,\n\"description\"\n:\n\"The phone number of the customer, e.g., 650-123-4567.\"\n},\n},\n\"required\"\n:\n[\n\"id\"\n,\n\"date\"\n,\n\"total_amount\"\n],\n},\n},\n},\n\"required\"\n:\n[\n\"records\"\n],\n},\n)\nFunction with enum parameter\nThe following example uses a Python dictionary to declare a function that takes an integer\nenum\nparameter:\nset_status_func\n=\nFunctionDeclaration\n(\nname\n=\n\"set_status\"\n,\ndescription\n=\n\"set a ticket's status field\"\n,\n# Function parameters are specified in JSON schema format\nparameters\n=\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"status\"\n:\n{\n\"type\"\n:\n\"integer\"\n,\n\"enum\"\n:\n[\n\"10\"\n,\n\"20\"\n,\n\"30\"\n],\n# Provide integer (or any other type) values as strings.\n}\n},\n},\n)\nFunction with ref and def\nThe following JSON function declaration uses the\nref\nand\ndefs\nattributes:\n{\n\"contents\"\n:\n...\n,\n\"tools\"\n:\n[\n{\n\"function_declarations\"\n:\n[\n{\n\"name\"\n:\n\"get_customer\"\n,\n\"description\"\n:\n\"Search for a customer by name\"\n,\n\"parameters\"\n:\n{\n\"type\"\n:\n\"object\"\n,\n\"properties\"\n:\n{\n\"first_name\"\n:\n{\n\"ref\"\n:\n\"#/defs/name\"\n},\n\"last_name\"\n:\n{\n\"ref\"\n:\n\"#/defs/name\"\n}\n},\n\"defs\"\n:\n{\n\"name\"\n:\n{\n\"type\"\n:\n\"string\"\n}\n}\n}\n}\n]\n}\n]\n}\nUsage notes:\nUnlike, the OpenAPI schema, specify\nref\nand\ndefs\nwithout the\n$\nsymbol.\nref\nmust refer to direct child of\ndefs\n; no\nexternal references.\nThe maximum depth of nested schema is 32.\nRecursion depth in\ndefs\n(self-reference) is limited to two.\nfrom_func\nwith array parameter\nThe following code sample declares a function that multiplies an array of numbers and uses\nfrom_func\nto generate the\nFunctionDeclaration\nschema.\nfrom\ntyping\nimport\nList\n# Define a function. Could be a local function or you can import the requests library to call an API\ndef\nmultiply_numbers\n(\nnumbers\n:\nList\n[\nint\n]\n=\n[\n1\n,\n1\n])\n->\nint\n:\n\"\"\"\nCalculates the product of all numbers in an array.\nArgs:\nnumbers: An array of numbers to be multiplied.\nReturns:\nThe product of all the numbers. If the array is empty, returns 1.\n\"\"\"\nif\nnot\nnumbers\n:\n# Handle empty array\nreturn\n1\nproduct\n=\n1\nfor\nnum\nin\nnumbers\n:\nproduct\n*=\nnum\nreturn\nproduct\nmultiply_number_func\n=\nFunctionDeclaration\n.\nfrom_func\n(\nmultiply_numbers\n)\n\"\"\"\nmultiply_number_func contains the following schema:\n{'name': 'multiply_numbers',\n'description': 'Calculates the product of all numbers in an array.',\n'parameters': {'properties': {'numbers': {'items': {'type': 'INTEGER'},\n'description': 'list of numbers',\n'default': [1.0, 1.0],\n'title': 'Numbers',\n'type': 'ARRAY'}},\n'description': 'Calculates the product of all numbers in an array.',\n'title': 'multiply_numbers',\n'property_ordering': ['numbers'],\n'type': 'OBJECT'}}\n\"\"\"\nBest practices for function calling\nWrite clear and detailed function names, parameter descriptions, and instructions\nFunction names should start with a letter or an underscore and contain only characters a-z, A-Z, 0-9, underscores, dots or dashes with a maximum length of 64.\nBe extremely clear and specific in your function and parameter descriptions.\nThe model relies on these to choose the correct function and provide\nappropriate arguments. For example, a\nbook_flight_ticket\nfunction could\nhave the description\nbook flight tickets after confirming users' specific requirements, such as time, departure, destination, party size and preferred airline\nUse strong typed parameters\nIf the parameter values are from a finite set, add an\nenum\nfield instead of putting the set of values into the description. If the parameter value is always an integer, set the type to\ninteger\nrather than\nnumber\n.\nTool Selection\nWhile the model can use an arbitrary number of tools, providing too many can\nincrease the risk of selecting an incorrect or suboptimal tool. For best\nresults, aim to provide only the relevant tools for the context or task,\nideally keeping the active set to a maximum of 10-20. If you have a large\ntotal number of tools, consider dynamic tool selection based on\nconversation context.\nIf you provide generic, low-level tools (like\nbash\n) the model might use the tool\nmore often, but with less accuracy. If you provide a specific, high-level tool\n(like\nget_weather\n), the model will be able to use the tool more accurately, but\nthe tool might not be used as often.\nUse system instructions\nWhen using functions with date, time, or location parameters, include the\ncurrent date, time, or relevant location information (for example, city and\ncountry) in the system instruction. This provides the model with the necessary\ncontext to process the request accurately, even if the user's prompt lacks\ndetails.\nPrompt engineering\nFor best results, prepend the user prompt with the following details:\nAdditional context for the model-for example,\nYou are a flight API assistant to help with searching flights based on user preferences.\nDetails or instructions on how and when to use the functions-for example,\nDon't make assumptions on the departure or destination airports. Always use a future date for the departure or destination time.\nInstructions to ask clarifying questions if user queries are ambiguous-for example,\nAsk clarifying questions if not enough information is available.\nUse generation configuration\nFor the temperature parameter, use\n0\nor another low value. This instructs\nthe model to generate more confident results and reduces hallucinations.\nUse structured output\nFunction calling can be used together with\nstructured output\nto let the model always predict function calls or outputs that adheres to a specific schema, so that you receive consistently formatted responses when model does not generate function calls.\nValidate the API call\nIf the model proposes the invocation of a function that would send an order,\nupdate a database, or otherwise have significant consequences, validate the\nfunction call with the user before executing it.\nUse thought signatures\nThought signatures\nshould always be used\nwith function calling for best results.\nPricing\nThe pricing for function calling is based on the number of characters within the\ntext inputs and outputs. To learn more, see\nVertex AI pricing\n.\nHere, text input (prompt)\nrefers to the user prompt for the current conversation turn, the function\ndeclarations for the current conversation turn, and the history of the\nconversation. The history of the conversation includes the queries, the function\ncalls, and the function responses of previous conversation turns.\nVertex AI truncates the history of the conversation at 32,000 characters.\nText output (response) refers to the function calls and the text responses\nfor the current conversation turn.\nUse cases of function calling\nYou can use function calling for the following tasks:\nUse Case\nExample description\nExample link\nIntegrate with external APIs\nGet weather information using a meteorological API\nNotebook tutorial\nConvert addresses to latitude/longitude coordinates\nNotebook tutorial\nConvert currencies using a currency exchange API\nCodelab\nBuild advanced chatbots\nAnswer customer questions about products and services\nNotebook tutorial\nCreate an assistant to answer financial and news questions about companies\nNotebook tutorial\nStructure and control function calls\nExtract structured entities from raw log data\nNotebook tutorial\nExtract single or multiple parameters from user input\nNotebook tutorial\nHandle lists and nested data structures in function calls\nNotebook tutorial\nHandle function calling behavior\nHandle parallel function calls and responses\nNotebook tutorial\nManage when and which functions the model can call\nNotebook tutorial\nQuery databases with natural language\nConvert natural language questions into SQL queries for BigQuery\nSample app\nMultimodal function calling\nUse images, videos, audio, and PDFs as input to trigger function calls\nNotebook tutorial\nHere are some more use cases:\nInterpret voice commands\n: Create functions that correspond with\nin-vehicle tasks. For example, you can create functions that turn on the\nradio or activate the air conditioning. Send audio files of the user's voice\ncommands to the model, and ask the model to convert the audio into text and\nidentify the function that the user wants to call.\nAutomate workflows based on environmental triggers\n: Create functions to\nrepresent processes that can be automated. Provide the model with data from\nenvironmental sensors and ask it to parse and process the data to determine\nwhether one or more of the workflows should be activated. For example, a\nmodel could process temperature data in a warehouse and choose to activate a\nsprinkler function.\nAutomate the assignment of support tickets\n: Provide the model with\nsupport tickets, logs, and context-aware rules. Ask the model to process all\nof this information to determine who the ticket should be assigned to. Call\na function to assign the ticket to the person suggested by the model.\nRetrieve information from a knowledge base\n: Create functions that\nretrieve academic articles on a given subject and summarize them. Enable the\nmodel to answer questions about academic subjects and provide citations for\nits answers.\nWhat's next\nSee the\nAPI reference for function calling\n.\nLearn about\nVertex AI Agent Engine\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 48558,
      "line_count": 4232
    },
    {
      "filename": "vertex-ai_generative-ai_docs_provisioned-throughput.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nProvisioned Throughput overview\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains what Provisioned Throughput is and when to use Provisioned Throughput.\nIntroduction to Provisioned Throughput\nProvisioned Throughput is a fixed-cost, fixed-term subscription\navailable in several term-lengths that reserves throughput for\nsupported generative AI models\non Vertex AI.\nTo reserve your throughput, you must specify the model and\navailable\nlocations\nin which the model\nruns.\nWhen to use Provisioned Throughput\nIf any of the following considerations apply to your use case, consider using\nProvisioned Throughput:\nYou are building real-time generative AI production applications, such as\nchatbots and agents.\nYour critical workloads consistently require high throughput. Throughput\nmeasurement depends on the model.\nYou want to provide a consistent and predictable experience for users of your\napplications.\nYou want deterministic generative AI costs by paying a fixed monthly or weekly\nprice with control of overages.\nProvisioned Throughput is one of two ways to consume your\ngenerative AI models. The second way is pay-as-you-go, which is also referred to\nas\non-demand\n.\nWhat's next\nSupported models\nusing Provisioned Throughput.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 1864,
      "line_count": 50
    },
    {
      "filename": "vertex-ai_generative-ai_docs_reference_python_latest_vertexai.generative_models.Tool.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.Tool",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.generative_models.Tool\n================================================================================\n\nHome\nTechnology areas\nPython\nClient libraries\nSend feedback\nStay organized with collections\nSave and categorize content based on your preferences.\nVersion latest\nkeyboard_arrow_down\n1.122.0 (latest)\n1.121.0\n1.120.0\n1.119.0\n1.118.0\n1.117.0\n1.95.1\n1.94.0\n1.93.1\n1.92.0\n1.91.0\n1.90.0\n1.89.0\n1.88.0\n1.87.0\n1.86.0\n1.85.0\n1.84.0\n1.83.0\n1.82.0\n1.81.0\n1.80.0\n1.79.0\n1.78.0\n1.77.0\n1.76.0\n1.75.0\n1.74.0\n1.73.0\n1.72.0\n1.71.1\n1.70.0\n1.69.0\n1.68.0\n1.67.1\n1.66.0\n1.65.0\n1.63.0\n1.62.0\n1.60.0\n1.59.0\nVertex Generative AI SDK for Python\nThe Gen AI Modules in the Vertex SDK help developers use Google’s generative AI\nGemini models\nto build AI-powered features and applications in Vertex.\nThe modules currently available are: Evaluation, Agent Engines, Prompt Management, and Prompt Optimization. See below for instructions on getting started with each module. For other Gemini features on Vertex, use the\nGen AI SDK\n.\nInstallation\nTo install the\ngoogle-cloud-aiplatform\nPython package, run the following command:\npip3 install --upgrade --user \"google-cloud-aiplatform>=1.114.0\"\nImports:\nimport\nvertexai\nfrom vertexai import types\nClient initialization\nclient = vertexai.Client(project='my-project', location='us-central1')\nGen AI Evaluation\nTo run evaluation, first generate model responses from a set of prompts.\nimport pandas as pd\nprompts_df = pd.DataFrame({\n\"prompt\": [\n\"What is the capital of France?\",\n\"Write a haiku about a cat.\",\n\"Write a Python function to calculate the factorial of a number.\",\n\"Translate 'How are you?' to French.\",\n],\n})\ninference_results = client.evals.run_inference(\nmodel=\"gemini-2.5-flash\",\n(\"def factorial(n):\\n\"\n\"    if n < 0:\\n\"\n\"        return 'Factorial does not exist for negative numbers'\\n\"\n\"    elif n == 0:\\n\"\n\"        return 1\\n\"\n\"    else:\\n\"\n\"        fact = 1\\n\"\n\"        i = 1\\n\"\n\"        while i <= n:\\n\"\n\"            fact *= i\\n\"\n\"            i += 1\\n\"\n\"        return fact\"),\n)\ninference_results.show()\nThen run evaluation by providing the inference results and specifying the metric types.\neval_result = client.evals.evaluate(\ndataset=inference_results,\nmetrics=[\ntypes.RubricMetric.GENERAL_QUALITY,\n]\n)\neval_result.show()\nAgent Engine with Agent Development Kit (ADK)\nFirst, define a function that looks up the exchange rate:\ndef get_exchange_rate(\ncurrency_from: str = \"USD\",\ncurrency_to: str = \"EUR\",\ncurrency_date: str = \"latest\",\n):\n\"\"\"Retrieves the exchange rate between two currencies on a specified date.\nUses the Frankfurter API (https://api.frankfurter.app/) to obtain\nexchange rate data.\nReturns:\ndict: A dictionary containing the exchange rate information.\nExample: {\"amount\": 1.0, \"base\": \"USD\", \"date\": \"2023-11-24\",\n\"rates\": {\"EUR\": 0.95534}}\n\"\"\"\nimport requests\nresponse = requests.get(\nf\"https://api.frankfurter.app/{currency_date}\",\nparams={\"from\": currency_from, \"to\": currency_to},\n)\nreturn response.json()\nNext, define an ADK Agent:\nfrom google.adk.agents import Agent\nfrom vertexai.agent_engines import AdkApp\napp = AdkApp(agent=Agent(\nmodel=\"gemini-2.0-flash\",        # Required.\nname='currency_exchange_agent',  # Required.\ntools=[get_exchange_rate],       # Optional.\n))\nTest the agent locally using US dollars and Swedish Krona:\nasync for event in app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nTo deploy the agent to Agent Engine:\nremote_app = client.agent_engines.create(\nagent=app,\nconfig={\n\"requirements\": [\"google-cloud-aiplatform[agent_engines,adk]\"],\n},\n)\nYou can also run queries against the deployed agent:\nasync for event in remote_app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nPrompt Optimization\nTo do a zero-shot prompt optimization, use the\noptimize_prompt\nmethod.\nprompt = \"Generate system instructions for a question-answering assistant\"\nresponse = client.prompt_optimizer.optimize_prompt(prompt=prompt)\nprint(response.raw_text_response)\nif response.parsed_response:\nprint(response.parsed_response.suggested_prompt)\nTo call the data-driven prompt optimization, call the\noptimize\nmethod.\nIn this case however, we need to provide\nvapo_config\n. This config needs to\nhave either service account or project\nnumber\nand the config path.\nPlease refer to this\ntutorial\nfor more details on config parameter.\nimport logging\nproject_number = PROJECT_NUMBER # replace with your project number\nservice_account = f\"{project_number}-compute@developer.gserviceaccount.com\"\nvapo_config = types.PromptOptimizerVAPOConfig(\nconfig_path=\"gs://your-bucket/config.json\",\nservice_account_project_number=project_number,\nwait_for_completion=False\n)\n# Set up logging to see the progress of the optimization job\nlogging.basicConfig(encoding='utf-8', level=logging.INFO, force=True)\nresult = client.prompt_optimizer.optimize(method=\"vapo\", config=vapo_config)\nWe can also call optimize method async.\nawait client.aio.prompt_optimizer.optimize(method=\"vapo\", config=vapo_config)\nPrompt Management\nFirst define your prompt as a dictionary or\ntypes.Prompt\nobject. Then call\ncreate()\n.\nprompt = {\n\"prompt_data\": {\n\"contents\": [{\"parts\": [{\"text\": \"Hello, {name}! How are you?\"}]}],\n\"system_instruction\": {\"parts\": [{\"text\": \"Please answer in a short sentence.\"}]},\n\"variables\": [\n{\"name\": {\"text\": \"Alice\"}},\n],\n\"model\": \"gemini-2.5-flash\",\n},\n}\nprompt_resource = client.prompts.create(\nprompt=prompt,\n)\nNote that you can also use the\ntypes.Prompt\nobject to define your prompt. Some\nof the types used to do this are from the\nGen AI SDK\n.\nimport types\nfrom google.genai import types as genai_types\nprompt = types.Prompt(\nprompt_data=types.PromptData(\ncontents=[genai_types.Content(parts=[genai_types.Part(text=\"Hello, {name}! How are you?\")])],\nsystem_instruction=genai_types.Content(parts=[genai_types.Part(text=\"Please answer in a short sentence.\")]),\nvariables=[\n{\"name\": genai_types.Part(text=\"Alice\")},\n],\nmodel=\"gemini-2.5-flash\",\n),\n)\nRetrieve a prompt by calling\nget()\nwith the\nprompt_id\n.\nretrieved_prompt = client.prompts.get(prompt_id=prompt_resource.prompt_id)\nAfter creating or retrieving a prompt, you can call\ngenerate_content()\nwith\nthat prompt using the Gen AI SDK.\nThe following uses a utility function available on Prompt objects to transform a\nPrompt object into a list of Content objects for use with\ngenerate_content\n. To\nrun this you need to have the Gen AI SDK installed, which you can do via\npip install google-genai\n.\nfrom google import genai\nfrom google.genai import types as genai_types\n# Create a Client in the Gen AI SDK\ngenai_client = genai.Client(vertexai=True, project=\"your-project\", location=\"your-location\")\n# Call generate_content() with the prompt\nresponse = genai_client.models.generate_content(\nmodel=retrieved_prompt.prompt_data.model,\ncontents=retrieved_prompt.assemble_contents(),\n)\nWarning\nThe following Generative AI modules in the Vertex AI SDK are deprecated as of\nJune 24, 2025 and will be removed on June 24, 2026:\nvertexai.generative_models\n,\nvertexai.language_models\n,\nvertexai.vision_models\n,\nvertexai.tuning\n,\nvertexai.caching\n. Please use the\nGoogle Gen AI SDK\nto access these\nfeatures. See\nthe migration guide\nfor details. You can continue using all other Vertex AI SDK modules, as they are\nthe recommended way to use the API.\nImports:\nimport\nvertexai\nInitialization:\nvertexai.init(project='my-project', location='us-central1')\nBasic generation:\nfrom vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\"gemini-pro\")\nprint(model.generate_content(\"Why is sky blue?\"))\nUsing images and videos\nfrom vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-pro-vision\")\n# Local image\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_model.generate_content([\"What is shown in this image?\", image]))\n# Image from Cloud Storage\nimage_part = generative_models.Part.from_uri(\"gs://download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\", mime_type=\"image/jpeg\")\nprint(vision_model.generate_content([image_part, \"Describe this image?\"]))\n# Text and video\nvideo_part = Part.from_uri(\"gs://cloud-samples-data/video/animals.mp4\", mime_type=\"video/mp4\")\nprint(vision_model.generate_content([\"What is in the video? \", video_part]))\nChat\nfrom vertexai.generative_models import GenerativeModel, Image\nvision_model = GenerativeModel(\"gemini-ultra-vision\")\nvision_chat = vision_model.start_chat()\nimage = Image.load_from_file(\"image.jpg\")\nprint(vision_chat.send_message([\"I like this image.\", image]))\nprint(vision_chat.send_message(\"What things do I like?.\"))\nSystem instructions\nfrom vertexai.generative_models import GenerativeModel\nmodel = GenerativeModel(\n\"gemini-1.0-pro\",\nsystem_instruction=[\n\"Talk like a pirate.\",\n\"Don't use rude words.\",\n],\n)\nprint(model.generate_content(\"Why is sky blue?\"))\nFunction calling\n# First, create tools that the model is can use to answer your questions.\n# Describe a function by specifying it's schema (JsonSchema format)\nget_current_weather_func = generative_models.FunctionDeclaration(\nname=\"get_current_weather\",\ndescription=\"Get the current weather in a given location\",\nparameters={\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g. San Francisco, CA\"\n},\n\"unit\": {\n\"type\": \"string\",\n\"enum\": [\n\"celsius\",\n\"fahrenheit\",\n]\n}\n},\n\"required\": [\n\"location\"\n]\n},\n)\n# Tool is a collection of related functions\nweather_tool = generative_models.Tool(\nfunction_declarations=[get_current_weather_func],\n)\n# Use tools in chat:\nmodel = GenerativeModel(\n\"gemini-pro\",\n# You can specify tools when creating a model to avoid having to send them with every request.\ntools=[weather_tool],\n)\nchat = model.start_chat()\n# Send a message to the model. The model will respond with a function call.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\n# Then send a function response to the model. The model will use it to answer.\nprint(chat.send_message(\nPart.from_function_response(\nname=\"get_current_weather\",\nresponse={\n\"content\": {\"weather\": \"super nice\"},\n}\n),\n))\nAutomatic Function calling\nNote: The\nFunctionDeclaration.from_func\nconverter does not support nested types for parameters. Please provide full\nFunctionDeclaration\ninstead.\nfrom vertexai.preview.generative_models import GenerativeModel, Tool, FunctionDeclaration, AutomaticFunctionCallingResponder\n# First, create functions that the model can use to answer your questions.\ndef get_current_weather(location: str, unit: str = \"centigrade\"):\n\"\"\"Gets weather in the specified location.\nArgs:\nlocation: The location for which to get the weather.\nunit: Optional. Temperature unit. Can be Centigrade or Fahrenheit. Defaults to Centigrade.\n\"\"\"\nreturn dict(\nlocation=location,\nunit=unit,\nweather=\"Super nice, but maybe a bit hot.\",\n)\n# Infer function schema\nget_current_weather_func = FunctionDeclaration.from_func(get_current_weather)\n# Tool is a collection of related functions\nweather_tool = Tool(\nfunction_declarations=[get_current_weather_func],\n)\n# Use tools in chat:\nmodel = GenerativeModel(\n\"gemini-pro\",\n# You can specify tools when creating a model to avoid having to send them with every request.\ntools=[weather_tool],\n)\n# Activate automatic function calling:\nafc_responder = AutomaticFunctionCallingResponder(\n# Optional:\nmax_automatic_function_calls=5,\n)\nchat = model.start_chat(responder=afc_responder)\n# Send a message to the model. The model will respond with a function call.\n# The SDK will automatically call the requested function and respond to the model.\n# The model will use the function call response to answer the original question.\nprint(chat.send_message(\"What is the weather like in Boston?\"))\nEvaluation\nTo perform bring-your-own-response(BYOR) evaluation, provide the model responses in the\nresponse\ncolumn in the dataset. If a pairwise metric is used for BYOR evaluation, provide the baseline model responses in the\nbaseline_model_response\ncolumn.\nimport pandas as pd\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples\neval_dataset = pd.DataFrame({\n\"prompt\"  : [...],\n\"reference\": [...],\n\"response\" : [...],\n\"baseline_model_response\": [...],\n})\neval_task = EvalTask(\ndataset=eval_dataset,\nmetrics=[\n\"bleu\",\n\"rouge_l_sum\",\nMetricPromptTemplateExamples.Pointwise.FLUENCY,\nMetricPromptTemplateExamples.Pairwise.SAFETY\n],\nexperiment=\"my-experiment\",\n)\neval_result = eval_task.evaluate(experiment_run_name=\"eval-experiment-run\")\nTo perform evaluation with Gemini model inference, specify the\nmodel\nparameter with a\nGenerativeModel\ninstance.  The input column name to the model is\nprompt\nand must be present in the dataset.\nfrom vertexai.evaluation import EvalTask\nfrom vertexai.generative_models import GenerativeModel\neval_dataset = pd.DataFrame({\n\"reference\": [...],\n\"prompt\"  : [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[\"exact_match\", \"bleu\", \"rouge_1\", \"rouge_l_sum\"],\nexperiment=\"my-experiment\",\n).evaluate(\nmodel=GenerativeModel(\"gemini-1.5-pro\"),\nexperiment_run_name=\"gemini-eval-run\"\n)\nIf a\nprompt_template\nis specified, the\nprompt\ncolumn is not required. Prompts can be assembled from the evaluation dataset, and all prompt template variable names must be present in the dataset columns.\nimport pandas as pd\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples\nfrom vertexai.generative_models import GenerativeModel\neval_dataset = pd.DataFrame({\n\"context\"    : [...],\n\"instruction\": [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[MetricPromptTemplateExamples.Pointwise.SUMMARIZATION_QUALITY],\n).evaluate(\nmodel=GenerativeModel(\"gemini-1.5-pro\"),\nprompt_template=\"{instruction}. Article: {context}. Summary:\",\n)\nTo perform evaluation with custom model inference, specify the\nmodel\nparameter with a custom inference function. The input column name to the\ncustom inference function is\nprompt\nand must be present in the dataset.\nfrom openai import OpenAI\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples\nclient = OpenAI()\ndef custom_model_fn(input: str) -> str:\nresponse = client.chat.completions.create(\nmodel=\"gpt-3.5-turbo\",\nmessages=[\n{\"role\": \"user\", \"content\": input}\n]\n)\nreturn response.choices[0].message.content\neval_dataset = pd.DataFrame({\n\"prompt\"  : [...],\n\"reference\": [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[MetricPromptTemplateExamples.Pointwise.SAFETY],\nexperiment=\"my-experiment\",\n).evaluate(\nmodel=custom_model_fn,\nexperiment_run_name=\"gpt-eval-run\"\n)\nTo perform pairwise metric evaluation with model inference step, specify\nthe\nbaseline_model\ninput to a\nPairwiseMetric\ninstance and the candidate\nmodel\ninput to the\nEvalTask.evaluate()\nfunction. The input column name\nto both models is\nprompt\nand must be present in the dataset.\nimport pandas as pd\nfrom vertexai.evaluation import EvalTask, MetricPromptTemplateExamples, PairwiseMetric\nfrom vertexai.generative_models import GenerativeModel\nbaseline_model = GenerativeModel(\"gemini-1.0-pro\")\ncandidate_model = GenerativeModel(\"gemini-1.5-pro\")\npairwise_groundedness = PairwiseMetric(\nmetric_prompt_template=MetricPromptTemplateExamples.get_prompt_template(\n\"pairwise_groundedness\"\n),\nbaseline_model=baseline_model,\n)\neval_dataset = pd.DataFrame({\n\"prompt\"  : [...],\n})\nresult = EvalTask(\ndataset=eval_dataset,\nmetrics=[pairwise_groundedness],\nexperiment=\"my-pairwise-experiment\",\n).evaluate(\nmodel=candidate_model,\nexperiment_run_name=\"gemini-pairwise-eval-run\",\n)\nAgent Engine\nBefore you begin, install the packages with\npip3 install --upgrade --user \"google-cloud-aiplatform[agent_engines,adk]>=1.111\"\nFirst, define a function that looks up the exchange rate:\ndef get_exchange_rate(\ncurrency_from: str = \"USD\",\ncurrency_to: str = \"EUR\",\ncurrency_date: str = \"latest\",\n):\n\"\"\"Retrieves the exchange rate between two currencies on a specified date.\nUses the Frankfurter API (https://api.frankfurter.app/) to obtain\nexchange rate data.\nReturns:\ndict: A dictionary containing the exchange rate information.\nExample: {\"amount\": 1.0, \"base\": \"USD\", \"date\": \"2023-11-24\",\n\"rates\": {\"EUR\": 0.95534}}\n\"\"\"\nimport requests\nresponse = requests.get(\nf\"https://api.frankfurter.app/{currency_date}\",\nparams={\"from\": currency_from, \"to\": currency_to},\n)\nreturn response.json()\nNext, define an ADK Agent:\nfrom google.adk.agents import Agent\nfrom vertexai.agent_engines import AdkApp\napp = AdkApp(agent=Agent(\nmodel=\"gemini-2.0-flash\",        # Required.\nname='currency_exchange_agent',  # Required.\ntools=[get_exchange_rate],       # Optional.\n))\nTest the agent locally using US dollars and Swedish Krona:\nasync for event in app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nTo deploy the agent to Agent Engine:\nvertexai.init(\nproject='my-project',\nlocation='us-central1',\nstaging_bucket=\"gs://my-staging-bucket\",\n)\nremote_app = vertexai.agent_engines.create(\napp,\nrequirements=[\"google-cloud-aiplatform[agent_engines,adk]\"],\n)\nYou can also run queries against the deployed agent:\nasync for event in remote_app.async_stream_query(\nuser_id=\"user-id\",\nmessage=\"What is the exchange rate from US dollars to SEK today?\",\n):\nprint(event)\nDocumentation\nYou can find complete documentation for the Vertex AI SDKs and the Gemini model in the Google Cloud\ndocumentation\nContributing\nSee\nContributing\nfor more information on contributing to the Vertex AI Python SDK.\nLicense\nThe contents of this repository are licensed under the\nApache License, version 2.0\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-10-30 UTC.",
      "content_length": 18029,
      "line_count": 599
    },
    {
      "filename": "vertex-ai_generative-ai_docs_start_libraries.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/libraries",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/libraries\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGoogle Gen AI libraries\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page provides information on downloading and installing the latest\nlibraries for the Gemini API. If you're new to the Gemini\nAPI, get started with the\nAPI quickstart\n.\nImportant note about google-genai libraries\nWe've recently launched a new set of libraries that provide a more consistent\nand streamlined experience for accessing Google's generative AI models across\ndifferent Google services.\nVertex AI libraries are only supported on the Vertex AI platform.\nKey library updates\nLanguage\nVertex AI library\nNew library (Recommended)\nPython\ngoogle-cloud-aiplatform\nGenerativeModel Module Deprecated in May 2026\ngoogle-genai\nGo\ncloud.google.com/vertexai\nDeprecated in May 2026\ngoogle.golang.org/genai\nJavaScript and TypeScript\n@google-cloud/vertexai\nDeprecated in May 2026\n@google/genai\nAvailable in\nPreview\nJava\ngoogle-cloud-vertexai\nDeprecated in May 2026\njava-genai\nAvailable in\nPreview\nUsers are encouraged to start with the new library and migrate from previous\nlibraries.\nInstall a library\nThe following examples can help you get started in various programming languages.\nPython\nYou can install our\nPython library\nby running:\npip\ninstall\ngoogle-genai\nGo\nYou can install our\nGo library\nby running:\ngo\nget\ngoogle.golang.org/genai\nJavaScript and TypeScript\nYou can install our\nJavaScript and TypeScript library\nby running:\nnpm\ninstall\n@google/genai\nThe\nnew\nJavaScript and TypeScript library is available in\npreview\n, which means\nit may not be feature complete and that we may need to introduce breaking changes.\nHowever, we\nhighly recommend\nyou start using the\nnew SDK\nover the previous, deprecated version as long as you're comfortable with these caveats.\nWe're actively working towards a GA (General Availability) release for this library.\nJava\nYou can install our\nJava library\nby adding the dependencies in Maven:\n<dependencies>\n<dependency>\n<groupId>com.google.genai</groupId>\n<artifactId>google-genai</artifactId>\n<version>0.8.0</version>\n</dependency>\n</dependencies>\nThe new Java library is available in\npreview\n, which means\nit may not be feature complete and that we may need to introduce breaking changes.\nHowever, we\nhighly recommend\nyou start using the\nnew\nSDK over the\nprevious, deprecated version as long as you're comfortable with these caveats.\nWe're actively working towards a GA (General Availability) release for this library.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 3042,
      "line_count": 113
    },
    {
      "filename": "vertex-ai_generative-ai_docs_start_quickstarts_quickstart-multimodal.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nGemini API in Vertex AI quickstart\nStay organized with collections\nSave and categorize content based on your preferences.\nThis quickstart shows you how to install the Google Gen AI SDK for your\nlanguage of choice and then make your first API request. The samples vary\nslightly based on whether you authenticate to Vertex AI using an\nAPI key\nor\napplication default credentials (ADC)\n.\nChoose your authentication method:\nADC\nAPI key\nBefore you begin\nIf you haven't configured ADC yet, follow these instructions:\nConfigure your project\nSelect a project, enable billing, enable the Vertex AI API, and install gcloud CLI:\nSign in\nto your Google Account.\nIf you don't already have one,\nsign up for a new account\n.\nIn the Google Cloud console, on the project selector page,\nselect or create a Google Cloud project.\nRoles required to select or create a project\nSelect a project\n: Selecting a project doesn't require a specific\nIAM role—you can select any project that you've been\ngranted a role on.\nCreate a project\n: To create a project, you need the Project Creator\n(\nroles/resourcemanager.projectCreator\n), which contains the\nresourcemanager.projects.create\npermission.\nLearn how to grant\nroles\n.\nGo to project selector\nVerify that billing is enabled for your Google Cloud project\n.\nEnable the Vertex AI API.\nRoles required to enable APIs\nTo enable APIs, you need the Service Usage Admin IAM\nrole (\nroles/serviceusage.serviceUsageAdmin\n), which\ncontains the\nserviceusage.services.enable\npermission.\nLearn how to grant\nroles\n.\nEnable the API\nInstall\nthe Google Cloud CLI.\nIf you're using an external identity provider (IdP), you must first\nsign in to the gcloud CLI with your federated identity\n.\nTo\ninitialize\nthe gcloud CLI, run the following command:\ngcloud\ninit\nIn the Google Cloud console, on the project selector page,\nselect or create a Google Cloud project.\nRoles required to select or create a project\nSelect a project\n: Selecting a project doesn't require a specific\nIAM role—you can select any project that you've been\ngranted a role on.\nCreate a project\n: To create a project, you need the Project Creator\n(\nroles/resourcemanager.projectCreator\n), which contains the\nresourcemanager.projects.create\npermission.\nLearn how to grant\nroles\n.\nGo to project selector\nVerify that billing is enabled for your Google Cloud project\n.\nEnable the Vertex AI API.\nRoles required to enable APIs\nTo enable APIs, you need the Service Usage Admin IAM\nrole (\nroles/serviceusage.serviceUsageAdmin\n), which\ncontains the\nserviceusage.services.enable\npermission.\nLearn how to grant\nroles\n.\nEnable the API\nInstall\nthe Google Cloud CLI.\nIf you're using an external identity provider (IdP), you must first\nsign in to the gcloud CLI with your federated identity\n.\nTo\ninitialize\nthe gcloud CLI, run the following command:\ngcloud\ninit\nCreate local authentication credentials\nCreate local authentication credentials for your user account:\ngcloud\nauth\napplication-default\nlogin\nIf an authentication error is returned, and you are using an external identity provider\n(IdP), confirm that you have\nsigned in to the gcloud CLI with your federated identity\n.\nRequired roles\nTo get the permissions that\nyou need to use the Gemini API in Vertex AI,\nask your administrator to grant you the\nVertex AI User\n(\nroles/aiplatform.user\n)\nIAM role on your project.\nFor more information about granting roles, see\nManage access to projects, folders, and organizations\n.\nYou might also be able to get\nthe required permissions through\ncustom\nroles\nor other\npredefined\nroles\n.\nInstall the SDK and set up your environment\nOn your local machine, click one of the following tabs to install the SDK for\nyour programming language.\nPython Gen AI SDK\nInstall and update the Gen AI SDK for Python by running this command.\npip\ninstall\n--upgrade\ngoogle-genai\nSet environment variables:\n# Replace the `GOOGLE_CLOUD_PROJECT_ID` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT_ID\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nGo Gen AI SDK\nInstall and update the Gen AI SDK for Go by running this command.\ngo\nget\ngoogle.golang.org/genai\nSet environment variables:\n# Replace the `GOOGLE_CLOUD_PROJECT_ID` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT_ID\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nNode.js Gen AI SDK\nInstall and update the Gen AI SDK for Node.js by running this command.\nnpm\ninstall\n@google/genai\nSet environment variables:\n# Replace the `GOOGLE_CLOUD_PROJECT_ID` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT_ID\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nJava Gen AI SDK\nInstall and update the Gen AI SDK for Java by running this command.\nMaven\nAdd the following to your\npom.xml\n:\n<dependencies>\n<dependency>\n<groupId>com.google.genai</groupId>\n<artifactId>google-genai</artifactId>\n<version>0.7.0</version>\n</dependency>\n</dependencies>\nSet environment variables:\n# Replace the `GOOGLE_CLOUD_PROJECT_ID` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT_ID\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nREST\nSet environment variables:\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT_ID\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nAPI_ENDPOINT\n=\nYOUR_API_ENDPOINT\nMODEL_ID\n=\n\"gemini-2.5-flash\"\nGENERATE_CONTENT_API\n=\n\"generateContent\"\nMake your first request\nUse the\ngenerateContent\nmethod to send a request to the Gemini API in Vertex AI:\nPython\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nHttpOptions\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"How does AI work?\"\n,\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\nGo\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\n\"google.golang.org/genai\"\n)\n//\ngenerateWithText\nshows\nhow\nto\ngenerate\ntext\nusing\na\ntext\nprompt\n.\nfunc\ngenerateWithText\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\n\"gemini-2.5-flash\"\n,\ngenai\n.\nText\n(\n\"How does AI work?\"\n),\nnil\n,\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThat\n's a great question! Understanding how AI works can feel like ...\n//\n...\n//\n**\n1.\nThe\nFoundation\n:\nData\nand\nAlgorithms\n**\n//\n...\nreturn\nnil\n}\nNode.js\nconst\n{\nGoogleGenAI\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'global'\n;\nasync\nfunction\ngenerateContent\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n'gemini-2.5-flash'\n,\ncontents\n:\n'How does AI work?'\n,\n});\nconsole\n.\nlog\n(\nresponse\n.\ntext\n);\nreturn\nresponse\n.\ntext\n;\n}\nJava\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.HttpOptions\n;\npublic\nclass\nTextGenerationWithText\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\nmodelId\n=\n\"gemini-2.5-flash\"\n;\ngenerateContent\n(\nmodelId\n);\n}\n//\nGenerates\ntext\nwith\ntext\ninput\npublic\nstatic\nString\ngenerateContent\n(\nString\nmodelId\n)\n{\n//\nInitialize\nclient\nthat\nwill\nbe\nused\nto\nsend\nrequests\n.\nThis\nclient\nonly\nneeds\nto\nbe\ncreated\n//\nonce\n,\nand\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nhttpOptions\n(\nHttpOptions\n.\nbuilder\n()\n.\napiVersion\n(\n\"v1\"\n)\n.\nbuild\n())\n.\nbuild\n())\n{\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\n\"How does AI work?\"\n,\nnull\n);\nSystem\n.\nout\n.\nprint\n(\nresponse\n.\ntext\n());\n//\nExample\nresponse\n:\n//\nOkay\n,\nlet\n's break down how AI works. It'\ns\na\nbroad\nfield\n,\nso\nI\n'll focus on the ...\n//\n//\nHere\n's a simplified overview:\n//\n...\nreturn\nresponse\n.\ntext\n();\n}\n}\n}\nREST\nTo send this prompt request, run the curl command from the command line or\ninclude the REST call in your application.\ncurl\n-X\nPOST\n-H\n\"Content-Type: application/json\"\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\"https://\n${\nAPI_ENDPOINT\n}\n/v1/projects/\n${\nGOOGLE_CLOUD_PROJECT\n}\n/locations/\n${\nGOOGLE_CLOUD_LOCATION\n}\n/publishers/google/models/\n${\nMODEL_ID\n}\n:\n${\nGENERATE_CONTENT_API\n}\n\"\n-d\n$'{\n\"contents\": {\n\"role\": \"user\",\n\"parts\": {\n\"text\": \"Explain how AI works in a few words\"\n}\n}\n}'\nThe model returns a response. Note that the response is generated in sections\nwith each section separately evaluated for safety.\nGenerate images\nGemini can generate and process images conversationally. You can prompt\nGemini with text, images, or a combination of both to achieve various\nimage-related tasks, such as image generation and editing. The following code\ndemonstrates how to generate an image based on a descriptive prompt:\nYou must include\nresponseModalities: [\"TEXT\", \"IMAGE\"]\nin your\nconfiguration. Image-only output is not supported with these models.\nPython\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nGenerateContentConfig\n,\nModality\nfrom\nPIL\nimport\nImage\nfrom\nio\nimport\nBytesIO\nclient\n=\ngenai\n.\nClient\n()\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash-image\"\n,\ncontents\n=\n(\n\"Generate an image of the Eiffel tower with fireworks in the background.\"\n),\nconfig\n=\nGenerateContentConfig\n(\nresponse_modalities\n=\n[\nModality\n.\nTEXT\n,\nModality\n.\nIMAGE\n],\ncandidate_count\n=\n1\n,\nsafety_settings\n=\n[\n{\n\"method\"\n:\n\"PROBABILITY\"\n},\n{\n\"category\"\n:\n\"HARM_CATEGORY_DANGEROUS_CONTENT\"\n},\n{\n\"threshold\"\n:\n\"BLOCK_MEDIUM_AND_ABOVE\"\n},\n],\n),\n)\nfor\npart\nin\nresponse\n.\ncandidates\n[\n0\n]\n.\ncontent\n.\nparts\n:\nif\npart\n.\ntext\n:\nprint\n(\npart\n.\ntext\n)\nelif\npart\n.\ninline_data\n:\nimage\n=\nImage\n.\nopen\n(\nBytesIO\n((\npart\n.\ninline_data\n.\ndata\n)))\nimage\n.\nsave\n(\n\"output_folder/example-image-eiffel-tower.png\"\n)\n# Example response:\n#   I will generate an image of the Eiffel Tower at night, with a vibrant display of\n#   colorful fireworks exploding in the dark sky behind it. The tower will be\n#   illuminated, standing tall as the focal point of the scene, with the bursts of\n#   light from the fireworks creating a festive atmosphere.\nNode.js\nconst\nfs\n=\nrequire\n(\n'fs'\n);\nconst\n{\nGoogleGenAI\n,\nModality\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'us-central1'\n;\nasync\nfunction\ngenerateImage\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContentStream\n({\nmodel\n:\n'gemini-2.5-flash-image'\n,\ncontents\n:\n'Generate an image of the Eiffel tower with fireworks in the background.'\n,\nconfig\n:\n{\nresponseModalities\n:\n[\nModality\n.\nTEXT\n,\nModality\n.\nIMAGE\n],\n},\n});\nconst\ngeneratedFileNames\n=\n[];\nlet\nimageIndex\n=\n0\n;\nfor\nawait\n(\nconst\nchunk\nof\nresponse\n)\n{\nconst\ntext\n=\nchunk\n.\ntext\n;\nconst\ndata\n=\nchunk\n.\ndata\n;\nif\n(\ntext\n)\n{\nconsole\n.\ndebug\n(\ntext\n);\n}\nelse\nif\n(\ndata\n)\n{\nconst\noutputDir\n=\n'output-folder'\n;\nif\n(\n!\nfs\n.\nexistsSync\n(\noutputDir\n))\n{\nfs\n.\nmkdirSync\n(\noutputDir\n,\n{\nrecursive\n:\ntrue\n});\n}\nconst\nfileName\n=\n`\n$\n{\noutputDir\n}\n/\ngenerate_content_streaming_image_\n$\n{\nimageIndex\n++\n}\n.\npng\n`\n;\nconsole\n.\ndebug\n(\n`\nWriting\nresponse\nimage\nto\nfile\n:\n$\n{\nfileName\n}\n.\n`\n);\ntry\n{\nfs\n.\nwriteFileSync\n(\nfileName\n,\ndata\n);\ngeneratedFileNames\n.\npush\n(\nfileName\n);\n}\ncatch\n(\nerror\n)\n{\nconsole\n.\nerror\n(\n`\nFailed\nto\nwrite\nimage\nfile\n$\n{\nfileName\n}:\n`\n,\nerror\n);\n}\n}\n}\n//\nExample\nresponse\n:\n//\nI\nwill\ngenerate\nan\nimage\nof\nthe\nEiffel\nTower\nat\nnight\n,\nwith\na\nvibrant\ndisplay\nof\n//\ncolorful\nfireworks\nexploding\nin\nthe\ndark\nsky\nbehind\nit\n.\nThe\ntower\nwill\nbe\n//\nilluminated\n,\nstanding\ntall\nas\nthe\nfocal\npoint\nof\nthe\nscene\n,\nwith\nthe\nbursts\nof\n//\nlight\nfrom\nthe\nfireworks\ncreating\na\nfestive\natmosphere\n.\nreturn\ngeneratedFileNames\n;\n}\nJava\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.Blob\n;\nimport\ncom.google.genai.types.Candidate\n;\nimport\ncom.google.genai.types.Content\n;\nimport\ncom.google.genai.types.GenerateContentConfig\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.Part\n;\nimport\ncom.google.genai.types.SafetySetting\n;\nimport\njava.awt.image.BufferedImage\n;\nimport\njava.io.ByteArrayInputStream\n;\nimport\njava.io.File\n;\nimport\njava.io.IOException\n;\nimport\njava.util.ArrayList\n;\nimport\njava.util.List\n;\nimport\njavax.imageio.ImageIO\n;\npublic\nclass\nImageGenMmFlashWithText\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\nthrows\nIOException\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\nmodelId\n=\n\"gemini-2.5-flash-image\"\n;\nString\noutputFile\n=\n\"resources/output/example-image-eiffel-tower.png\"\n;\ngenerateContent\n(\nmodelId\n,\noutputFile\n);\n}\n//\nGenerates\nan\nimage\nwith\ntext\ninput\npublic\nstatic\nvoid\ngenerateContent\n(\nString\nmodelId\n,\nString\noutputFile\n)\nthrows\nIOException\n{\n//\nClient\nInitialization\n.\nOnce\ncreated\n,\nit\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nbuild\n())\n{\nGenerateContentConfig\ncontentConfig\n=\nGenerateContentConfig\n.\nbuilder\n()\n.\nresponseModalities\n(\n\"TEXT\"\n,\n\"IMAGE\"\n)\n.\ncandidateCount\n(\n1\n)\n.\nsafetySettings\n(\nSafetySetting\n.\nbuilder\n()\n.\nmethod\n(\n\"PROBABILITY\"\n)\n.\ncategory\n(\n\"HARM_CATEGORY_DANGEROUS_CONTENT\"\n)\n.\nthreshold\n(\n\"BLOCK_MEDIUM_AND_ABOVE\"\n)\n.\nbuild\n())\n.\nbuild\n();\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\n\"Generate an image of the Eiffel tower with fireworks in the background.\"\n,\ncontentConfig\n);\n//\nGet\nparts\nof\nthe\nresponse\nList<Part>\nparts\n=\nresponse\n.\ncandidates\n()\n.\nflatMap\n(\ncandidates\n-\n>\ncandidates\n.\nstream\n()\n.\nfindFirst\n())\n.\nflatMap\n(\nCandidate\n::\ncontent\n)\n.\nflatMap\n(\nContent\n::\nparts\n)\n.\norElse\n(\nnew\nArrayList\n<>\n());\n//\nFor\neach\npart\nprint\ntext\nif\npresent\n,\notherwise\nread\nimage\ndata\nif\npresent\nand\n//\nwrite\nit\nto\nthe\noutput\nfile\nfor\n(\nPart\npart\n:\nparts\n)\n{\nif\n(\npart\n.\ntext\n()\n.\nisPresent\n())\n{\nSystem\n.\nout\n.\nprintln\n(\npart\n.\ntext\n()\n.\nget\n());\n}\nelse\nif\n(\npart\n.\ninlineData\n()\n.\nflatMap\n(\nBlob\n::\ndata\n)\n.\nisPresent\n())\n{\nBufferedImage\nimage\n=\nImageIO\n.\nread\n(\nnew\nByteArrayInputStream\n(\npart\n.\ninlineData\n()\n.\nflatMap\n(\nBlob\n::\ndata\n)\n.\nget\n()));\nImageIO\n.\nwrite\n(\nimage\n,\n\"png\"\n,\nnew\nFile\n(\noutputFile\n));\n}\n}\nSystem\n.\nout\n.\nprintln\n(\n\"Content written to: \"\n+\noutputFile\n);\n//\nExample\nresponse\n:\n//\nHere\nis\nthe\nEiffel\nTower\nwith\nfireworks\nin\nthe\nbackground\n...\n//\n//\nContent\nwritten\nto\n:\nresources\n/\noutput\n/\nexample\n-\nimage\n-\neiffel\n-\ntower\n.\npng\n}\n}\n}\nImage understanding\nGemini can understand images as well. The following code uses the image\ngenerated in the previous section and uses a different model to infer\ninformation about the image:\nPython\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nHttpOptions\n,\nPart\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n[\n\"What is shown in this image?\"\n,\nPart\n.\nfrom_uri\n(\nfile_uri\n=\n\"gs://cloud-samples-data/generative-ai/image/scones.jpg\"\n,\nmime_type\n=\n\"image/jpeg\"\n,\n),\n],\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...\nGo\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithTextImage\nshows\nhow\nto\ngenerate\ntext\nusing\nboth\ntext\nand\nimage\ninput\nfunc\ngenerateWithTextImage\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\n\"What is shown in this image?\"\n},\n{\nFileData\n:\n&\ngenai\n.\nFileData\n{\n//\nImage\nsource\n:\nhttps\n:\n//\nstorage\n.\ngoogleapis\n.\ncom\n/\ncloud\n-\nsamples\n-\ndata\n/\ngenerative\n-\nai\n/\nimage\n/\nscones\n.\njpg\nFileURI\n:\n\"gs://cloud-samples-data/generative-ai/image/scones.jpg\"\n,\nMIMEType\n:\n\"image/jpeg\"\n,\n}},\n},\nRole\n:\n\"user\"\n},\n}\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nnil\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nThe\nimage\nshows\nan\noverhead\nshot\nof\na\nrustic\n,\nartistic\narrangement\non\na\nsurface\nthat\n...\nreturn\nnil\n}\nNode.js\nconst\n{\nGoogleGenAI\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'global'\n;\nasync\nfunction\ngenerateContent\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nimage\n=\n{\nfileData\n:\n{\nfileUri\n:\n'gs://cloud-samples-data/generative-ai/image/scones.jpg'\n,\nmimeType\n:\n'image/jpeg'\n,\n},\n};\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n'gemini-2.5-flash'\n,\ncontents\n:\n[\nimage\n,\n'What is shown in this image?'\n],\n});\nconsole\n.\nlog\n(\nresponse\n.\ntext\n);\nreturn\nresponse\n.\ntext\n;\n}\nJava\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.Content\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.HttpOptions\n;\nimport\ncom.google.genai.types.Part\n;\npublic\nclass\nTextGenerationWithTextAndImage\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\nmodelId\n=\n\"gemini-2.5-flash\"\n;\ngenerateContent\n(\nmodelId\n);\n}\n//\nGenerates\ntext\nwith\ntext\nand\nimage\ninput\npublic\nstatic\nString\ngenerateContent\n(\nString\nmodelId\n)\n{\n//\nInitialize\nclient\nthat\nwill\nbe\nused\nto\nsend\nrequests\n.\nThis\nclient\nonly\nneeds\nto\nbe\ncreated\n//\nonce\n,\nand\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nhttpOptions\n(\nHttpOptions\n.\nbuilder\n()\n.\napiVersion\n(\n\"v1\"\n)\n.\nbuild\n())\n.\nbuild\n())\n{\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\nContent\n.\nfromParts\n(\nPart\n.\nfromText\n(\n\"What is shown in this image?\"\n),\nPart\n.\nfromUri\n(\n\"gs://cloud-samples-data/generative-ai/image/scones.jpg\"\n,\n\"image/jpeg\"\n)),\nnull\n);\nSystem\n.\nout\n.\nprint\n(\nresponse\n.\ntext\n());\n//\nExample\nresponse\n:\n//\nThe\nimage\nshows\na\nflat\nlay\nof\nblueberry\nscones\narranged\non\nparchment\npaper\n.\nThere\nare\n...\nreturn\nresponse\n.\ntext\n();\n}\n}\n}\nCode execution\nThe Gemini API in Vertex AI code execution feature enables the model to generate and\nrun Python code and learn iteratively from the results until it arrives at a\nfinal output. Vertex AI provides code execution as a tool, similar to\nfunction calling. You can use this code execution capability to build\napplications that benefit from code-based reasoning and that produce text\noutput. For example:\nPython\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\n(\nHttpOptions\n,\nTool\n,\nToolCodeExecution\n,\nGenerateContentConfig\n,\n)\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n))\nmodel_id\n=\n\"gemini-2.5-flash\"\ncode_execution_tool\n=\nTool\n(\ncode_execution\n=\nToolCodeExecution\n())\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\nmodel_id\n,\ncontents\n=\n\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\"\n,\nconfig\n=\nGenerateContentConfig\n(\ntools\n=\n[\ncode_execution_tool\n],\ntemperature\n=\n0\n,\n),\n)\nprint\n(\n\"# Code:\"\n)\nprint\n(\nresponse\n.\nexecutable_code\n)\nprint\n(\n\"# Outcome:\"\n)\nprint\n(\nresponse\n.\ncode_execution_result\n)\n# Example response:\n# # Code:\n# def fibonacci(n):\n#     if n <= 0:\n#         return 0\n#     elif n == 1:\n#         return 1\n#     else:\n#         a, b = 0, 1\n#         for _ in range(2, n + 1):\n#             a, b = b, a + b\n#         return b\n#\n# fib_20 = fibonacci(20)\n# print(f'{fib_20=}')\n#\n# # Outcome:\n# fib_20=6765\nGo\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\ngenai\n\"google.golang.org/genai\"\n)\n//\ngenerateWithCodeExec\nshows\nhow\nto\ngenerate\ntext\nusing\nthe\ncode\nexecution\ntool\n.\nfunc\ngenerateWithCodeExec\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nprompt\n:=\n\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\"\ncontents\n:=\n[]\n*\ngenai\n.\nContent\n{\n{\nParts\n:\n[]\n*\ngenai\n.\nPart\n{\n{\nText\n:\nprompt\n},\n},\nRole\n:\n\"user\"\n},\n}\nconfig\n:=\n&\ngenai\n.\nGenerateContentConfig\n{\nTools\n:\n[]\n*\ngenai\n.\nTool\n{\n{\nCodeExecution\n:\n&\ngenai\n.\nToolCodeExecution\n{}},\n},\nTemperature\n:\ngenai\n.\nPtr\n(\nfloat32\n(\n0.0\n)),\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nconfig\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nfor\n_\n,\np\n:=\nrange\nresp\n.\nCandidates\n[\n0\n]\n.\nContent\n.\nParts\n{\nif\np\n.\nText\n!=\n\"\"\n{\nfmt\n.\nFprintf\n(\nw\n,\n\"Gemini:\n%s\n\"\n,\np\n.\nText\n)\n}\nif\np\n.\nExecutableCode\n!=\nnil\n{\nfmt\n.\nFprintf\n(\nw\n,\n\"Language:\n%s\n\\n\n%s\n\\n\n\"\n,\np\n.\nExecutableCode\n.\nLanguage\n,\np\n.\nExecutableCode\n.\nCode\n)\n}\nif\np\n.\nCodeExecutionResult\n!=\nnil\n{\nfmt\n.\nFprintf\n(\nw\n,\n\"Outcome:\n%s\n\\n\n%s\n\\n\n\"\n,\np\n.\nCodeExecutionResult\n.\nOutcome\n,\np\n.\nCodeExecutionResult\n.\nOutput\n)\n}\n}\n//\nExample\nresponse\n:\n//\nGemini\n:\nOkay\n,\nI\ncan\ndo\nthat\n.\nFirst\n,\nI\n'll calculate the 20th Fibonacci number. Then, I need ...\n//\n//\nLanguage\n:\nPYTHON\n//\n//\ndef\nfibonacci\n(\nn\n):\n//\n...\n//\n//\nfib_20\n=\nfibonacci\n(\n20\n)\n//\nprint\n(\nf\n'\n{\nfib_20\n=}\n'\n)\n//\n//\nOutcome\n:\nOUTCOME_OK\n//\nfib_20\n=\n6765\n//\n//\nNow\nthat\nI\nhave\nthe\n20\nth\nFibonacci\nnumber\n(\n6765\n),\nI\nneed\nto\nfind\nthe\nnearest\npalindrome\n.\n...\n//\n...\nreturn\nnil\n}\nNode.js\nconst\n{\nGoogleGenAI\n}\n=\nrequire\n(\n'@google/genai'\n);\nconst\nGOOGLE_CLOUD_PROJECT\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_PROJECT\n;\nconst\nGOOGLE_CLOUD_LOCATION\n=\nprocess\n.\nenv\n.\nGOOGLE_CLOUD_LOCATION\n||\n'global'\n;\nasync\nfunction\ngenerateAndExecuteCode\n(\nprojectId\n=\nGOOGLE_CLOUD_PROJECT\n,\nlocation\n=\nGOOGLE_CLOUD_LOCATION\n)\n{\nconst\nclient\n=\nnew\nGoogleGenAI\n({\nvertexai\n:\ntrue\n,\nproject\n:\nprojectId\n,\nlocation\n:\nlocation\n,\n});\nconst\nresponse\n=\nawait\nclient\n.\nmodels\n.\ngenerateContent\n({\nmodel\n:\n'gemini-2.5-flash'\n,\ncontents\n:\n'Calculate 20th fibonacci number. Then find the nearest palindrome to it.'\n,\nconfig\n:\n{\ntools\n:\n[{\ncodeExecution\n:\n{}}],\ntemperature\n:\n0\n,\n},\n});\nconsole\n.\ndebug\n(\nresponse\n.\nexecutableCode\n);\n//\nExample\nresponse\n:\n//\nCode\n:\n//\nfunction\nfibonacci\n(\nn\n)\n{\n//\nif\n(\nn\n<\n=\n0\n)\n{\n//\nreturn\n0\n;\n//\n}\nelse\nif\n(\nn\n===\n1\n)\n{\n//\nreturn\n1\n;\n//\n}\nelse\n{\n//\nlet\na\n=\n0\n,\nb\n=\n1\n;\n//\nfor\n(\nlet\ni\n=\n2\n;\ni\n<\n=\nn\n;\ni\n++\n)\n{\n//\n[\na\n,\nb\n]\n=\n[\nb\n,\na\n+\nb\n];\n//\n}\n//\nreturn\nb\n;\n//\n}\n//\n}\n//\n//\nconst\nfib20\n=\nfibonacci\n(\n20\n);\n//\nconsole\n.\nlog\n(\n`\nfib20\n=$\n{\nfib20\n}\n`\n);\nconsole\n.\ndebug\n(\nresponse\n.\ncodeExecutionResult\n);\n//\nOutcome\n:\n//\nfib20\n=\n6765\nreturn\nresponse\n.\ncodeExecutionResult\n;\n}\nJava\nimport\ncom.google.genai.Client\n;\nimport\ncom.google.genai.types.GenerateContentConfig\n;\nimport\ncom.google.genai.types.GenerateContentResponse\n;\nimport\ncom.google.genai.types.HttpOptions\n;\nimport\ncom.google.genai.types.Tool\n;\nimport\ncom.google.genai.types.ToolCodeExecution\n;\npublic\nclass\nToolsCodeExecWithText\n{\npublic\nstatic\nvoid\nmain\n(\nString\n[]\nargs\n)\n{\n//\nTODO\n(\ndeveloper\n):\nReplace\nthese\nvariables\nbefore\nrunning\nthe\nsample\n.\nString\nmodelId\n=\n\"gemini-2.5-flash\"\n;\ngenerateContent\n(\nmodelId\n);\n}\n//\nGenerates\ntext\nusing\nthe\nCode\nExecution\ntool\npublic\nstatic\nString\ngenerateContent\n(\nString\nmodelId\n)\n{\n//\nInitialize\nclient\nthat\nwill\nbe\nused\nto\nsend\nrequests\n.\nThis\nclient\nonly\nneeds\nto\nbe\ncreated\n//\nonce\n,\nand\ncan\nbe\nreused\nfor\nmultiple\nrequests\n.\ntry\n(\nClient\nclient\n=\nClient\n.\nbuilder\n()\n.\nlocation\n(\n\"global\"\n)\n.\nvertexAI\n(\ntrue\n)\n.\nhttpOptions\n(\nHttpOptions\n.\nbuilder\n()\n.\napiVersion\n(\n\"v1\"\n)\n.\nbuild\n())\n.\nbuild\n())\n{\n//\nCreate\na\nGenerateContentConfig\nand\nset\ncodeExecution\ntool\nGenerateContentConfig\ncontentConfig\n=\nGenerateContentConfig\n.\nbuilder\n()\n.\ntools\n(\nTool\n.\nbuilder\n()\n.\ncodeExecution\n(\nToolCodeExecution\n.\nbuilder\n()\n.\nbuild\n())\n.\nbuild\n())\n.\ntemperature\n(\n0.0\nF\n)\n.\nbuild\n();\nGenerateContentResponse\nresponse\n=\nclient\n.\nmodels\n.\ngenerateContent\n(\nmodelId\n,\n\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\"\n,\ncontentConfig\n);\nSystem\n.\nout\n.\nprintln\n(\n\"Code:\n\\n\n\"\n+\nresponse\n.\nexecutableCode\n());\nSystem\n.\nout\n.\nprintln\n(\n\"Outcome:\n\\n\n\"\n+\nresponse\n.\ncodeExecutionResult\n());\n//\nExample\nresponse\n//\nCode\n:\n//\ndef\nfibonacci\n(\nn\n):\n//\nif\nn\n<\n=\n0\n:\n//\nreturn\n0\n//\nelif\nn\n==\n1\n:\n//\nreturn\n1\n//\nelse\n:\n//\na\n,\nb\n=\n1\n,\n1\n//\nfor\n_\nin\nrange\n(\n2\n,\nn\n):\n//\na\n,\nb\n=\nb\n,\na\n+\nb\n//\nreturn\nb\n//\n//\nfib_20\n=\nfibonacci\n(\n20\n)\n//\nprint\n(\nf\n'\n{\nfib_20\n=}\n'\n)\n//\n//\nOutcome\n:\n//\nfib_20\n=\n6765\nreturn\nresponse\n.\nexecutableCode\n();\n}\n}\n}\nFor more examples of code execution, check out the\ncode execution\ndocumentation\n.\nWhat's next\nNow that you made your first API request, you might want to explore the\nfollowing guides that show how to set up more advanced Vertex AI\nfeatures for production code:\nGoogle Gen AI libraries\nOpenAI compatibility\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 26769,
      "line_count": 3386
    },
    {
      "filename": "vertex-ai_generative-ai_docs_use-provisioned-throughput.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput",
      "title": "Technology areas",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput\n================================================================================\n\nHome\nTechnology areas\nGenerative AI on Vertex AI\nDocumentation\nSend feedback\nUse Provisioned Throughput\nStay organized with collections\nSave and categorize content based on your preferences.\nThis page explains how Provisioned Throughput works, how to control\noverages or bypass Provisioned Throughput, and how to monitor usage.\nHow Provisioned Throughput works\nThis section explains how Provisioned Throughput works by using\nquota checking through the quota enforcement period.\nProvisioned Throughput quota checking\nYour Provisioned Throughput maximum quota is a multiple of the\nnumber of generative AI scale units (GSUs) purchased and the throughput per GSU.\nIt's checked each time you make a request within your\nquota enforcement\nperiod\n, which is how frequently the maximum Provisioned Throughput\nquota is enforced.\nAt the time a request is received, the true response size is unknown. Because we\nprioritize speed of response for real-time applications,\nProvisioned Throughput estimates the output token size. If the\ninitial estimate exceeds the available Provisioned Throughput\nmaximum quota, the request is processed as pay-as-you-go. Otherwise, it is\nprocessed as Provisioned Throughput. This is done by comparing the\ninitial estimate to your Provisioned Throughput maximum quota.\nWhen the response is generated and the true output token size is known, actual\nusage and quota are reconciled by adding the difference between the estimate and\nthe actual usage to your available Provisioned Throughput quota\namount.\nProvisioned Throughput quota enforcement period\nFor Gemini models, the\nquota enforcement period can take up to 30 seconds and is subject to change.\nThis means that you might temporarily experience prioritized traffic that\nexceeds your quota amount on a per-second basis in some cases, but you shouldn't\nexceed your quota on a 30-second basis. These periods are based on the\nVertex AI internal clock time and are independent of when requests are\nmade.\nFor example, if you purchase one GSU of\ngemini-2.0-flash-001\n, then you\nshould expect 3,360 tokens per second of always-on throughput. On average, you\ncan't exceed 100,800 tokens on a 30-second basis, which is calculated using\nthe following formula:\n3,360 tokens per second * 30 seconds = 100,800 tokens\nIf, in a day, you submitted only one request that consumed 8,000 tokens in a\nsecond, it might still be processed as a Provisioned Throughput\nrequest, even though you exceeded your 3,360 tokens per second limit at the time\nof the request. This is because the request didn't exceed the threshold of\n100,800 tokens per 30 seconds.\nControl overages or bypass Provisioned Throughput\nUse the API to control overages when you exceed your purchased throughput\nor to bypass Provisioned Throughput on a per-request basis.\nRead through each option to determine what you must do to meet your use case.\nDefault behavior\nIf you exceed your purchased amount of throughput, the overages go to on-demand\nand are billed at the\npay-as-you-go rate\n. After your\nProvisioned Throughput order is active, the default behavior takes place\nautomatically. You don't have to change your code to begin consuming your\norder as long as you are consuming it in the region provisioned.\nUse only Provisioned Throughput\nIf you are managing costs by avoiding on-demand charges, use only\nProvisioned Throughput. Requests which exceed the\nProvisioned Throughput order amount return an\nerror\n429\n.\nWhen sending requests to the API, set the\nX-Vertex-AI-LLM-Request-Type\nHTTP\nheader to\ndedicated\n.\nUse only pay-as-you-go\nThis is also referred to as using on-demand. Requests bypass the Provisioned Throughput\norder and are sent directly to pay-as-you-go. This might be useful\nfor experiments or applications that are in development.\nWhen sending requests to the API, set the\nX-Vertex-AI-LLM-Request-Type\nHTTP header to\nshared\n.\nExample\nPython\nInstall\npip install --upgrade google-genai\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nfrom\ngoogle\nimport\ngenai\nfrom\ngoogle.genai.types\nimport\nHttpOptions\nclient\n=\ngenai\n.\nClient\n(\nhttp_options\n=\nHttpOptions\n(\napi_version\n=\n\"v1\"\n,\nheaders\n=\n{\n# Options:\n# - \"dedicated\": Use Provisioned Throughput\n# - \"shared\": Use pay-as-you-go\n# https://cloud.google.com/vertex-ai/generative-ai/docs/use-provisioned-throughput\n\"X-Vertex-AI-LLM-Request-Type\"\n:\n\"shared\"\n},\n)\n)\nresponse\n=\nclient\n.\nmodels\n.\ngenerate_content\n(\nmodel\n=\n\"gemini-2.5-flash\"\n,\ncontents\n=\n\"How does AI work?\"\n,\n)\nprint\n(\nresponse\n.\ntext\n)\n# Example response:\n# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n#\n# Here's a simplified overview:\n# ...\nGo\nLearn how to install or update the\nGo\n.\nTo learn more, see the\nSDK reference documentation\n.\nSet environment variables to use the Gen AI SDK with Vertex AI:\n# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values\n# with appropriate values for your project.\nexport\nGOOGLE_CLOUD_PROJECT\n=\nGOOGLE_CLOUD_PROJECT\nexport\nGOOGLE_CLOUD_LOCATION\n=\nglobal\nexport\nGOOGLE_GENAI_USE_VERTEXAI\n=\nTrue\nimport\n(\n\"context\"\n\"fmt\"\n\"io\"\n\"net/http\"\n\"google.golang.org/genai\"\n)\n//\ngenerateText\nshows\nhow\nto\ngenerate\ntext\nProvisioned\nThroughput\n.\nfunc\ngenerateText\n(\nw\nio\n.\nWriter\n)\nerror\n{\nctx\n:=\ncontext\n.\nBackground\n()\nclient\n,\nerr\n:=\ngenai\n.\nNewClient\n(\nctx\n,\n&\ngenai\n.\nClientConfig\n{\nHTTPOptions\n:\ngenai\n.\nHTTPOptions\n{\nAPIVersion\n:\n\"v1\"\n,\nHeaders\n:\nhttp\n.\nHeader\n{\n//\nOptions\n:\n//\n-\n\"dedicated\"\n:\nUse\nProvisioned\nThroughput\n//\n-\n\"shared\"\n:\nUse\npay\n-\nas\n-\nyou\n-\ngo\n//\nhttps\n:\n//\ncloud\n.\ngoogle\n.\ncom\n/\nvertex\n-\nai\n/\ngenerative\n-\nai\n/\ndocs\n/\nuse\n-\nprovisioned\n-\nthroughput\n\"X-Vertex-AI-LLM-Request-Type\"\n:\n[]\nstring\n{\n\"shared\"\n},\n},\n},\n})\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to create genai client: %w\"\n,\nerr\n)\n}\nmodelName\n:=\n\"gemini-2.5-flash\"\ncontents\n:=\ngenai\n.\nText\n(\n\"How does AI work?\"\n)\nresp\n,\nerr\n:=\nclient\n.\nModels\n.\nGenerateContent\n(\nctx\n,\nmodelName\n,\ncontents\n,\nnil\n)\nif\nerr\n!=\nnil\n{\nreturn\nfmt\n.\nErrorf\n(\n\"failed to generate content: %w\"\n,\nerr\n)\n}\nrespText\n:=\nresp\n.\nText\n()\nfmt\n.\nFprintln\n(\nw\n,\nrespText\n)\n//\nExample\nresponse\n:\n//\nArtificial\nIntelligence\n(\nAI\n)\nisn\n't magic, nor is it a single \"thing.\" Instead, it'\ns\na\nbroad\nfield\nof\ncomputer\nscience\nfocused\non\ncreating\nmachines\nthat\ncan\nperform\ntasks\nthat\ntypically\nrequire\nhuman\nintelligence\n.\n//\n.....\n//\nIn\nSummary\n:\n//\n...\nreturn\nnil\n}\nREST\nAfter you\nset up your environment\n,\nyou can use REST to test a text prompt. The following sample sends a request to the publisher\nmodel endpoint.\ncurl\n-X\nPOST\n\\\n-H\n\"Authorization: Bearer\n$(\ngcloud\nauth\nprint-access-token\n)\n\"\n\\\n-H\n\"Content-Type: application/json\"\n\\\n-H\n\"X-Vertex-AI-LLM-Request-Type: dedicated\"\n\\\n# Options: dedicated, shared\n$URL\n\\\n-d\n'{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"Hello.\"}]}]}'\nUse Provisioned Throughput with an API Key\nIf you've purchased Provisioned Throughput for a specific project,\nGoogle model, and region, and want to use it to send a request with an API key,\nthen you must include the project ID, model, location, and API key as parameters\nin your request.\nFor information about how to create a Google Cloud API key bound to a\nservice account, see\nGet a Google Cloud API key\n.\nTo learn how to send requests to the Gemini API using an API key, see\nthe\nGeminiAPI in Vertex AI quickstart\n.\nFor example, the following sample shows\nhow to submit a request with an API key while using Provisioned Throughput:\nREST\nAfter you\nset up your environment\n,\nyou can use REST to test a text prompt. The following sample sends a request to the publisher\nmodel endpoint.\ncurl\n\\\n-X\nPOST\n\\\n-H\n\"Content-Type: application/json\"\n\\\n\"https://aiplatform.googleapis.com/v1/projects/\nPROJECT_ID\n/locations/\nLOCATION\n/publishers/google/models/\nMODEL_ID\n:generateContent?key=\nYOUR_API_KEY\n\"\n\\\n-d\n$'{\n\"contents\": [\n{\n\"role\": \"user\",\n\"parts\": [\n{\n\"text\": \"Explain how AI works in a few words\"\n}\n]\n}\n]\n}'\nMonitor Provisioned Throughput\nYou can self-monitor your Provisioned Throughput usage using a set\nof metrics that are measured on the\naiplatform.googleapis.com/PublisherModel\nresource type.\nProvisioned Throughput traffic monitoring is a public Preview\nfeature.\nDimensions\nYou can filter on metrics using the following dimensions:\nDimension\nValues\ntype\ninput\noutput\nrequest_type\ndedicated\n: Traffic is processed using Provisioned Throughput.\nspillover\n: Traffic is processed as\npay-as-you-go quota after you exceed your Provisioned Throughput\nquota.\nshared\n: If Provisioned Throughput is active,\nthen traffic is processed as pay-as-you-go quota using the shared\nHTTP header\n.\nIf Provisioned Throughput isn't active, then traffic is\nprocessed as pay-as-you-go, by default.\nPath prefix\nThe path prefix for a metric is\naiplatform.googleapis.com/publisher/online_serving\n.\nFor example, the full path for the\n/consumed_throughput\nmetric is\naiplatform.googleapis.com/publisher/online_serving/consumed_throughput\n.\nMetrics\nThe following Cloud Monitoring metrics are available on the\naiplatform.googleapis.com/PublisherModel\nresource for the\nGemini models. Use the\ndedicated\nrequest types to filter for\nProvisioned Throughput usage.\nMetric\nDisplay name\nDescription\n/dedicated_gsu_limit\nLimit (GSU)\nDedicated limit in GSUs. Use this metric to understand your Provisioned Throughput maximum quota in GSUs.\n/tokens\nTokens\nInput and output token count distribution.\n/token_count\nToken count\nAccumulated input and output token count.\n/consumed_token_throughput\nToken throughput\nThroughput usage, which accounts for the burndown rate in tokens and incorporates quota reconciliation. See\nProvisioned Throughput quota checking\n.\nUse this metric to understand how your Provisioned Throughput quota was used.\n/dedicated_token_limit\nLimit (tokens per second)\nDedicated limit in tokens per second. Use this metric to understand your Provisioned Throughput maximum quota for token-based models.\n/characters\nCharacters\nInput and output character count distribution.\n/character_count\nCharacter count\nAccumulated input and output character count.\n/consumed_throughput\nCharacter throughput\nThroughput usage, which accounts for the burndown rate in characters and incorporates quota reconciliation\nProvisioned Throughput quota checking\n.\nUse this metric to understand how your Provisioned Throughput quota was used.\nFor token-based models, this metric is equivalent to the throughput consumed in tokens multiplied by 4.\n/dedicated_character_limit\nLimit (characters per second)\nDedicated limit in characters per second. Use this metric to understand your Provisioned Throughput maximum quota for character-based models.\n/model_invocation_count\nModel invocation count\nNumber of model invocations (prediction requests).\n/model_invocation_latencies\nModel invocation latencies\nModel invocation latencies (prediction latencies).\n/first_token_latencies\nFirst token latencies\nDuration from request received to first token returned.\nAnthropic models also have a filter for Provisioned Throughput but\nonly for\ntokens\nand\ntoken_count\n.\nDashboards\nDefault monitoring dashboards for Provisioned Throughput provide\nmetrics\nthat let you better understand your usage and\nProvisioned Throughput utilization. To access the dashboards, do the\nfollowing:\nIn the Google Cloud console, go to the\nProvisioned Throughput\npage.\nGo to Provisioned Throughput\nTo view the Provisioned Throughput utilization of each model\nacross your orders, select the\nUtilization summary\ntab.\nIn the\nProvisioned Throughput utilization by model\ntable, you can view\nthe following for the selected time range:\nTotal number of GSUs you had.\nPeak throughput usage in terms of GSUs.\nThe average GSU utilization.\nThe number of times you reached your Provisioned Throughput limit.\nSelect a model from the\nProvisioned Throughput utilization by\nmodel\ntable to see more metrics specific to the selected model.\nLimitations of the dashboard\nThe dashboard might display unexpected results, especially for fluctuating\ntraffic that's either spiky or infrequent (for example, less than 1 query per\nsecond). The following reasons might contribute to those results:\nTime ranges that are larger than 12 hours can lead to a less accurate\nrepresentation of the quota enforcement period. Throughput metrics\nand their derivatives, such as utilization, display averages across alignment\nperiods that are based on the selected time range. When the time range\nexpands, each alignment period also expands. The alignment period expands\nacross the calculation of the average usage. Because quota enforcement is\ncalculated at a sub-minute level, setting the time range to a period of 12\nhours or less results in minute-level data that is more comparable to the\nactual quota enforcement period. For more information on alignment periods,\nsee\nAlignment: within-series\nregularization\n. For more\ninformation about time ranges, see\nRegularizing time\nintervals\n.\nIf multiple requests were submitted at the same time, monitoring aggregations\nmight impact your ability to filter down to specific requests.\nProvisioned Throughput throttles traffic when a request was made\nbut reports usage metrics after the quota is reconciled.\nProvisioned Throughput quota enforcement periods are independent\nfrom and might not align with monitoring aggregation periods or\nrequest-or-response periods.\nIf no errors occurred, you might see an error message within the error rate\nchart. For example,\nAn error occurred requesting data. One or more resources\ncould not be found.\nMonitor Genmedia models\nProvisioned Throughput monitoring isn't available on\nVeo 3 and Imagen models.\nAlerting\nAfter alerting is enabled, set default alerts to help you manage your traffic\nusage.\nEnable alerts\nTo enable alerts in the dashboard, do the following:\nIn the Google Cloud console, go to the\nProvisioned Throughput\npage.\nGo to Provisioned Throughput\nTo view the Provisioned Throughput utilization of each model\nacross your orders, select the\nUtilization summary\ntab.\nSelect\nRecommended alerts\n, and the following alerts display:\nProvisioned Throughput Usage Reached Limit\nProvisioned Throughput Utilization Exceeded 80%\nProvisioned Throughput Utilization Exceeded 90%\nCheck the alerts that help you manage your traffic.\nView more alert details\nTo view more information about alerts, do the following:\nGo to the\nIntegrations\npage.\nGo to Integrations\nEnter\nvertex\ninto the\nFilter\nfield and press\nEnter\n.\nGoogle\nVertex AI\nappears.\nTo view more information, click\nView details\n. The\nGoogle\nVertex AI details\npane displays.\nSelect\nAlerts\ntab, and you can select an\nAlert Policy\ntemplate.\nWhat's next\nTroubleshoot\nError code\n429\n.\nSend feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-11-07 UTC.",
      "content_length": 15376,
      "line_count": 736
    },
    {
      "filename": "vertex-ai_generative-ai_pricing.txt",
      "url": "https://docs.cloud.google.com/vertex-ai/generative-ai/pricing",
      "title": "Cost of building and deploying AI models in Vertex AI",
      "content": "URL: https://docs.cloud.google.com/vertex-ai/generative-ai/pricing\n================================================================================\n\nCost of building and deploying AI models in Vertex AI\nPrices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\nYou're charged only for requests that return a 200 response code. Requests returning any other response codes, such as 4xx and 5xx codes, aren't charged for the input or output.\nThis page covers pricing for Generative AI on Vertex AI. For all other Vertex AI pricing including\nML Platform and MLOps services please refer to\nVertex AI pricing page\n.\nGoogle models\nGemini 2.5\nModel\nType\nPrice (/1M tokens) <= 200K input tokens\nPrice (/1M tokens) > 200K input tokens\nPrice (/1M tokens) <= 200K\ncached\ninput tokens\nPrice (/1M tokens) > 200K\ncached\ninput tokens\nPrice (/1M tokens) <= 200K input tokens with batch API\nPrice (/1M tokens) > 200K input tokens with batch API\nGemini 2.5 Pro\nInput (text, image, video, audio)\n$1.25\n$2.5\n$0.125\n$0.250\n$0.625\n$1.25\nText output (response and reasoning)\n$10\n$15\nN/A\nN/A\n$5\n$7.5\nGemini 2.5 Pro\nComputer Use-Preview\nInput (text, image, video, audio)\n$1.25\n$2.5\nN/A\nN/A\nN/A\nN/A\nText output (response and reasoning)\n$10.00\n$15.00\nN/A\nN/A\nN/A\nN/A\nGemini 2.5\nFlash\nInput (text, image, video)\n$0.30\n$0.30\n$0.030\n$0.030\n$0.15\n$0.15\nAudio Input\n$1\n$1\n$0.100\n$0.100\n$0.5\n$0.5\nText output (response and reasoning)\n$2.50\n$2.50\nN/A\nN/A\n$1.25\n$1.25\nImage output***\n$30\n$30\nN/A\nN/A\n$15\n$15\nTuning for 1M training tokens\n$5.00\nN/A\nN/A\nN/A\nN/A\nN/A\nGemini 2.5 Flash Live API\n1M input text tokens\n$0.5\n$0.5\nN/A\nN/A\nN/A\nN/A\n1M input audio tokens\n$3\n$3\nN/A\nN/A\nN/A\nN/A\n1M input video/image tokens\n$3\n$3\nN/A\nN/A\nN/A\nN/A\n1M output text tokens\n$2\n$2\nN/A\nN/A\nN/A\nN/A\n1M output audio tokens\n$12\n$12\nN/A\nN/A\nN/A\nN/A\nGemini 2.5 Flash Lite\nInput (text, image, video)\n$0.1\n$0.1\n$0.010\n$0.010\n$0.05\n$0.05\nAudio Input\n$0.3\n$0.3\n$0.030\n$0.030\n$0.15\n$0.15\nText output (response and reasoning)\n$0.4\n$0.4\nN/A\nN/A\n$0.2\n$0.2\nGrounding with Google Search\nGemini 2.0 Flash\n,\n2.5 Flash\nand\n2.5 Flash-Lite\ninclude a combined 1,500 grounded prompts per day at no additional charge.\nGemini 2.5 Pro\nincludes 10,000 grounded prompts per day at no additional charge.\nGrounded prompts exceeding those limits are billed at\n$35 per 1,000 grounded prompts\n.\nA grounded prompt is a request submitted to Gemini that makes one or more queries to Google Search**. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nWeb Grounding for enterprise\n$45 per 1,000 grounded prompts\n. A grounded prompt is a request submitted to Gemini that makes one or more queries to Web Grounding for enterprise**. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nGrounding with your data\n$2.5 per 1,000 requests.\nGrounding with Google Maps\nGrounded prompts exceeding those limits are billed at\n$25 per 1,000 grounded prompts\n.\nOne grounded prompt is a request sent to Gemini that makes at least 1 query to Google Maps.\n* If a query input context is longer than 200K tokens, all tokens (input and output) are charged at long context rates.\n** Grounding with Google Search and Web Grounding for enterprise is billed only when a prompt successfully returns web results (i.e., results containing at least one grounding support URL from the web). Gemini model usage fees apply separately.\n*** A 1024x1024 image consumes 1290 tokens. Per image token count varies by image resolution. For more information on how to calculate tokens, you can refer to our\ndocumentation\n.\n**** Computer Use billing uses the Gemini 2.5 Pro SKU, to split out Computer Use costs, apply billing tags. See\nmore\nhere\n.\nLiveAPI Session's Context Window billing explained\n: You are charged per turn for all tokens present in the Session Context Window. The Session Context Window includes new tokens (current turn) + all accumulated tokens from previous turns. This means tokens from past turns are re-processed and accounted for in each new turn, up to your configured context window size. A \"turn\" is one user input and the model's response.\nProactive Audio Mode\n: When enabled, input tokens are charged while LiveAPI is listening. Output tokens are only charged when the API responds.\nWhen audio to text transcription is enabled, all text tokens generated for transcription are charged at the text token output rate.\nGemini 2.0\nGemini 2.0 is billed based on tokens. To calculate the number of input tokens in\nyour request prior to sending the request, you can use the\nSDK\ntokenizer\nor the\ncountTokens API\n.\nIf your request fails with a 400 or 500 error, you won't be charged for the\ntokens used.\nUse the toggle in the pricing table to compare token-based pricing and\nmodality-based pricing.\nToken-based pricing\nModel\nType\nPrice\nPrice with Batch API\nGemini 2.0 Flash\n1M Input tokens\n$0.15\n$0.075\n1M Input audio tokens\n$1.00\n$0.50\n1M Output text tokens\n$0.60\n$0.30\nTuning for 1M training tokens\n$3.00\nGemini 2.0 Flash Image Generation\n1M input tokens\n$0.15\n1M input audio tokens\n$1.00\n1M input video tokens\n$3\n1M output text tokens\n$0.60\n1M output image tokens\n$30.00\nGemini 2.0 Flash Live API\n1M input text tokens\n$0.5\n1M input audio tokens\n$3\n1M input video/image tokens\n$3\n1M output text tokens\n$2\n1M output audio tokens\n$12\nGemini 2.0 Flash Lite\n1M Input tokens\n$0.075\n$0.0375\n1M Input audio tokens\n$0.075\n$0.0375\n1M Output text tokens\n$0.30\n$0.15\nTuning for 1M training tokens\n$1.00\nGrounding with Google Search\nGemini 2.0 Flash\nand\n2.5 Flash\ninclude a combined 1,500 grounded prompts per day at no additional charge.\nGrounded prompts exceeding those limits are billed at\n$35 per 1,000 grounded prompts\n.\nA grounded prompt is a request submitted to Gemini that makes one or more queries to Google Search*. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nWeb Grounding for enterprise\n$45 per 1,000 grounded prompts\n. A grounded prompt is a request submitted to Gemini that makes one or more queries to Web Grounding for enterprise*. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nGrounding with your data\n$2.5 per 1,000 requests starting June 16, 2025.\nGrounding with Google Maps\nGemini models include a number of daily grounded prompts at no extra cost:\nGemini Flash\nand\nFlash-Lite\n: combined 1,500 grounded prompts per day.\nGemini Pro\n: 10,000 grounded prompts per day.\nGrounded prompts exceeding those limits are billed at\n$25 per 1,000 grounded prompts\n.\nOne grounded prompt is a request sent to Gemini that makes at least 1 query to Google Maps.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nModality-based pricing\nThe below modality pricing is based on average use cases for reference only. Actual billing will only be based on tokens:\n4 characters result in approximately 1 text token including white space.\nFor an 1024x1024 image, it consumes 1290 tokens. Per image token count varies by image resolution. For more information on how to calculate tokens, you can refer to our\ndocumentation\n.\nVideo input consumes 258 tokens per second at the sample rate of one frame per second. Video with audio bills for both video tokens and audio tokens.\nAudio input consumes 25 tokens per second without timestamp.\nModel\nType\nPrice\nPrice with Batch API\nGemini 2.0 Flash\nInput text ($/M char)\n$0.0375\n$0.01875\nInput image ($/image)\n$0.0001935\n$0.00009675\nInput video ($/sec)\n$0.0000387\n$0.00001935\nInput audio ($/sec)\n$0.000025\n$0.0000125\nOutput text ($/M char)\n$0.15\n$0.075\nGemini 2.0 Flash Image Generation\nInput text ($/M char)\n$0.0375\nInput image ($/image)\n$0.0001935\nInput video ($/sec)\n$0.0000387\nInput audio ($/sec)\n$0.000025\nOutput text ($/M char)\n$0.15\nOutput image image ($/image)\n$0.04\nGemini 2.0 Flash Lite\nInput text ($/M char)\n$0.01875\n$0.009375\nInput image ($/image)\n$0.00009675\n$0.000048375\nInput video ($/sec)\n$0.00001935\n$0.000009675\nInput audio ($/sec)\n$0.000001875\n$0.000000938\nOutput text ($/M char)\n$0.075\n$0.0375\nGrounding with Google Search\nGemini 2.0 Flash\nand\n2.5 Flash\ninclude a combined 1,500 grounded prompts per day at no additional charge.\nGrounded prompts exceeding those limits are billed at\n$35 per 1,000 grounded prompts\n.\nA grounded prompt is a request submitted to Gemini that makes one or more queries to Google Search*. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nWeb Grounding for enterprise\n$45 per 1,000 grounded prompts\n. A grounded prompt is a request submitted to Gemini that makes one or more queries to Web Grounding for enterprise*. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\n* Prices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\n* Training tokens are calculated by the total number of tokens in your training dataset, multiplied by your number of epochs.\n* PDFs are billed as image input, with one PDF page equivalent to one image.\n* Tuned model endpoint has the same prediction price as the base model.\n*  Grounding with Google Search and Web Grounding for enterprise is billed only when a prompt successfully returns web results (i.e., results containing at least one grounding support URL from the web). Gemini model usage fees apply separately.\n* Gemini 2.0 Flash Live API: 25 tokens per second of audio (input/output), 258 tokens per second of video (input). Grounding with Google Search remains free of charge while Gemini 2.0 Flash Live API is in Preview.\nLiveAPI Session's Context Window billing explained\n: You are charged per turn for all tokens present in the Session Context Window. The Session Context Window includes new tokens (current turn) + all accumulated tokens from previous turns. This means tokens from past turns are re-processed and accounted for in each new turn, up to your configured context window size. A \"turn\" is one user input and the model's response.\nWhen audio to text transcription is enabled, all text tokens generated for transcription are charged at the text token output rate.\nVertex AI Model Optimizer Pricing  (Experimental)*\nVertex AI Model Optimizer simplifies use of Gemini for enterprise customers by providing a single meta-endpoint for Gemini model requests--customers using this service do not have to specify whether to use Flash, Pro, or a specific version. Instead they simply provide a configurable setting (cost, quality, or balance) to indicate their preferences, and Model Optimizer applies the right level of intelligence appropriate for the task by sending each query to the best fit model.\nVertex AI Model Optimizer applies dynamic pricing. This means that the average price per token is dependent on the model intelligence level applied to complete the task. For this reason, pricing examples are provided below to illustrate likely scenarios based on your configuration setting (see tables below). Model Optimizer SKUs are $1 skus that function as a purchasing unit to apply for your billing, you are still billed on a consumption basis after you have used the models.\n5:1 I/O ratio\nExample 1\nchat bot\nNOTE: these ranges are not guarantees, individual customer results may vary\nCustomer Preference\nCustomer Input Tokens Sent to MO\nCustomer Output Tokens Sent to MO\nAverage Input Price per Million Tokens (High Range)\nAverage Output Price per Million Tokens (High Range)\nAverage Input Price per Million Tokens (Low Range)\nAverage Output Price per Million Tokens (Low Range)\nCost\n10,000,000\n2,000,000\n$0.63\n$2.50\n$0.16\n$0.63\nBalanced\n10,000,000\n2,000,000\n$1.26\n$5.00\n$0.63\n$2.50\nQuality\n10,000,000\n2,000,000\n$1.89\n$7.50\n$1.26\n$5.00\n1:20  I/O ratio\nExample 2 Content generation\nCustomer Preference\nCustomer Input Tokens Sent to MO\nCustomer Output Tokens Sent to MO\nAverage Input Price per Million Tokens (High Range)\nAverage Output Price per Million Tokens (High Range)\nAverage Input Price per Million Tokens (Low Range)\nAverage Output Price per Million Tokens (Low Range)\nCost\n1,000,000\n20,000,000\n$0.63\n$2.50\n$0.16\n$0.63\nBalanced\n1,000,000\n20,000,000\n$1.26\n$5.00\n$0.63\n$2.50\nQuality\n1,000,000\n20,000,000\n$1.89\n$7.50\n$1.26\n$5.00\n* Model Optimizer is a paid experimental offering, and may route requests to experimental versions of Gemini on Vertex.\nOther Gemini models\nAll Gemini models other than Gemini 2.0 or Gemini 2.5 are billed based on modalities such as\ncharacters, images, video/audio seconds. Text input is charged by every 1,000\ncharacters of input (prompt) and every 1,000 characters of output (response).\nCharacters are counted by UTF-8 code points and white space is excluded from the\ncount, resulting in approximately 4 characters per token. Prediction requests\nthat lead to filtered responses are charged for the input only. At the end of\neach billing cycle, fractions of one cent ($0.01) are rounded to one cent. Media\ninput is charged per image or per second (video). If your request fails with a\n400 or 500 error, you won't be charged for the tokens used.\nModel\nFeature\nType\nPrice\n( =< 128K input tokens)\nPrice\n( > 128K input tokens)\nGemini 1.5 Flash\nMultimodal\nImage Input\nVideo Input\nText Input\nAudio Input\n$0.00002 / image\n$0.00002 / second\n$0.00001875 / 1k characters\n$0.000002 / second\n$0.00004 / image\n$0.00004 / second\n$0.0000375 / 1k characters\n$0.000004 / second\nText Output\n$0.000075 / 1k characters\n$0.00015 / 1k characters\nTuning*\nTraining Token\n$8 / M tokens\nGemini 1.5 Pro\nMultimodal\nImage Input\nVideo Input\nText Input\nAudio Input\n$0.00032875 / image\n$0.00032875 / second\n$0.0003125 / 1k characters\n$0.00003125 / second\n$0.0006575 / image\n$0.0006575 / second\n$0.000625 / 1k characters\n$0.0000625 / second\nText Output\n$0.00125 / 1k characters\n$0.0025 / 1k characters\nTuning*\nTraining Token\n$80 / M tokens\nGemini 1.0 Pro\nMultimodal\nImage Input\nVideo Input\nText Input\n$0.0025 / image\n$0.002 / second\n$0.000125 / 1k characters\nText Output\n$0.000375 / 1k characters\nGrounding with Google Search\nText\n$35 per 1,000 grounded prompts\n.\nA grounded prompt is a request submitted to Gemini that makes one or more queries to Google Search*. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nWeb Grounding for enterprise\nText\n$45 per 1,000 grounded prompts\n.\nA grounded prompt is a request submitted to Gemini that makes one or more queries to Web Grounding for enterprise*. Even if multiple search queries are sent to Google Search, there is only one charge for a grounded prompt.\nPlease contact your account team if you require more than 1 million grounded prompts per day.\nGrounding with your data\nText\n$2.5 per 1,000 requests starting June 16, 2025.\n* Prices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\n* If a query context is longer than 128K, all tokens are charged at long context rates.\n* Gemini models are available in batch mode at 50% discount.\n* Gemini 1.0 Pro only support up to 32K context window.\n* PDFs are billed as image input, with one PDF page equivalent to one image.\n* Tuned model endpoint has the same prediction price as the base model.\n* Grounding with Google Search and Web Grounding for enterprise is billed only when a prompt successfully returns web results (i.e., results containing at least one grounding support URL from the web). Gemini model usage fees apply separately.\nImagen\nWith Imagen on Vertex AI, you can generate novel images and edit images based on text prompts you provide, or edit only parts of images using a mask area you define along with a host of other capabilities.\nModel\nFeature\nDescription\nInput\nOutput\nPrice\nImagen 4 Ultra\nImage generation\nGenerate an image\nText prompt\nImage\n$0.06 per image\nImagen 4\nUpscaling\nIncrease resolution of a generated image to 2K, 3K, and 4K\nImage\nImage\n$0.06 per image\nImagen 4\nImage generation\nGenerate an image\nText prompt\nImage\n$0.04 per image\nImagen 4 Fast\nImage generation\nGenerate an image\nText prompt\nImage\n$0.02 per image\nImagen 3\nImage generation\nGenerate an image\nEdit an image\nCustomize an image\nText prompt\nImage\n$0.04 per image\nImagen 3 Fast\nImage generation\nGenerate an image\nText prompt\nImage\n$0.02 per image\nImagen 2, Imagen 1\nImage generation\nGenerate an image\nText prompt\nImage\n$0.020 per image\nImagen 2, Imagen 1\nImage editing\nEdit an image using mask free or mask approach\nImage/Text prompt\nImage\n$0.020 per image\nImagen 1\nUpscaling\nIncrease resolution of a generated image to 2k and 4k\nImage\nImage\n$0.003 per image\nImagen 1\nFine-tuning\nEnable a \"subject\" provided by the user to used in Imagen prompts (few shot training)\nSubject(s) with text identifier and 4-8 images per subject\nFine-tuned model (after training with user provided subjects)\n$ per node hour (Vertex AI custom training pricing)\nImagen\nVisual Captioning\nGenerate a short or long text caption for an image\nImage\nText caption\n$0.0015/image\nImagen\nVisual Q&A\nProvide an answer based on a question referencing an image\nImage/Text prompt\nText answer\n$0.0015/image\nImagen\nProduct Recontext\nRe-imagine products in a new scene\n1-3 Images of the same product and a text prompt describing desired scene\nImage\n$0.12 per image\nVertex Virtual Try-On\nCreate images of people wearing different clothes\n1 image of a person and 1 image of clothing\nImage\n$0.06 per image\nPrices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\nVeo\nVeo creates incredibly high-quality videos in a wide range of subjects and styles, bringing an improved understanding of real-world physics and the nuances of human movement and expression.\nModel\nFeature\nDescription\nInput\nOutput\nOutput Resolution\nPrice\nVeo 3.1\nVideo + Audio generation\nGenerate high-quality videos with synchronized speech/sound effects from a text prompt or reference image\nText/Image prompt\nVideo + Audio\n720p, 1080p\n$0.40/second\nVeo 3.1\nVideo generation\nGenerate high-quality videos from a text prompt or reference image\nText/Image prompt\nVideo\n720p, 1080p\n$0.20/second\nVeo 3.1 Fast\nVideo + Audio generation\nGenerate videos with synchronized speech/sound effects from a text prompt or reference image faster\nText/Image prompt\nVideo + Audio\n720p, 1080p\n$0.15/second\nVeo 3.1 Fast\nVideo generation\nGenerate videos from a text prompt or reference image faster\nText/Image prompt\nVideo\n720p, 1080p\n$0.10/second\nVeo 3\nVideo + Audio generation\nGenerate high-quality videos with synchronized speech/sound effects from a text prompt or reference image\nText/Image prompt\nVideo + Audio\n720p, 1080p\n$0.40/second\nVeo 3\nVideo generation\nGenerate high-quality videos from a text prompt or reference image\nText/Image prompt\nVideo\n720p, 1080p\n$0.20/second\nVeo 3 Fast\nVideo + Audio generation\nGenerate videos with synchronized speech/sound effects from a text prompt or reference image faster\nText/Image prompt\nVideo + Audio\n720p, 1080p\n$0.15/second\nVeo 3 Fast\nVideo generation\nGenerate videos from a text prompt or reference image faster\nText/Image prompt\nVideo\n720p, 1080p\n$0.10/second\nVeo 2\nVideo generation\nGenerate videos from a text prompt or reference image\nText/Image prompt\nVideo\n720p\n$0.50/second\nVeo 2\nAdvanced Controls\nGenerate videos through start and end frame interpolation, extend generated videos, and apply camera controls\nText/Image/Video prompt\nVideo\n720p\n$0.50/second\nLyria\nLyria 2 offers high-quality instrumental music generation that is ideal for sophisticated composition and detailed creative exploration where nuanced output is key.\nModel\nFeature\nDescription\nInput\nOutput\nPrice\nLyria 2\nMusic generation\nGenerate music from a text prompt\nText prompt\nMusic\n$0.06 per 30 seconds\nUnderstand embedding costs for your AI applications\nModel\nType\nRegion\nPrice per 1,000 input tokens\nGemini Embedding\nInput\nGlobal\nOnline requests: $0.00015\nBatch requests: $0.00012\nOutput\nGlobal\nOnline requests: No charge\nBatch requests: No charge\nModel\nType\nRegion\nPrice per 1,000 characters\nEmbeddings for Text\n(Excluding Gemini Embedding)\nInput\nGlobal\nOnline requests: $0.000025\nBatch requests: $0.00002\nOutput\nGlobal\nOnline requests: No charge\nBatch requests: No charge\nModel\nFeature\nDescription\nInput\nOutput\nPrice\nmultimodalembedding\nEmbeddings for Multimodal: Text\nGenerate embeddings using text as an input\nText\nEmbeddings\n$0.0002 / 1k characters input\nEmbeddings for Multimodal: Image\nGenerate embeddings using image as an input\nImage\nEmbeddings\n$0.0001 / image input\nEmbeddings for Multimodal: Video Plus\nVideo Plus\nVideo\nEmbeddings (up to 15 embeddings per min of video)\n$0.0020 per second of video\nEmbeddings for Multimodal: Video Standard\nVideo Standard\nVideo\nEmbeddings (up to 8 embeddings per min of video)\n$0.0010 per second of video\nEmbeddings for Multimodal: Video Essential\nVideo Essential\nVideo\nEmbeddings (up to 4 embeddings per min of video)\n$0.0005 per second of video\nOpen Source Model\nType\nPrice per 1,000 input tokens\nmultilingual-e5-small\nInput:\nOutput:\nBatch Input:\nBatch Output:\nOnline requests: $0.000015\nOnline requests: No charge\nBatch requests: $0.0000075\nBatch requests: No charge\nmultilingual-e5-large\nInput:\nOutput:\nBatch Input:\nBatch Output:\nOnline requests: $0.000025\nOnline requests: No charge\nBatch requests: $0.0000125\nBatch requests: No charge\nPrices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\nPricing for vertex AI's code completion\nGenerative AI on Vertex AI charges by every 1,000 characters of input (prompt)\nand every 1,000 characters of output (response). Characters are counted by UTF-8\ncode points and white space is excluded from the count. During the Preview\nstage, charges are 100% discounted. Prediction requests that lead to filtered\nresponses are charged for the input only. At the end of each billing cycle,\nfractions of one cent ($0.01) are rounded to one cent.\nModel\nType\nRegion\nPrice per 1,000 characters\nCodey for Code Completion\nInput\nGlobal\nOnline requests: $0.00025\nOutput\nGlobal\nOnline requests: $0.0005\nPrices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\nTranslation (Text)\nUse the Vertex AI API and Translation LLM to translate text. LLM translations tend to be more fluent and human sounding than classic translation models, but have more limited language support\n(Learn More)\n.\nModel\nMethod\nUsage\nPrice per million characters\nLLM\nText\ntranslation\n*\nThe number of input characters per month\n$10 per million characters\n*\nThe number of output characters per month\n$10 per million characters\n*\nPrices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\n*\nPrice is per character processed by the model. For details about counted characters, see\nCharged characters\nContext Cache Storage price for Explicit Caching\nModel\nFeature\nType\nPrice (/1M tokens)\n<= 200K input tokens\nPrice (/1M tokens)\n> 200K input tokens\nGemini 2.5 Pro\nContext Cache Storage\nInput (text, image, video, audio)\n$4.5 (/M Tok/hr)\n$4.5 (/M Tok/hr)\nGemini 2.5 Flash\nContext Cache Storage\nInput (text, image, video, audio)\n$1 (/M Tok/hr)\n$1 (/M Tok/hr)\nGemini 2.5 Flash Lite\nContext Cache Storage\nInput (text, image, video, audio)\n$1 (/M Tok/hr)\n$1 (/M Tok/hr)\nGemini 2.0 Models\nToken-based pricing\nModel\nType\nStorage\n(M tok-hour)\nPrice\nGemini 2.0 Flash\n1M Input tokens\n$1.00\n$0.0375\n1M Input audio tokens\n$1.00\n$0.25\n1M Output text tokens\nNA\nNA\nGemini 2.0 Flash Lite\n1M Input tokens\n$1.00\n$0.01875\n1M Input audio tokens\n$1.00\n$0.01875\n1M Output text tokens\nNA\nNA\nModality-based pricing\nThe below modality pricing is based on average use cases for reference only. Actual billing will only be based on tokens:\n4 characters result in approximately 1 text token including white space.\nFor an 1024x1024 image, it consumes 1290 tokens. Per image token count varies by image resolution. For more information on how to calculate tokens, you can refer to our\ndocumentation\n.\nVideo input consumes 258 tokens per second at the sample rate of one frame per second. Video with audio bills for both video tokens and audio tokens.\nAudio input consumes 25 tokens per second without timestamp.\nModel\nType\nStorage\n(Modality-hour)\nPrice\nGemini 2.0 Flash\nInput text ($/M char)\n$0.25\n$0.009375\nInput image ($/image)\n$0.00129\n$0.000048375\nInput video ($/sec)\n$0.000258\n$0.000009675\nInput audio ($/sec)\n$0.000025\n$0.00000625\nOutput text ($/M char)\nNA\nNA\nGemini 2.0 Flash Lite\nInput text ($/M char)\n$0.25\n$0.0046875\nInput image ($/image)\n$0.00129\n$0.0000241875\nInput video ($/sec)\n$0.000258\n$0.000009675\nInput audio ($/sec)\n$0.000258\n$0.0000048375\nOutput text ($/M char)\nNA\nNA\nGrounding with Google Search\nGemini 2.0 Flash includes up to 1,500 grounded requests per day at no additional charge. Grounded requests\nexceeding 1,500 per day are billed at $35 per 1,000 requests (up to 1 million requests per day).\nPlease contact your account team if you require more than 1 million requests per day.\nWeb Grounding for enterprise\n$45 per 1,000 request (up to 1 million requests per day) starting May 5, 2025.\nPlease contact your account team if you require more than 1 million requests per day.\n* Prices are listed in US Dollars (USD).\nIf you pay in a currency other than USD, the prices listed in your currency on\nCloud Platform SKUs\napply.\n* PDFs are billed as image input, with one PDF page equivalent to one image.\n* Tuned model endpoint has the same prediction price as the base model.\n* Grounding with Google Search is billed only for requests that return results containing at least one grounding support URL from the web. Standard Gemini model usage fees also apply.\nProvisioned Throughput\nProvisioned throughput\nassures throughput for your generative AI needs and is transacted via\ngenerative AI scale units\n,\nor GSUs. Learn more about how much throughput each GSU provides\nhere\nand use our online estimator\nhere\n.\nDuration\nPrice per GSU\nPer\n1 week commit\n$1,200\nWeek\n1 month commit\n$2,700\nMonth\n3 month commit\n$2,400\nMonth\n1 year commit\n$2,000\nMonth\nExample cost calculation\nA user needs to ensure they can support 10 queries per second (QPS) of a query with input of 1,000 text tokens and 500 audio tokens and receive an output of 300 text tokens using gemini-2.0-flash.\nUsing the throughput and burndown rate\ntable\n,\nfor gemini-2.0-flash we know an input text token's burndown rate is 1 token, an\ninput audio token's burndown rate is 7 tokens, and an output text token's burndown rate is 4 tokens.\nThe user's total input tokens is 1,000* (1 token per input text token) + 500* (7 tokens per input audio token) =  4,500 burndown adjusted input tokens. The user's total output tokens is 300* (4 tokens per output text token) = 1,200 burndown adjusted output tokens. Adding them together gives us 4,500 burndown adjusted input tokens + 1,200 burndown adjusted output tokens = 5,700 total tokens per query.\nMultiplying the total tokens per query by QPS gives us 5,700 total tokens per query * 10 QPS = 57,000 total tokens per second.\nDividing this by the total throughput per second per GSU gives us 57,000 total tokens per second ÷ 3,360 per-second throughput per GSU = 16.96 GSUs. The minimum GSU purchase increment for this model is 1, so the user would need 17 GSUs.\nIf the user wanted to sustain this throughput for 1 week, it would cost $1,200 * 17 GSUs = $20,400 per week. If they wanted to sustain this throughput for 1 month, it would cost $2,700 * 17 GSUs = $45,900 per month. If they wanted to sustain this throughput for 3 months, it would cost $2,400 * 17 GSUs = $40,800 per month. And finally, if they wanted to sustain this throughput for 1 year, it would cost $2,000 * 17 GSUs = $34,000 per month.\nModel Tuning\nModel tuning is an effective way to customize large models to your tasks. It's a key step to improve the model's quality and efficiency. Model tuning provides the following benefits:\nHigher quality for your specific tasks\nIncreased model robustness\nLower inference latency and cost due to shorter prompts\nTuning is charged per million training tokens. Training tokens are calculated by the total number of tokens in your training dataset, multiplied by your number of epochs. For model inference, Gemini tuned model endpoint has the same prediction price as the base model.\nModel\nType\nPrice (/1M training tokens)\nGemini 2.5 Pro\nSupervised fine-tuning\n$25\nGemini 2.5 Flash\nSupervised fine-tuning\n$5\nGemini 2.5 Flash Lite\nSupervised fine-tuning\n$1.5\nGemma 3 27B IT\nSupervised fine-tuning\n$6.83\nLlama 3.1 8B\nSupervised fine-tuning\n$0.67\nLlama 3.2 1B\nSupervised fine-tuning\n$0.28\nLlama 3.2 3B\nSupervised fine-tuning\n$0.61\nLlama 3.3 70B\nSupervised fine-tuning\n$6.72\nLlama 4 Scout 17B 16E\nSupervised fine-tuning\n$5.77\nQwen 3 32B\nSupervised fine-tuning\n$6.57\n* Training tokens are calculated by the total number of tokens in your training dataset, multiplied by your number of epochs.\n* A Gemini tuned model endpoint has the same prediction price as the base model.\nCompare pricing for partner models on Vertex AI\nPartner models are a curated list of generative AI models developed by\nGoogle partners. Partner models are offered as managed APIs. For more\ninformation, see\nOverview of partner models\n.\nThe following sections list pricing details for Google partner models.\nAI21 Lab's models\nModel\nPricing\nJamba 1.5 Large (Deprecated)\nInput: $2 / million tokens\nOutput: $8 / million tokens\nJamba 1.5 Mini (Deprecated)\nInput: $0.20 / million tokens\nOutput: $0.40 / million tokens\nAnthropic’s Claude models\nModels with regional pricing\nGlobal\nModel\nPrice (/1M tokens) < 200K input tokens\nPrice (/1M tokens) >= 200K input tokens\nClaude Sonnet 4.5\nInput: $3.00\nOutput: $15.00\nBatch Input: $1.50\nBatch Output: $7.50\nCache Write: $3.75\nCache Hit: $0.30\nBatch Cache Write: $1.88\nBatch Cache Hit: $0.15\nInput: $6.00\nOutput: $22.50\nBatch Input: $3.00\nBatch Output: $11.25\nCache Write: $7.50\nCache Hit: $0.60\nBatch Cache Write: $3.75\nBatch Cache Hit: $0.30\nClaude Haiku 4.5\nInput: $1.00\nOutput: $5.00\nBatch Input: $0.50\nBatch Output: $2.50\nCache Write: $1.25\nCache Hit: $0.10\nBatch Cache Write: $0.625\nBatch Cache Hit: $0.05\nus-east5\nModel\nPrice (/1M tokens) < 200K input tokens\nPrice (/1M tokens) >= 200K input tokens\nClaude Sonnet 4.5\nInput: $3.30\nOutput: $16.50\nBatch Input: $1.65\nBatch Output: $8.25\nCache Write: $4.13\nCache Hit: $0.33\nBatch Cache Write: $2.06\nBatch Cache Hit: $0.17\nInput: $6.60\nOutput: $24.75\nBatch Input: $3.30\nBatch Output: $12.38\nCache Write: $8.25\nCache Hit: $0.66\nBatch Cache Write: $4.13\nBatch Cache Hit: $0.33\nClaude Haiku 4.5\nInput: $1.10\nOutput: $5.50\nBatch Input: $0.55\nBatch Output: $2.75\nCache Write: $1.375\nCache Hit: $0.11\nBatch Cache Write: $0.688\nBatch Cache Hit: $0.055\neurope-west1\nModel\nPrice (/1M tokens) < 200K input tokens\nPrice (/1M tokens) >= 200K input tokens\nClaude Sonnet 4.5\nInput: $3.30\nOutput: $16.50\nBatch Input: $1.65\nBatch Output: $8.25\nCache Write: $4.13\nCache Hit: $0.33\nBatch Cache Write: $2.06\nBatch Cache Hit: $0.17\nInput: $6.60\nOutput: $24.75\nBatch Input: $3.30\nBatch Output: $12.38\nCache Write: $8.25\nCache Hit: $0.66\nBatch Cache Write: $4.13\nBatch Cache Hit: $0.33\nClaude Haiku 4.5\nInput: $1.10\nOutput: $5.50\nBatch Input: $0.55\nBatch Output: $2.75\nCache Write: $1.375\nCache Hit: $0.11\nBatch Cache Write: $0.688\nBatch Cache Hit: $0.055\nasia-southeast1\nModel\nPrice (/1M tokens) < 200K input tokens\nPrice (/1M tokens) >= 200K input tokens\nClaude Sonnet 4.5\nInput: $3.30\nOutput: $16.50\nBatch Input: $1.65\nBatch Output: $8.25\nCache Write: $4.13\nCache Hit: $0.33\nBatch Cache Write: $2.06\nBatch Cache Hit: $0.17\nInput: $6.60\nOutput: $24.75\nBatch Input: $3.30\nBatch Output: $12.38\nCache Write: $8.25\nCache Hit: $0.66\nBatch Cache Write: $4.13\nBatch Cache Hit: $0.33\nasia-east1\nModel\nPrice (/1M tokens) < 200K input tokens\nPrice (/1M tokens) >= 200K input tokens\nClaude Haiku 4.5\nInput: $1.10\nOutput: $5.50\nBatch Input: $0.55\nBatch Output: $2.75\nCache Write: $1.375\nCache Hit: $0.11\nBatch Cache Write: $0.688\nBatch Cache Hit: $0.055\n* If a query input context is longer than or equal to 200K tokens, all tokens (input and output) are charged at long context rates.\nModels with uniform pricing across all regions\nModel\nPrice (/1M tokens) < 200K input tokens\nPrice (/1M tokens) >= 200K input tokens\nClaude Opus 4.1\nInput: $15\nOutput: $75\nBatch Input: $7.50\nBatch Output: $37.50\nCache Write: $18.75\nCache Hit: $1.50\nBatch Cache Write: $9.375\nBatch Cache Hit: $0.75\nN/A\nClaude Opus 4\nInput: $15\nOutput: $75\nBatch Input: $7.50\nBatch Output: $37.50\nCache Write: $18.75\nCache Hit: $1.50\nBatch Cache Write: $9.375\nBatch Cache Hit: $0.75\nN/A\nClaude Sonnet 4\nInput: $3\nOutput: $15\nBatch Input: $1.50\nBatch Output: $7.50\nCache Write: $3.75\nCache Hit: $0.30\nBatch Cache Write: $1.875\nBatch Cache Hit: $0.15\nInput: $6\nOutput: $22.50\nBatch Input: $3\nBatch Output: $11.25\nCache Write: $7.50\nCache Hit: $0.60\nBatch Cache Write: $3.75\nBatch Cache Hit: $0.30\nClaude 3.7 Sonnet\nInput: $3\nOutput: $15\nBatch Input: $1.50\nBatch Output: $7.50\nCache Write: $3.75\nCache Hit: $0.30\nBatch Cache Write: $1.875\nBatch Cache Hit: $0.15\nN/A\nClaude 3.5 Haiku\nInput: $0.80\nOutput: $4\nBatch Input: $0.40\nBatch Output: $2\nCache Write: $1\nCache Hit: $0.08\nBatch Cache Write: $0.50\nBatch Cache Hit: $0.04\nN/A\nClaude 3 Haiku\nInput: $0.25\nOutput: $1.25\nCache Write: $0.30\nCache Hit: $0.03\nN/A\nClaude 3.5 Sonnet v2 (Deprecated)\nInput: $3\nOutput: $15\nBatch Input: $1.50\nBatch Output: $7.50\nCache Write: $3.75\nCache Hit: $0.30\nBatch Cache Write: $1.875\nBatch Cache Hit: $0.15\nN/A\nClaude 3.5 Sonnet (Deprecated)\nInput: $3\nOutput: $15\nCache Write: $3.75\nCache Hit: $0.30\nN/A\nClaude 3 Opus (deprecated)\nInput: $15\nOutput: $75\nCache Write: $18.75\nCache Hit: $1.50\nN/A\n* If a query input context is longer than or equal to 200K tokens, all tokens (input and output) are charged at long context rates.\nPricing for tools\nTool\nPrice\nWeb Search Request\n$10 per 1000 searches\nModels Supported: Claude Haiku 4.5, Claude Sonnet 4.5, Claude Sonnet 4, Claude Opus 4.1, and Claude Opus 4.\n* If a query input context is longer than or equal to 200K tokens, all tokens (input and output) are charged at long context rates.\nDeepseek's models\nModel\nPricing\nDeepSeek-V3.1\nInput: $0.60 / million tokens\nOutput: $1.70 / million tokens\nBatch Input: $0.30 / million tokens\nBatch Output: $0.85 / million tokens\nDeepSeek-R1 (0528)\nInput: $1.35 / million tokens\nOutput: $5.40 / million tokens\nBatch Input: $0.675 / million tokens\nBatch Output: $2.70 / million tokens\nDeepSeek-OCR *\nInput: $0.30 / million tokens (or $0.0003/page)\nOutput: $1.20 / million tokens (or $0.00012/page)\nAvailable at no charge until Nov 10, 2025.\nMiniMax's models\nModel\nPricing\nMiniMax-M2 *\nInput: $0.30 / million tokens\nOutput: $1.20 / million tokens\nAvailable at no charge until Nov 10, 2025.\nQwen's models\nModel\nPricing\nQwen3-Next-80B-Thinking\nInput: $0.15 / million tokens\nOutput: $1.20 / million tokens\nQwen3-Next-80B-Instruct\nInput: $0.15 / million tokens\nOutput: $1.20 / million tokens\nQwen3-Coder-480B-A35B-Instruct\nInput: $1.00 / million tokens\nOutput: $4.00 / million tokens\nBatch Input: $0.50 / million tokens\nBatch Output: $2.00 / million tokens\nQwen3-235B-A22B-Instruct-2507\nInput: $0.25 / million tokens\nOutput: $1.00 / million tokens\nBatch Input: $0.125 / million tokens\nBatch Output: $0.50 / million tokens\nOpenAI's models\nModel\nPricing\ngpt-oss-120b\nInput: $0.09 / million tokens\nOutput: $0.36 / million tokens\nBatch Input: $0.045 / million tokens\nBatch Output: $0.18 / million tokens\ngpt-oss-20b\nInput: $0.07 / million tokens\nOutput: $0.25 / million tokens\nBatch Input: $0.035 / million tokens\nBatch Output: $0.125 / million tokens\nMeta's Llama models\nModel\nPricing\nLlama 3.1 405B\nInput: $5.00 / million tokens\nOutput: $16.00 / million tokens\nLlama 3.3 70B\nInput: $0.72 / million tokens\nOutput: $0.72 / million tokens\nBatch Input: $0.36 / million tokens\nBatch Output: $0.36 / million tokens\nLlama 4 Scout\nInput: $0.25 / million tokens\nOutput: $0.70 / million tokens\nBatch Input: $0.125 / million tokens\nBatch Output: $0.35 / million tokens\nLlama 4 Maverick\nInput: $0.35 / million tokens\nOutput: $1.15 / million tokens\nBatch Input: $0.175 / million tokens\nBatch Output: $0.575 / million tokens\nMistral AI’s models\nModel\nPricing\nMistral OCR (25.05)\nInput: $0.0005 / million tokens (or $0.0005/page)\nOutput: $0.0005 / million tokens (or $0.0005/page)\nMistral Medium 3\nInput: $0.40 / million tokens\nOutput: $2.00 / million tokens\nMistral Small 3.1 (25.03)\nInput: $0.10 / million tokens\nOutput: $0.30 / million tokens\nMistral Large (24.11) (deprecated)\nInput: $2.00 / million tokens\nOutput: $6.00 / million tokens\nCodestral 2\nInput: $0.30 / million tokens\nOutput: $0.90 / million tokens\nCodestral (25.01) (deprecated)\nInput: $0.30 / million tokens\nOutput: $0.90 / million tokens\nRequest a custom quote\nWith Google Cloud's pay-as-you-go pricing, you only pay for the services you\nuse. Connect with our sales team to get a custom quote for your organization.\nContact sales",
      "content_length": 37923,
      "line_count": 1375
    }
  ]
}